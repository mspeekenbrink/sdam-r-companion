[["index.html", "An R companion to Statistics: data analysis and modelling Preface 0.1 Acknowledgements", " An R companion to Statistics: data analysis and modelling Maarten Speekenbrink 2022-11-20 Preface This is a companion to the book Statistics: Data analysis and modelling. It covers how to perform the analyses discussed in that book, mostly using “base” R and a relatively small selection of add-on packages. R is a programming language and environment specifically designed for data analysis. It is flexible, relatively fast, and has a large number of users and contributors. However, R is known to have a somewhat steep learning curve, so if you want to learn R, you will have to put in some extra effort (compared to e.g. JASP or SPSS). This effort will certainly pay off in the end, but it is up to you to decide whether you want to make this investment. This companion is meant to show you how to use R to do the types of analyses covered in “Statistics: Data analysis and modelling”. It is certainly not meant as a complete course on R. There are lots of good resources on R available on the internet and I suggest that, if you are serious about learning R, you also look elsewhere. Some sources you might find useful are: Beginner’s guide to R (Computer World) Interactive introduction to R programming (DataCamp) Try R (another interactive tutorial by codeschool) A freely downloadable book on R and statistics specifically focused on psychology students (especially part II and III are relevant): Learning statistics with R (Danielle Navarro) 0.1 Acknowledgements Parts of these notes were adapted from other sources (if there is a licence allowing that). I acknowledge these sources in the text or footnotes. "],["introduction.html", "Chapter 1 Introduction 1.1 What is R?1 1.2 Getting started 1.3 Working with RStudio 1.4 Installing packages 1.5 Getting help 1.6 First steps: R as a calculator 1.7 Data 1.8 Exploring data: Descriptive statistics 1.9 Exploring data: Creating plots 1.10 A note about namespaces and loading packages", " Chapter 1 Introduction 1.1 What is R?1 R is a statistical programming language that has rapidly gained popularity in many scientific fields. It was developed by Ross Ihaka and Robert Gentleman as an open source implementation of the “S” programming language. (Next time you need a fun fact, you can say “Did you know that S came before R?”) R is also the name of the software that uses this language for statistical computing. With a huge online support community and dedicated packages that provide extra functionality for virtually any application and field of study, there’s hardly anything you can’t do in R. If you already know your way around statistical software like JASP or SPSS, the main difference is that R has no graphical user interface, which means there are no buttons to click and no dropdown menus. R can be run entirely by typing commands into a text interface (welcome to the Matrix!). This may seem a little daunting, but it also means a whole lot more flexibility, as you are not relying on a pre-determined toolkit for your analyses. If you need any more convincing, why are we using R and not one of the many other statistical packages like JASP, SPSS, MATLAB, Minitab, or even Microsoft Excel? Well, R is great because: R is free and open source, and always will be! Anybody can use the code and see exactly how it works. Because R is a programming language rather than a graphical interface, the user can easily save scripts as small text files for use in the future, or share them with collaborators. R has a very active and helpful online community - normally a quick search is all it takes to find that somebody has already solved the problem you’re having. 1.2 Getting started If you want to use R and RStudio, you should first install R, and after that install RStudio. R and RStudio are separate programs, and RStudio requires R to be installed. 1.2.1 Download R You can download R from CRAN (The Comprehensive R Archive Network). Select the link appropriate for your operating system and follow the instructions. You will want to download the installer for the latest release (currently version 4.0.2) of the base R software. As you can see, the CRAN website has a rather distinctive “old-school” look. Don’t let that fool you though. R itself is anything but old school. 1.2.2 Download R Studio R does not come with a graphical interface by default. Most people nowadays interact with R through second-party graphical platforms that provide extra functionality. Probably the most popular graphical front-end to R is RStudio. This is actually a full “integrated development environment” (IDE), but mostly, we will use it as a place where we can keep scripts, plots, and R output together in one place. Like R, RStudio is open source software and free to download for anyone that wants to. You can download RStudio from the RStudio website (select the free open source desktop version). 1.3 Working with RStudio When you open RStudio, you will see something like Figure 1.1. You will probably not see exactly the same layout, but once you click on File in the top menu, and then New File &gt; R Script, you should be pretty close. You can get direct access to the R environment itself in the console panel. If you type in commands here, they will be interpreted by R, and possibly some output is given. Working directly within the R console is handy if you want to do simple calculations, or try out different commands and functions. When you are conducting analyses, you will want to keep the commands that produce useful results somewhere, so you don’t have to type in everything again if you want to rerun the analyses, or if you need to change something. That is where R scripts come in handy. Basically, these are text files where you store a collection of commands that can be interpreted by R. Within RStudio, you can select lines of the script, and by clicking on Run, those lines will get pasted to the R console. R scripts should only contain working R commands. You can comment on your code by preceding a line (or the end of a line) by a hash-symbol (“#”). Anything after the has symbol is not evaluated by the R interpreter. Figure 1.1: The RStudio interface consists of four main panels. The source panel (top left) is where you can keep scripts. The console panel (bottom left) is where the R commands and text output go; this is where R truly lives. The environment and history panel (top right) shows which R objects are currently available in memory and a history of all the R commands the R console received. The files, plots, packages, etc panel contains a file browser, a display panel for all plots, a list of installed R packages, and a browser for help files. Another useful way to store R commands is in a different file format, called R Markdown. R Markdown allows you to combine text, plots, and R commands all in a single file. This file can then be “parsed” to produce a variety of document formats, such as HTML, pdf, and even Microsoft Word. If you click on File &gt; New File &gt; R Markdown in RStudio, you can see an example of such a file. As the name suggests, R Markdown is a combination of R and Markdown. Markdown is a lightweight markup language for creating formatted text documents with a plain-text editor. A markup language is, roughly put, a system which defines elements in a document by their role, for instance defining certain elements as titles or headers, and others as quoted text or test that should be emphasised. Common examples of markup languages are HTML and XML. If you know a little HTML, you might know that in modern implementations, it separates content and markup (HTML) from style (CSS). This separation allows you to easily create a variety of documents which are visually very different from the same HTML source file. When you use a word processor such as Microsoft Word, it creates a single document which specifies both content and style, and in a way which is specific to the word processor used. Markdown aims to provide a way to define a software-agnostic markup language, separating content from style, which can be used to produce a variety of output formats from the same source file. R markdown add to this an integration with R. Effectively, R Markdown first evaluates all the R code in an R Markdown file to create a “plain” markdown file, which can then be parsed into a variety of output formats. The great thing about this is that you can create automatically reproducible documents, and you don’t have to copy-paste results of analyses between R and your word processor, avoiding the common mistakes that this brings. And R Markdown is really flexible. For example, using a package like bookdown, you can even write whole books in R Markdown (like this one)! We will discuss R Markdown in more detail at a later point. If you want to get started already, a very useful resource is R Markdown: The Definitive Guide. 1.4 Installing packages Part of the popularity of R stems from the thousands of packages that extend the basic capabilities of R. Packages extend the functionality of base R, and can provide new objects, functions, datasets, etc. Before these additional packages can be used, they need to be installed first. 1.4.1 Installing the sdamr package The “Statistics: Data analysis and modelling” book has an associated R package which contains the data sets used as examples in the book, as well as some additional functions. It is available from the Comprehensive R Archive Network (CRAN), which is a large repository of R packages. You can install the package from there by simply typing in the R console: install.packages(&quot;sdamr&quot;) If for some reason the sdamr package is not available there, you will see a warning (package ‘sdamr’ is not available for this version of R). If that happens, you can install the development version from GitHub (see below). The source code of the sdamr package is hosted in GitHub, and the package can be installed from there as well with help of the remotes package. So you will first need to install that package, and then you can use the install_github function to install the sdamr package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;mspeekenbrink/sdam-r&quot;) Note that by typing remotes:: before the function call, we are telling R that the function is available in the remotes package. This avoids you having to load the package (i.e. by library(remotes)) first. 1.4.2 Loading packages Once installed, the functionality of additional packages can be made available by loading them with the library function. To load the sdamr package, you would type library(sdamr) This also provides a check whether the package is properly installed (if not, you will get an error message). Technically, loading a package with the library() function attaches all its functions and datasets to the global namespace. Roughly speaking, that means that if you type in the name of a function from a package, R will now where to look for the code of that function so that it can execute it. 1.5 Getting help R may be tricky to master, especially at the start, but help is never far away: From within R If you want more information on a specific function, use a question mark before it (e.g., ?plot) or the help function (e.g., help(plot)) If you don’t know the function name, use two question marks (e.g., ??plot) or the help.search function (e.g., help.search(\"plot\")) If you know a function is in a package, use search help on the package (e.g., ?ggplot2) The RSiteSearch(\"keyword\") function will will search for “keyword” in all functions available in R, associated packages, and the R-Help News groups (if desired). Online Stack Overflow is a platform in which you can ask questions about R and its many packages. Many questions will already have been asked, so its archive of questions and answers is particularly useful. The meta-search engine at www.rseek.org may also be handy. R has an active help mailing list as well, but when asking questions there, make sure you read the posting guide, as some people on there sometimes get a little grumpy. 1.6 First steps: R as a calculator R can be used as a console-based calculator. Here are some examples. You can add numbers using the + operator, e.g \\(2 + 11\\): 2 + 11 # addition ## [1] 13 You can multiply numbers using the * operator, e.g \\(2 \\times 11\\): 2 * 11 # multiplication ## [1] 22 You can divide numbers using the / operator, e.g. \\(\\frac{2}{11}\\): 2 / 11 # division ## [1] 0.1818182 You can raise numbers to a power by using the ^ operator, e.g. \\(2^{11}\\): 2^(11) # exponentiation ## [1] 2048 You can take a square-root by using the sqrt function, e.g. \\(\\sqrt{2}\\): sqrt(2) # square root ## [1] 1.414214 which is mathematically equivalent to raising a number to the power of \\(\\tfrac{1}{2}\\), e.g. \\(2^{\\tfrac{1}{2}}\\): 2^(1/2) # another way to compute the square root ## [1] 1.414214 In performing arithmetical operations, it is important to realise that R obeys the commonly accepted rules of precedence in performing arithmetic operations, which is: Brackets Powers Multiplication and division Addition and subtraction This means that anything within brackets is evaluated first (following the order of the remaining operations), than any elements involving powers are calculated, and after this those involving multiplication or division, and finally addition and multiplication. So 2 + 11*3 ## [1] 35 is evaluated as \\((11 \\times 3) + 2\\), whilst (2 + 11)*3 ## [1] 39 is evaluated as \\((2+11) \\times 3\\). Another way to put this is that R doesn’t evaluate expression left-to-right. It takes the whole expression, first computes the results within brackets, then computes any powers, then multiplies or divides the results of this, and finally adds or subtracts the results of that. Many common errors result from not using brackets properly, or forgetting about the precedence of arithmetic operations. Unrelated to this, but also important: the hash symbol (“#”) is used for comments, such that anything following a “#” is not evaluated. 1.7 Data You can load in data files that come with R packages by using the data function, with as argument the name of the dataset you want to load (as a string, so make sure you use quotation signs). For instance, you can load the dataset fifa2010teams from the sdamr package as follows: library(sdamr) data(&quot;fifa2010teams&quot;) A loaded dataset will show up in the Environment panel in RStudio. If you click on the name of the dataset, you can then see the data as a table in the Source panel. You can also view the data in the R console by simply typing the name of the dataset. This will often produce a lot of output. If you just want to view a part of the dataset, you can use the head function, which will show the first 6 rows: head(fifa2010teams) ## nr team matches_played goals_for goals_scored goals_against ## 1 1 Germany 7 16 16 5 ## 2 2 Netherlands 7 12 11 6 ## 3 3 Uruguay 7 11 11 8 ## 4 4 Argentina 5 10 9 6 ## 5 5 Brazil 5 9 9 4 ## 6 6 Spain 7 8 8 2 ## penalty_goal own_goals_for yellow_cards indirect_red_cards direct_red_cards ## 1 0 0 13 0 0 ## 2 0 0 24 0 0 ## 3 1 0 11 0 1 ## 4 0 0 7 0 0 ## 5 0 0 9 0 1 ## 6 0 0 8 0 0 You can also get a quick summary of the characteristics of the variables in the data through the summary function: summary(fifa2010teams) ## nr team matches_played goals_for ## Min. : 1.00 Length:32 Min. :3.00 Min. : 0.000 ## 1st Qu.: 8.75 Class :character 1st Qu.:3.00 1st Qu.: 2.000 ## Median :16.50 Mode :character Median :3.50 Median : 3.000 ## Mean :16.50 Mean :4.00 Mean : 4.531 ## 3rd Qu.:24.25 3rd Qu.:4.25 3rd Qu.: 5.250 ## Max. :32.00 Max. :7.00 Max. :16.000 ## goals_scored goals_against penalty_goal own_goals_for ## Min. : 0.000 Min. : 1.000 Min. :0.0000 Min. :0 ## 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.:0.0000 1st Qu.:0 ## Median : 3.000 Median : 5.000 Median :0.0000 Median :0 ## Mean : 4.469 Mean : 4.531 Mean :0.2812 Mean :0 ## 3rd Qu.: 5.250 3rd Qu.: 5.250 3rd Qu.:0.2500 3rd Qu.:0 ## Max. :16.000 Max. :12.000 Max. :2.0000 Max. :0 ## yellow_cards indirect_red_cards direct_red_cards ## Min. : 2.000 Min. :0 Min. :0.0000 ## 1st Qu.: 6.000 1st Qu.:0 1st Qu.:0.0000 ## Median : 7.500 Median :0 Median :0.0000 ## Mean : 8.156 Mean :0 Mean :0.2812 ## 3rd Qu.: 9.000 3rd Qu.:0 3rd Qu.:0.2500 ## Max. :24.000 Max. :0 Max. :2.0000 1.7.1 Data types Data in R is generally stored in vectors, which are fixed-length collections of values of a particular data type. Common data types are logical: values which can either be TRUE or FALSE numeric: numbers of all kinds, such as 1, 356, and 34.5782 character: characters and strings, such as q and Hello You can combine values of a data type in a vector by using the c() function (which stands for “combine”). For instance c(TRUE, FALSE, TRUE, TRUE) ## [1] TRUE FALSE TRUE TRUE c(3,4,802.376) ## [1] 3.000 4.000 802.376 c(&quot;Coffee&quot;,&quot;now&quot;,&quot;please&quot;) ## [1] &quot;Coffee&quot; &quot;now&quot; &quot;please&quot; If you combine elements of different data types, then R will convert them to the most “general” type necessary. Combining a logical value with a numeric one, for instance, will convert logical value TRUE to 1, and FALSE to 0. Combining a character element with other elements, will convert everything to character elements: c(TRUE, FALSE, 12) ## [1] 1 0 12 c(TRUE, 5.67788, &quot;let&#39;s see what happens&quot;) ## [1] &quot;TRUE&quot; &quot;5.67788&quot; &quot;let&#39;s see what happens&quot; 1.7.2 Objects Objects are named things that are stored in memory and available to functions etc. Objects can be vectors, such as discussed above, but also more general types, such as matrices, factors, and data frames. If you want to create an object, you use the assignment operator &lt;-, with on the left side the name you want to give to the object, and on the right side the content of the object. For instance, we can store a numeric vector as the object my_vector as follows: my_vector &lt;- c(1,2,10:20) Note a little trick above, where 10:20 stands for a sequence of integers, i.e. \\(10, 11, 12, \\ldots, 20\\). my_vector is now an object in R memory (you should see it show up in the Environment panel), and can be called by name, as in: my_vector ## [1] 1 2 10 11 12 13 14 15 16 17 18 19 20 A matrix is a collection of vectors of the same length, joined as columns or rows. mat &lt;- matrix(1:10,ncol=2) mat # matrices are filled column-wise ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 mat[,2] # select the second column (the result is a vector) ## [1] 6 7 8 9 10 mat[3,1] # select the value in the third row and first column ## [1] 3 A factor is useful for nominal and ordinal data. A factor is a vector with integers, where each integer is provided with a unique label. For instance # construct a factor by giving integer values and specifying the accompanying # labels fact &lt;- factor(c(1,2,2,3),labels=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) fact # display it ## [1] red green green blue ## Levels: red green blue fact == &quot;green&quot; # determine which elements equal (==) &#39;green&#39; ## [1] FALSE TRUE TRUE FALSE A list is a collection of different R objects. This is a very general type of object, and the elements of a list can even be lists themselves. A list allows you to keep different types of information together, but you probably won’t need to use it much for the content discussed here. But let’s quickly look at some aspects of a list: lst &lt;- list(a=mat, b=fact) # construct a named list with a matrix and factor lst ## $a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 ## ## $b ## [1] red green green blue ## Levels: red green blue If a list is named, meaning the elements have names, like above, you can select elements from the list by using a dollar sign and then the name of the elements: lst$a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 lst$b ## [1] red green green blue ## Levels: red green blue You can also select elements by an index number, which should go in between double square brackets. For instance, if you want to select the first element, you can type lst[[1]] A data.frame is probably one of the most useful features of R for data analysis. A data.frame is like a matrix, in that it is a rectangular collection of data, but the columns are variables which can be of a different type (e.g., numeric, factors, or characters). You can construct data frames through the data.frame function, for instance as my_data_frame &lt;- data.frame(var1 = 1:10, var2 = 10:1, var3 = rep(c(&quot;a&quot;,&quot;b&quot;),times=5)) my_data_frame ## var1 var2 var3 ## 1 1 10 a ## 2 2 9 b ## 3 3 8 a ## 4 4 7 b ## 5 5 6 a ## 6 6 5 b ## 7 7 4 a ## 8 8 3 b ## 9 9 2 a ## 10 10 1 b Constructing a data frame looks similar to constructing a list, but all the (named) arguments should have the same length. If you for instance try data.frame(var1 = 1:10, var2 = 1:11) you will get an error. You won’t always get an error. If the length of longer elements are multiples of the length of shorter elements, R will fill in the missing values by repeating the shorter elements until they are of the same length as the longer elements. For instance data.frame(var1 = 1:10, var2 = 1:5, var3 = 1) ## var1 var2 var3 ## 1 1 1 1 ## 2 2 2 1 ## 3 3 3 1 ## 4 4 4 1 ## 5 5 5 1 ## 6 6 1 1 ## 7 7 2 1 ## 8 8 3 1 ## 9 9 4 1 ## 10 10 5 1 This can be handy, but also risky, as sometimes you might not realise that R is filling in values for you, and your analyses might give rather unexpected results. I would therefore always ensure that you create a data frame with elements of the same length. Most of the time, you won’t create data frames yourself within R, but you will load in external data as a data frame. 1.7.3 Importing data R can load data in many formats. Personally, I mainly use data stored in “comma separated value” (CSV) format. This is one of the most portable ways of storing data, so that it can be used in a variety of programs like R, SPSS, JASP, Excel, etc. Data in a comma-separated value (CSV) format can be read through the read.csv function. A nice thing about R is that it can read data directly from the World Wide Web. So you don’t need to first download data, and then open it from within R. [TODO: example] At some point, you will probably also come across data stored in Excel or SPSS format. regarding Excel, it is safest to first save a spreadsheet as a CSV file, and then load this file into R. Alternatively, the xlsx package provides the function read.xlsx to directly read a spreadsheet into R. To load data in SPSS format, the package foreign package provides the read.spss function. 1.8 Exploring data: Descriptive statistics Measures of location and spread can be computed through specialized functions, namely mean, median, IQR (inter-quartile range), var (variance), and sd (standard deviation). E.g. mean(fifa2010teams$goals_for) ## [1] 4.53125 median(fifa2010teams$goals_for) ## [1] 3 will give you the mean and median of variable goals_for in data.frame fifa2010teams. You can obtain the inter-quartile range as IQR(fifa2010teams$goals_for,type=1) ## [1] 3 Note the use of the type argument here. There are many ways in which to compute and estimate percentiles and quantiles. Using type=1 gives you the same result as the way I explained how to compute the IQR in the book. By default, R will use type = 7, which gives different results (type ?quantile for more information). The var and sd functions from base R do not actually provide the sample variance_ and sample standard deviation. Rather, they give unbiased estimates of the “true” (population) variance and standard deviation. To compute the variance and standard deviation of the sample data, you can use the sample_var and sample_sd functions in the sdamr package: sample_var(fifa2010teams$goals_for) ## [1] 13.24902 sample_sd(fifa2010teams$goals_for) ## [1] 3.639921 There is no function in base R to compute the mode2, but the sdamr package provides the function sample_mode to do just that: sample_mode(fifa2010teams$goals_for) ## [1] 3 1.9 Exploring data: Creating plots There are two common ways to plot data with R. Base R has various plotting functions, such as plot, hist, boxplot, which are useful for quick plots to explore your data. The resulting plots are not always the most aesthetically pleasing. The R package ggplot2 provides means to create a wide range of useful and beautiful plots. It is based on the idea of a “grammar of graphics”, which makes it extremely flexible, but also a little difficult to get your head around. In the following, I will show you how to use both base R and ggplot2. 1.9.1 Histogram R has many built-in plotting functions. These tend to be a little basic, and much prettier plots can be made with packages such as ggplot2 (Wickham et al. 2022, my current favourite!). But for quick data exploration, the built-in plotting functions are faster. A histogram is plotted through the hist function. In the following example, I first generate some random data, store it in an object called dat and then plot a histogram: hist(fifa2010teams$goals_for) There are many parameters you can change. In the following, I give the plot a new title and x-axis labels, as well as request the number of bins to be 20: hist(fifa2010teams$goals_for,main=&quot;Histogram of points scored by teams in the FIFA 2010 World Cup&quot;, xlab=&quot;Goals for&quot;, breaks=20) To create a nicer looking plot, you can use ggplot2. library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Well, that’s not actually so pretty. We can make it better by changing some of the defaults: library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram(bins=10, colour=&quot;black&quot;, fill=&#39;#8C8279&#39;) + xlab(&quot;Goals scored&quot;) Note that within the geom_histogram function, I’ve specified to use 10 bins, and draw a black line around the bars, and fill the bars with colour specified by the hexadecimal colour code ‘#8C8279’. Finally, I’m using the xlab function to generate a better label for the x-axis. ggplot2 is very powerful and flexible, so there are many such adjustments you can make. A useful resource with practical guidance on creating a variety of plots with ggplot2 is the R graphics cookbook. A more thorough treatment of ggplot2 and the underlying ideas of a Grammar of Graphics, is ggplot2: elegant graphics for data analysis. 1.9.2 Boxplot For a quick boxplot, you can use the base R function with the same name: boxplot(fifa2010teams$goals_for) ggplot2 also provides a boxplot through the geom_boxplot function. Note that in the aes specification, I’m now using goals_for as the y-axis. ggplot(fifa2010teams,aes(y=goals_for)) + geom_boxplot() Not very pretty! A somewhat better version can be obtained by: ggplot(fifa2010teams,aes(x=&quot;&quot;,y=goals_for)) + geom_boxplot(width=.2) + xlab(&quot;&quot;) 1.9.3 Scatterplot A quick scatterplot can be created with the plot function, in which you specify the variable to show on the x-axis and the variable on the y-axis: plot(x=fifa2010teams$matches_played, y=fifa2010teams$goals_for) To get a similar plot with ggplot2, you can use the geom_point function: ggplot(fifa2010teams, aes(x=matches_played, y=goals_for)) + geom_point() In ggplot2, specification of the variables on the x-axis and y-axis is done within the aesthetics specification (aes) within the initial call to ggplot. The later functions, such as geom_point, inherit the specifications provided there. 1.9.4 Raincloud plot A basic (but reasonably flexible) function to create a raincloud plot is provided in the sdamr package through the plot_raincloud function. The data argument expects a data frame, and the y argument expects the name of the variable for which you want to create the plot. Note that as the jitter applied to the plot is random, to get exactly the same plot again, you need to set the random number seed through set.seed before. This is only necessary if you want to recreate a plot exactly. We’ll talk more about random number generation later. set.seed(467) plot_raincloud(data=fifa2010teams, y=goals_for) ## Warning: Removed 1 rows containing missing values (`geom_segment()`). 1.10 A note about namespaces and loading packages Loading packages with library() and attaching their functionality to the global namespace is convenient, but it can lead to errors as well. This is because packages can override functions already in the namespace. For example, a package might provide a function called mean(), which is also provided in base R. After loading the additional package, whenever you then call mean(), you will use that function from the additional package, and not from base R. As you might want to use a large number of additional packages, keeping track which functions will be used can be tricky. Whenever a function is “overwritten” (or “masked”) by a new function, R will provide a warning message. For example, if you load the dplyr package (you may need to install this first with install.packages(\"dplyr\")): library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union you will see that this package offers a number of functions with the same name as functions in the base and stats packages from base R. So whenever you now call the filter() function, you will use the function as provided by dplyr, and not as provided by stats. To avoid problems, it is best to explicitly indicate from which package (or namespace) you would like to use a function. This is done by typing package_name:: before a function call (where package_name should be replaced with the actual name of the package). For example, after loading the dplyr package, we can still use the function from the stats package by using stats::filter instead of filter. By explicitly stating the namespace of packages, you don’t have to load a package at all. Because you are telling R where to look for a function definition, it does not have to be available in the global namespace. If you are using many functions from a package repeatedly, it will be cumbersome to specify the name of the package before each function call. In this case, it makes sense to load the package. In other cases, I would suggest to specify the namespace explicitly. References "],["statistical-modelling.html", "Chapter 2 Statistical modelling 2.1 Distributions 2.2 Estimation 2.3 Hypothesis testing directly with the binomial distribution", " Chapter 2 Statistical modelling As R was written by statisticians for statisticians, it naturally has very good support for statistical modelling. In particular, there are convenient functions for sampling observations from a wide-variety of probability distributions, and computing likelihoods according to those distributions. And of course, there is code to estimate and test many statistical models. 2.1 Distributions R includes functions to calculate probabilities and generate random data from a wide variety of distributions (type ?distributions for an overview). The main distributions that we will use are: Distribution R name Parameters Binomial binom size (\\(n\\)), prob (\\(\\theta\\)) Chi-squared chisq df F f df1, df2 Normal norm mean (\\(\\mu\\)), sd (\\(\\sigma\\)) Student’s t t df The statistical distribution functions all have a similar interface: you can generate random data by calling the R name with an r (for random) before it (e.g. rbinom for random draws from the Binomial distribution, and rnorm for random draws from the Normal distribution) you can compute the probability (or density function value) of a particular value by calling the function with a d (for density) before it (e.g. dbinom and dnorm) you can compute a cumulative probability (the probability of a particular value and anything lower than it) by calling the function with a p (for probability) before it (e.g. pbinom) you can compute a quantile (the value such that there is a particular probability of sampling a value equal to or lower that it) by calling the function with a q (for probability) before it (e.g. qbinom) 2.1.1 Generating random data let’s use a coin tossing model to generate lots of replications of an experiment in which Paul is asked to provide 8 predictions. The dependent variable in each experiment is \\(Y_i = k\\), where \\(k\\) is the number of correct predictions (out of \\(n=8\\)). To generate 100 replications of such an experiment (e.g. 100 “alternate universes” where Paul might have made different predictions), we can use the rbinom function: set.seed(4638) Y &lt;- rbinom(n=100, size=8, prob=0.5) Y ## [1] 5 4 5 2 5 2 4 5 3 4 3 5 4 5 3 4 6 3 2 3 0 3 5 5 1 2 5 4 6 3 5 4 4 1 3 5 3 ## [38] 5 6 8 5 5 3 2 3 3 3 5 4 5 4 2 2 4 5 3 5 4 4 5 3 5 5 5 2 3 4 8 1 6 4 3 4 4 ## [75] 3 6 3 4 5 4 1 3 1 5 3 5 4 5 5 4 5 3 3 6 4 4 3 5 1 6 You can see quite some variability in the outcomes, perhaps easier in a histogram hist(Y, breaks=8) Computers can’t actually generate real random data. Computers are deterministic, performing computations according to instructions. But clever algorithms have been designed that produce sequences of numbers that are (almost) indistinguishable from purely random sequences. These algorithms are called random number generators and the default in R is the so-called “Mersenne-Twister” algorithm. You don’t have to worry about the details of such algorithms. We can just pretend that they produce truly random numbers. One thing I do want to point out is that before you use one of such algorithms, they need to be initialized with a special number, called the random seed. A number like this basically sets the algorithm in motion. Starting it with a different seed will produce a different sequence of random numbers. If you start it with the same random seed, you get exactly the same sequence of random numbers. If you don’t supply a value, R will use the current time as the random seed the first time you ask it to produce a random number in a session. For the purposes of replicating things, it may be useful to set the seed explicitly (so that a subsequent call to a random number generator will produce exactly the same results.) Therefore, you will now and then see calls such as set.seed(4638) in these notes, which ensures that these notes have the same content every time I run R. 2.2 Estimation 2.2.1 The likelihood function If we take a particular result, e.g. \\(Y=8\\) correct, it is straightforward to compute the probability of that result for different values of \\(\\theta\\). Earlier, we already used 1:10 to obtain a sequence of integers from 1 to 10. Not we ae going to use a more general function seq, to obtain a sequence of possible values for \\(\\theta\\) between 0 and 1. We will then supply this sequence as the prob argument to calculate the probability of the result according to each. possible_theta &lt;- seq(0,1,length=100) lik_theta &lt;- dbinom(8,size=8,possible_theta) plot(x=possible_theta,y=lik_theta,type=&quot;l&quot;) Instead of the first argument in dbinom being 8, you can try this for different values. The maximum likelihood estimate \\(\\hat{\\theta} = \\frac{k}{n}\\) is very simple to compute. Moreover, computations with R are vectorized. For instance, if Y is a vector, then Y/8 divides each element of Y by 8. And if Y and X are vectors of the same length, then Y/X divides each element of Y by the corresponding element of X. So, to compute the maximum likelihood estimate of \\(\\theta\\) for each of the simulated replications of the experiment, we can simply run: theta_est &lt;- Y/8 which stores the estimates in the object theta_est. 2.2.2 Calculating the likelihood ratio Having computed the estimate \\(\\hat{\\theta}\\) for each simulated dataset, we can also straightforwardly compute the likelihood ratio for each data set, comparing e.g. our MODEL R where we assume \\(\\theta = 0.5\\) to MODEL G where we use \\(\\theta = \\hat{\\theta}\\) instead. First, let’s caclulate the likelihood of each simulated dataset according to MODEL R. Again, we will make use of the fact that many functions in R are vectorized, such that if we ask R to calculate the probability for a vector of outcomes Y, the result is a vector with the probabilities for each element in Y likelihood_R &lt;- dbinom(Y, size=8, prob=.5) Remember, if you want to see what is in an R object, you can just type in the name: likelihood_R ## [1] 0.21875000 0.27343750 0.21875000 0.10937500 0.21875000 0.10937500 ## [7] 0.27343750 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 ## [13] 0.27343750 0.21875000 0.21875000 0.27343750 0.10937500 0.21875000 ## [19] 0.10937500 0.21875000 0.00390625 0.21875000 0.21875000 0.21875000 ## [25] 0.03125000 0.10937500 0.21875000 0.27343750 0.10937500 0.21875000 ## [31] 0.21875000 0.27343750 0.27343750 0.03125000 0.21875000 0.21875000 ## [37] 0.21875000 0.21875000 0.10937500 0.00390625 0.21875000 0.21875000 ## [43] 0.21875000 0.10937500 0.21875000 0.21875000 0.21875000 0.21875000 ## [49] 0.27343750 0.21875000 0.27343750 0.10937500 0.10937500 0.27343750 ## [55] 0.21875000 0.21875000 0.21875000 0.27343750 0.27343750 0.21875000 ## [61] 0.21875000 0.21875000 0.21875000 0.21875000 0.10937500 0.21875000 ## [67] 0.27343750 0.00390625 0.03125000 0.10937500 0.27343750 0.21875000 ## [73] 0.27343750 0.27343750 0.21875000 0.10937500 0.21875000 0.27343750 ## [79] 0.21875000 0.27343750 0.03125000 0.21875000 0.03125000 0.21875000 ## [85] 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 0.27343750 ## [91] 0.21875000 0.21875000 0.21875000 0.10937500 0.27343750 0.27343750 ## [97] 0.21875000 0.21875000 0.03125000 0.10937500 We can follow the same procedure to calculate the likelihood for each simulated dataset according to the estimated MODEL G. In this case though, we are going to use two vectorized arguments simultaneously. For each simulated dataset, we have an observed number of correct predictions (in our object Y), as well as a corresponding estimate of the probability of a correct response (in our object theta_est). If we supply each as arguments to the dbinom function, R will match each element in Y with the corresponding element of theta_est, i.e. Y[1] is matched with theta_est[1], y[2] with theta_est[2], etc. You will have to be quite certain that the indices (i.e., the number between the brackets, such as [1] and [2]) of one argument correspond to the indices of another to get the correct results. In this case, we computed theta_est directly from Y, and therefore we know each value in Y corresponds to the value of theta_est = y/8. Sowe can safely compute the likelihood of MODEL G with twp vectorised arguments as: likelihood_G &lt;- dbinom(Y, size=8, prob = theta_est) To compute the likelihood ratio for all these simulated datasets, we can now simply divide each element in likelihood_R by the corresponding element in likelihood_G: likelihood_ratio &lt;- likelihood_R/likelihood_G Great! We now have a distribution of likelihood ratio values, simulated from MODEL R. We can get closer and closer to the sampling distribution of the likelihood ratio values by increasing the number of simulated datasets. Here, I’m only simulating 100 datasets, but you can easily simulate many more. Let’s have a look at the distribution of likelihood ratio values with a histogram: hist(likelihood_ratio) Compared to an infinite number of datasets, 100 is quite a small number, but nevertheless the histogram already looks quite a bit like the true distribution of the likelihood ratio under MODEL R. 2.3 Hypothesis testing directly with the binomial distribution To perform hypothesis tests directly with the binomial distribution, you can use the binom.test function. This function has 5 arguments: x: the number of “successes” (e.g., the number of correct guesses). n: the number of trials (e.g. the total number of guesses) p: the probability of success assumed under the null hypothesis (e.g. \\(\\underline{\\theta}\\)). The default value is p = 0.5. alternative: the range of values for p (i.e. \\(\\theta\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\theta &gt; \\underline{\\theta}\\) allowed), or less (only values \\(\\theta &lt; \\underline{\\theta}\\) allowed). The default value is alternative = \"two.sided\". conf.level: the confidence level for the returned confidence interval. Should be specified as a probability (i.e., to get the 95% confidence interval, you should specify conf.level = 0.95). The default value is conf.level = 0.95. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(0 \\leq \\theta \\leq 1\\) corresponds to a two-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta \\neq 0.5\\). This test can be performed as: binom.test(x=8, n = 8, p = 0.5) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.007812 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.6305834 1.0000000 ## sample estimates: ## probability of success ## 1 The output provides the estimated \\(\\theta\\) for MODEL G under sample estimates: probability of success. Remember that the p-value of the test reflects the probability of the provided number of successes or more extreme ones under the null hypothesis. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(\\theta &gt; 0.5\\) corresponds to a one-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta &gt; 0.5\\). This test can be performed by binom.test(x=8, n = 8, p = 0.5, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.003906 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.687656 1.000000 ## sample estimates: ## probability of success ## 1 "],["the-one-sample-t-test.html", "Chapter 3 The one-sample t-test 3.1 Missing values 3.2 Selecting subsets of data 3.3 One-sample t-test 3.4 Effect size 3.5 Nonparametric bootstrap", " Chapter 3 The one-sample t-test You can open the anchoring data as follows: library(sdamr) data(anchoring) And view the first few rows of the data with the head function: head(anchoring) ## session_id sex age citizenship referrer us_or_international lab_or_online ## 1 2400853 f 18 US abington US In-lab ## 2 2400856 f 19 CN abington US In-lab ## 3 2400860 f 18 US abington US In-lab ## 4 2400868 m 18 US abington US In-lab ## 5 2400914 f 18 US abington US In-lab ## 6 2400916 f 18 US abington US In-lab ## anchor everest_feet everest_meters ## 1 high 30000 NA ## 2 high 30000 NA ## 3 low 5000 NA ## 4 low 2400 NA ## 5 low 10000 NA ## 6 high 25000 NA 3.1 Missing values Sometimes a data set has missing values. In R, a missing value is shown as NA (for Not Available). For example, you can see missing values in the everest_meters variable. In this case, these are generally due to most participants being asked to judge the height of Mount Everest in feet, rather than meters. If a variable has missing values, functions such as mean and sd will return a missing value, rather than a numeric value. For instance: mean(anchoring$everest_meters) ## [1] NA sd(anchoring$everest_meters) ## [1] NA The reason for this is that when there are missing values, it is not possible to compute or estimate the mean or standard deviation from all the data. To compute the values for just the non-missing cases, you could first delete all cases with missing values from your data.frame, for instance with the subset function discussed later. But I think it is better to keep the dataset as is, and use other ways to avoid issues with missing values. Luckily, many functions have arguments to deal with missing values. For instance, the two functions above have an na.rm argument (for not-available-remove). Setting this to TRUE will call the function only on the non-missing-values: mean(anchoring$everest_meters, na.rm=TRUE) ## [1] 6636.786 sd(anchoring$everest_meters, na.rm=TRUE) ## [1] 3942.714 Because the default in these functions is to set na.rm=FALSE, you will be aware of missing values in your data, which is useful, although having to then to explicitly set na.rm=TRUE can be a little annoying. 3.2 Selecting subsets of data There are two main ways to select a subset of observations in base R. You can either use an “indexing variable”, or use the subset function. I will discuss both below. 3.2.1 Indexing variables An indexing variable is used to specify the rows in a data.frame that you want to use. Generally, an indexing variable is a logical variable, which takes the value TRUE for cases (rows) that you want to include, and FALSE for cases that you want to exclude. Using an index variable, we will treat the data.frame as a matrix, which allows us to use square brackets, as in data[row,column] to either select rows or columns. For example, anchoring[,\"age\"] selects the column named “age” and returns it, while anchoring[1:10,] selects rows 1 to 10. The nice thing about R is that instead of providing row row numbers, we can create a logical variable based on the data itself to select rows. To do so, we can use the logical comparators and operators: == “equal to” != “not equal to” &gt; “greater than” &gt;= “greater than or equal to” &lt; “smaller than” &lt;= “smaller than or equal to” &amp; “and” | “or” Some examples of using index variables are as follows. An index variable which is TRUE for males and FALSE for females can be computed as follows: index &lt;- anchoring$sex == &quot;m&quot; Let’s see what this variable looks like: head(index) ## [1] FALSE FALSE FALSE TRUE FALSE FALSE It is indeed a logical variable which is TRUE whenever sex is equal to \"m\", and FALSE otherwise. You can use it to select all the males in the anchoring data by: dat &lt;- anchoring[index,] Note that you don’t have to create an index variable separately. You can obtain the same result by computing the index variable within the brackets, like so: dat &lt;- anchoring[anchoring$sex == &quot;m&quot;,] You can select all males over 30 years of age, and check the number of observations in this subset by the nrow function, as follows: dat &lt;- anchoring[anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;,] nrow(dat) ## [1] 361 You can select all participants who are male and over 30 years of age, or females who are female and over 30 years of age by: dat &lt;- anchoring[(anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;) | (anchoring$age &gt; 30 &amp; anchoring$sex == &quot;f&quot;),] 3.2.2 The subset function The subset function is quite similar to using index variables, but it doesn’t require the treatment of the data.frame as a matrix and it looks for variable names in the data.frame so you don’t have to use e.g. anchoring$ before the variable name. This makes the subset function a bit easier to use than using indexing variables. The subset function has the following arguments: * x: the object (e.g. the data.frame) for which you want to select a subset of cases * subset: a logical expression indicating elements or rows to keep * select: an optional expression which indicates which columns to select from a data frame I generally use just the first two arguments. We can replicate the selections above using the subset function as follows: dat &lt;- subset(anchoring, sex == &quot;m&quot;) dat &lt;- subset(anchoring, age &gt; 30 &amp; sex == &quot;m&quot;) dat &lt;- subset(anchoring, (age &gt; 30 &amp; sex == &quot;m&quot;) | (age &gt; 30 &amp; sex == &quot;f&quot;)) For more information on indexing and subsetting, have a look at e.g. http://www.cookbook-r.com/Basics/Getting_a_subset_of_a_data_structure/ The data analysed in the SDAM book was selected as follows: dat &lt;- subset(anchoring, anchor == &quot;low&quot; &amp; (referrer == &quot;swps&quot; | referrer == &quot;swpson&quot;)) Note the use of the brackets around the “or” argument. Here, we want to select those cases where the value of the anchor variable equals \"low\" and the value of the referrer variable equals \"swps\" or \"swpson\". Combinations of logical statements with the &amp; operator evaluate to TRUE when the elements on the left and right of it are evaluated as TRUE. By placing brackets around (referrer == \"swps\" | referrer == \"swpson\"), this part is TRUE whenever the value of the referrer variable equals \"swps\" or \"swpson\". Combining this with the left element, we then select those cases out of this subset for whom the anchor variable equals \"low\". Another way to get the same result is as: dat &lt;- subset(anchoring, (anchor == &quot;low&quot; &amp; referrer == &quot;swps&quot;) | (anchor == &quot;low&quot; &amp; referrer == &quot;swpson&quot;)) Knowing how logical statements are evaluated in computer languages is very important. I won’t pretend this is easy at first. But with practice (and probably many mistakes, like I have made and sometimes still make), you will get an intuitive understanding of it. And when in doubt, check that the results are as you expected, by for instance first assigning the outcome of your expression to a new variable in your data.frame and then inspecting the values for errors. For example, you could use something like tmp &lt;- anchoring ## assign a copy of the anchoring data to a &quot;throwaway object&quot; tmp$selected &lt;- tmp$anchor == &quot;low&quot; &amp; (tmp$referrer == &quot;swps&quot; | tmp$referrer == &quot;swpson&quot;) and then call ftable(tmp$anchor, tmp$referrer, tmp$selected) ## FALSE TRUE ## ## high abington 38 0 ## brasilia 32 0 ## charles 5 0 ## ithaca 39 0 ## jmu 70 0 ## ku 39 0 ## laurier 37 0 ## lse 133 0 ## luc 67 0 ## mcdaniel 48 0 ## msvu 18 0 ## mturk 440 0 ## pi 386 0 ## psu 43 0 ## qccuny 46 0 ## qccuny2 38 0 ## sdsu 77 0 ## swps 18 0 ## swpson 65 0 ## tamu 100 0 ## tamuc 36 0 ## tamuon 101 0 ## tilburg 17 0 ## ufl 59 0 ## unipd 46 0 ## uva 35 0 ## vcu 49 0 ## wisc 44 0 ## wku 44 0 ## wl 41 0 ## wpi 34 0 ## low abington 35 0 ## brasilia 44 0 ## charles 3 0 ## ithaca 38 0 ## jmu 91 0 ## ku 36 0 ## laurier 48 0 ## lse 127 0 ## luc 69 0 ## mcdaniel 40 0 ## msvu 31 0 ## mturk 472 0 ## pi 370 0 ## psu 44 0 ## qccuny 47 0 ## qccuny2 39 0 ## sdsu 72 0 ## swps 0 36 ## swpson 0 73 ## tamu 76 0 ## tamuc 43 0 ## tamuon 102 0 ## tilburg 48 0 ## ufl 60 0 ## unipd 56 0 ## uva 41 0 ## vcu 51 0 ## wisc 47 0 ## wku 54 0 ## wl 45 0 ## wpi 49 0 The ftable function is useful to obtain frequency tables in a slightly more readable format than through the table function. But as you can see, the output is still rather extensive. But we can clearly see that, as intended, the only cases for which the new selected variable is TRUE are all cases where referrer equals \"swps\" or \"swpson\". 3.3 One-sample t-test R has a t.test function which allows you to compute a variety of t-tests. For a one-sample t-test, you would use the following arguments: x: the variable for which to compute the t-test mu: the assumed value of the mean, i.e. \\(\\underline{\\mu}\\) alternative: similar as in binom.test, the range of values for mu (i.e. \\(\\mu\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\mu &gt; \\underline{\\mu}\\) allowed), or less (only values \\(\\mu &lt; \\underline{\\mu}\\) allowed). The default value is alternative = \"two.sided\". For instance, we can run the two-sided t-test also reported in the SDAM book by t.test(dat$everest_meters, mu=8848) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 1.558e-13 ## alternative hypothesis: true mean is not equal to 8848 ## 95 percent confidence interval: ## 5716.848 6907.537 ## sample estimates: ## mean of x ## 6312.193 A one-sided test where MODEL R assumes \\(\\mu = 8848\\) and MODEL G assumes that \\(\\mu &lt; 8848\\), is obtained by t.test(dat$everest_meters, mu=8848, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 7.791e-14 ## alternative hypothesis: true mean is less than 8848 ## 95 percent confidence interval: ## -Inf 6810.498 ## sample estimates: ## mean of x ## 6312.193 3.4 Effect size For a one-sample t-test, the computation of Cohen’s \\(d\\) is straightforward. You simply divide the difference between the mean and the assumed mean by the standard deviation: (mean(dat$everest_meters) - 8848)/sd(dat$everest_meters) ## [1] -0.8086795 Note the use of brackets around the difference between the sample mean and assumed mean. What we want to compute is \\[\\text{Cohen&#39;s } d = \\frac{\\overline{Y} - \\underline{\\mu}}{\\sigma_Y}\\] If I would have left the brackets out, R would compute the difference between the sample mean on the one hand, and the assumed mean divided by the standard deviation on the other hand: mean(dat$everest_meters) - 8848/sd(dat$everest_meters) ## [1] 6309.371 which is \\[\\overline{Y} - \\frac{\\underline{\\mu}}{\\sigma_Y}\\] and not what we intend to compute! Many errors are due to missing brackets. If you don’t want to calculate Cohen’s \\(d\\) in this way, you can also use the cohens_d function from the effectsize package (Ben-Shachar et al. 2022), which provides functions for a wide variety of effect-size measures. You can supply a t.test directly as an argument to the cohens_d function: effectsize::cohens_d(t.test(dat$everest_meters, mu=8848)) ## Cohen&#39;s d | 95% CI ## -------------------------- ## -0.81 | [-1.19, -0.59] ## ## - Deviation from a difference of 8848. The output does not only provide the value of Cohen’s \\(d\\) that we computed earlier, but also a confidence interval around this measure. In terms of the code, note that instead of first loading the effectsize package with library(effectsize), I called the function directly by prepending the cohens_d function with the package name, as in effectsize::cohens_d. Calling functions in this way without loading the package first is handy, because some packages may use the same name for different functions. If that is the case, when you call a function with that name, you will get the function as implemented in the package you loaded the last. This often leads to errors. Where possible, it is best to avoid loading packages, and always use functions without first loading a package by prepending the name of the function with the package name. 3.5 Nonparametric bootstrap (This section is rather advanced and you can skip it for now if you want. You may want to return to this if you need or want to apply a bootstrapping technique.) When discussing the Central Limit Theorem, I described how you can use bootstrapping to evaluate whether an assumed distribution for a statistic is likely to hold. Such a bootstrap analysis is quite easy to perform in R, although the idea itself is somewhat advanced. The key idea is to repeatedly sample, with replacement, from a given dataset, to obtain lots of new datasets, and then compute the statistic for those new dataset. To sample with replacement, the sample function in R comes in handy. In particular, you can use this function to sample the indices (from 1 to \\(n\\)) of observations in a dataset. For example, if I have a dataset with 10 observations, I might sample with replacement numbers between 1 and 10 as follows: set.seed(1234) sample(1:10, size = 10, replace = TRUE) ## [1] 10 6 5 9 5 6 4 2 7 6 The first argument to the sample function is the set of elements you want to sample from (numbers in the sequence from 1 to 10 in this case), the second specifies the number of samples (10 in this case), and the third specifies that we sample with replacement, which means that we can sample the same element multiple times. Here, you can see that the number 3 occurs 3 times in our new sample, and the number 10 twice. If we then use these numbers to select observations in our dataset, the new dataset would replicate observation 3 three times, and observation 10 two times. Such replications of observations is exactly what we want in a nonparametric bootstrap. One way to get a bootstrap distribution of the sample mean, by sampling nsim new datasets of the same size as the original dataset (given as \\(n\\)=nrow(dat)) would be as follows: set.seed(23456) nsim &lt;- 100000 # number of new datasets boot_mean &lt;- rep(0.0,nsim) # initialize a vector to store the means in the new datasets for(i in 1:nsim) { # iterate for each bootstrap sample y &lt;- dat[sample(1:nrow(dat),size=nrow(dat),replace=TRUE),]$everest_meters # sample data boot_mean[i] &lt;- mean(y) # compute mean and store it in an element of boot_mean } R is rather slow in performing for loops. It is more efficient to generate all the y samples for all simulations in one go. Instead of repeatedly sampling size=nrow(dat) samples, we can sample the indices for all bootstrap samples in one go as size=nsim*nrow(dat). If we then place the sampled y values in a matrix, we can compute the mean for each column in that matrix to get our boot_mean variable. This is how I performed the bootstrapped distribution of the sample mean: set.seed(23456) nsim &lt;- 100000 # number of new datasets boot_mean &lt;- rep(0.0,nsim) # initialize a vector to store the means in the new datasets y &lt;- matrix(dat[sample(1:nrow(dat),size=nsim*nrow(dat),replace=TRUE),]$everest_meters,ncol=nsim) boot_mean &lt;- colMeans(y) I then plotted the histogram with the overlaid theoretical distribution as follows: ggplot(data.frame(mean=boot_mean), aes(x=mean)) + geom_histogram(binwidth=30) + stat_function(fun = function(x) dnorm(x, mean = mean(boot_mean), sd = sd(boot_mean)) * 30 * nsim, linetype=2) + ylab(&quot;count&quot;) Note the use of the stat_function in which I supplied a function to compute the Normal density function with a mean and standard deviation equal to those of the boot_mean variable. This then plots the density of that variable, assuming it follows a Normal distribution. We can also plot the Q-Q plot as follows: ggplot(data.frame(mean=boot_mean), aes(sample = mean)) + stat_qq() + stat_qq_line() + ylab(&quot;sample&quot;) + xlab(&quot;theoretical&quot;) To get a bootstrapped distribution of the \\(t\\)-statistic, we can follow a mostly similar procedure. In this case, we want to determine the distribution of the \\(t\\)-statistic assuming that the null-hypothesis is true. To get this, rather than testing against \\(\\underline{\\mu} - 8848\\), we set \\(\\underline{\\mu} = \\overline{Y}\\) here. The reasoning behind this is that \\(\\overline{Y}\\) is our best estimate of \\(\\mu\\), whilst the assumed value \\(\\underline{\\mu}\\) might be completely wrong. If the distribution of the \\(t\\)-statistic with our best guess \\(\\underline{\\mu} = \\overline{Y}\\) follows a t-distribution, we could assume it would also follow a t-distribution if the null-hypothesis were true. The code to get the bootstrapped distribution of the \\(t\\)-statistic is: set.seed(23456) nsim &lt;- 100000 # number of new datasets boot_t &lt;- rep(0.0,nsim) s_mean &lt;- mean(dat$everest_meters) y &lt;- matrix(dat[sample(1:nrow(dat),size=nsim*nrow(dat),replace=TRUE),]$everest_meters,ncol=nsim) boot_t &lt;- apply(y, MARGIN=2, FUN=function(x) t.test(x, mu=s_mean)$statistic) Note the use of the apply function on the last line. The apply function can be used to apply any function provided in the FUN argument over a particular (set) of dimensions in an array. In this case, y is a matrix, and specifying MARGIN=2 means that we apply the function over each column in that matrix (setting the argument to MARGIN=1 would apply the function to each row in the matrix). The function that we apply to each column of y (which is each bootstrapped sample) is the t.test function, and we only store the value of the statistic variable of the output of that function (which is the value of the \\(t\\)-statistic). We can plot the bootstrap distribution with the theoretical distribution overlaid as follows: ggplot(data.frame(t=boot_t), aes(x=t)) + geom_histogram(binwidth=.1) + stat_function(fun = function(x) dt(x, df = nrow(dat) - 1) * .1 * nsim, linetype=2) + ylab(&quot;count&quot;) and the Q-Q plot as: ggplot(data.frame(t=boot_t), aes(sample = t)) + stat_qq(distribution = stats::qt, dparams = list(df = nrow(dat) - 1)) + stat_qq_line(distribution = stats::qt, dparams = list(df = nrow(dat) - 1)) + ylab(&quot;sample&quot;) + xlab(&quot;theoretical&quot;) References "],["regression.html", "Chapter 4 Regression 4.1 Estimating an testing a simple regression model 4.2 Model comparisons 4.3 Estimating and testing a multiple regression model 4.4 Residuals and predicted values 4.5 Plotting pairwise scatterplots for many variables 4.6 Multicollinearity and outlier detection", " Chapter 4 Regression 4.1 Estimating an testing a simple regression model Regression analysis is done through the lm function, with the following syntax: lm(formula, data,...). The first argument is called formula and expects a symbolic description of your model. I will tell you more about how to specify models with the formula syntax later, when we discuss moderation. For now, a few simple examples will suffice. To specify a simple regression model where you predict a dependent variable y by a predictor x, you would use the formula y ~ x On the left-hand side of the formula you need to provide the name of the dependent variable. After the name of the dependent variable, you need to put a tilde (~) (which you can read is “is modelled as a function of”). On the right-hand side, you then provide the name of the predictor. R will automatically include an intercept in the model. In R, the intercept is actually represented as a special predictor which always (for every row in the data set) has the value 1. The formula above is actually interpreted as y ~ x + 1 Because the intercept is included in most models, the authors of R have decided to save the you trouble of typing + 1 in each formula, by making this part of the formula implicit. You can fit a model without an intercept (which is the same as fixing the value of the intercept to 0), by instead of + 1, putting - 1 in the formula, as in y ~ x - 1 The second argument of the lm function is called data and expects the name of the data.frame in which the variables are stored. To see how the lm function works in practice, let’s open the trump2016 data provided in the sdamr package. This is the data analysed in Chapter 4 and 5. We open and inspect the data as usual: library(sdamr) # load the Trump data data(&quot;trump2016&quot;) # remove the data from the District of Columbia (Washintgon D.C.) dat &lt;- subset(trump2016,state != &quot;District of Columbia&quot;) head(dat) ## state hate_groups population hate_groups_per_million ## 1 Alabama 27 4863300 5.55 ## 2 Alaska 0 741894 0.00 ## 3 Arizona 18 6931071 2.60 ## 4 Arkansas 16 2988248 5.35 ## 5 California 79 39250017 2.01 ## 6 Colorado 16 5540545 2.89 ## percent_bachelors_degree_or_higher percent_in_poverty percent_Trump_votes ## 1 15.4 18.5 62.9 ## 2 29.7 10.4 52.9 ## 3 27.7 17.4 49.5 ## 4 21.8 18.7 60.4 ## 5 32.3 15.4 32.7 ## 6 39.2 11.5 44.4 You can see that there are a number of variables in the dataset (and not all of these were analysed in the book). For more information on the variables in the dataset, you can call the help file with ?trump2016. Now let’s estimate a simple regression model to predict the percentage of votes for Trump by the number of hate groups per million citizens: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million, data=dat) modg ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Coefficients: ## (Intercept) hate_groups_per_million ## 42.9 2.3 I’ve named the resulting object modg for MODEL G. You can pick any name you like for R objects. Note that when you just print a fitted linear model (by e.g., typing the name of the object modg), R will show the parameter estimates, but nothing else. You can get the important statistics by calling the summary function on the fitted model: summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.8206 -5.8570 -0.0529 5.2632 20.7883 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.8968 2.4094 17.804 &lt; 2e-16 *** ## hate_groups_per_million 2.3004 0.6715 3.426 0.00127 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.142 on 48 degrees of freedom ## Multiple R-squared: 0.1965, Adjusted R-squared: 0.1797 ## F-statistic: 11.74 on 1 and 48 DF, p-value: 0.001265 This provides quite a lot of useful information. The output of the summary function consists of four parts: Call simply shows you the call to the lm function used to fit the model (including the model formula) Residuals shows you some summary statistics for the prediction errors of the estimated model (which are often referred to as residuals) Coefficients shows you a table with: the name of variable for which the parameter was estimated Estimate: the estimated parameters Std. Error: the standard error of the estimates (this is the standard deviation of the sampling distribution of the estimates) t value: the t statistic of the hypothesis test that the true value of the parameter is equal to 0. Pr(&gt;|t|): the p-value, which is the probability that, given that the null hypothesis is true (i.e. the true value of the parameter is equal to 0), you would find a t-statistic at least as extreme as the one computed found for this data. Some overall model statistics: Residual standard error: this is and unbiased estimate of the standard deviation of the errors. Multiple R-squared: the \\(R^2\\) or proportion of variance of the dependent variable “explained” by the model. Adjusted R-squared: an unbiased estimate of the true value of \\(R^2\\) F-statistic: the results of a model comparison comparing the estimated model (MODEL G) to a MODEL R which only includes an intercept. 4.2 Model comparisons Comparing regression models and computing the \\(F\\) statistic can be done through the anova() function. Let’s first estimate a restricted version of MODEL G above where we fix the slope of hate_groups_per_million to 0. This MODEL R is identical to a model with only an intercept. We can estimate this by not providing any predictor names, but now explicitly providing the intercept term 1. # fit a MODEL R with only an intercept modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) We can then compute the \\(F\\) test by entering this MODEL R, and the MODEL G we estimated earlier, as arguments in the anova function: anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 48 4011.4 1 980.78 11.736 0.001265 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output lists the formula’s of the models we compare, and then provides a table with test results. The columns in this table are * Res.Df: the denominator degrees of freedom, i.e. \\(n=-\\text{npar}(M)\\) * RSS: the “residual sum of squares” or Sum of Squared Error of the model, i.e. \\(\\text{SSE}(M)\\) * Df: the numerator degrees of freedom,,i.e. $(G) - (R) * Sum of Sq: the reduction in the Sum of Squared Error, i.e. \\(\\text{SSE}(R) - \\text{SSE}(R)\\) * F: the \\(F\\) statistic of the test * Pr(&gt;F): the p-value of the test. We can obtain a test for the intercept by fitting a different MODEL R, now without an intercept, and comparing it to MODEL G # fit a MODEL R without an intercept (through &quot; - 1&quot;) modr &lt;- lm(percent_Trump_votes ~ hate_groups_per_million - 1, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ hate_groups_per_million - 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 30501.7 ## 2 48 4011.4 1 26490 316.98 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output of the anova function isn’t particularly pretty. Also, if you want to do multiple model comparisons, first estimating models and then comparing them with the anova function becomes a little cumbersome. An easier way to obtain all the model comparisons is to use the Anova function from the car (Fox, Weisberg, and Price 2022) package to automatically construct different possible versions of MODEL R, each being one particular restriction of MODEL G which fixes the relevant parameter to 0. If you don’t have the car package installed yet, you need to install it first (e.g. by install.packages(\"car\"). You can then call: library(car) ## Loading required package: carData Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 26490.3 1 316.981 &lt; 2.2e-16 *** ## hate_groups_per_million 980.8 1 11.736 0.001265 ** ## Residuals 4011.4 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that it is important to give the type = 3 argument in the Anova function. This will construct MODEL R by fixing a single parameter to 0 in turn (i.e. first fixing \\(\\beta_0=0\\) and estimating all other parameters, then another model fixing \\(\\beta_1 = 0\\) for estimating all other parameters), etc. 4.3 Estimating and testing a multiple regression model To specify a multiple regression model for a dependent variable named y and with three predictors, named x1, x2, and x3, you would use the formula y ~ x1 + x2 + x3 This is similar to the earlier formula, but you now need to provide the names of all the predictors, separated by a + sign. For instance, we can fit a model with two predictors (which we will call modg, for MODEL G), as follows: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million + ## percent_bachelors_degree_or_higher, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.7268 -4.0621 0.0426 2.8937 15.8359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81.9945 6.1555 13.321 &lt; 2e-16 *** ## hate_groups_per_million 1.3138 0.5102 2.575 0.0132 * ## percent_bachelors_degree_or_higher -1.2187 0.1839 -6.625 3.03e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.643 on 47 degrees of freedom ## Multiple R-squared: 0.5845, Adjusted R-squared: 0.5668 ## F-statistic: 33.06 on 2 and 47 DF, p-value: 1.087e-09 the output of the summary function contains the same elements as before, but the table of coefficients now includes an additional row for percent_bachelors_degree_or_higher. Also, note that all the estimates are different, because the slopes reflect unique effects, and these differ compared to models with other predictors. Finally, I’d like to point out that the last row of the output contains the “whole model test”, which compares the estimated model to a model with only an intercept. Recall that the estimate of the intercept in this latter model equals the sample mean. So we are now comparing a model with two predictors to a model which predicts all values as the sample mean. The difference in the number of estimated parameters for this comparison is \\(\\text{npar}(G) - \\text{npar}(R) = 3 - 1 = 2\\). Hence, the degrees of freedom are \\(\\text{df}_1 = 2\\) and \\(\\text{df}_2 = n - \\text{npar}(G) = 50 - 3 = 47\\). We can also get all the model comparisons for this MODEL G through: Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 7830.7 1 177.4381 &lt; 2.2e-16 *** ## hate_groups_per_million 292.7 1 6.6318 0.01322 * ## percent_bachelors_degree_or_higher 1937.2 1 43.8956 3.028e-08 *** ## Residuals 2074.2 47 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Finally, we can also obtain a “whole model test”, by comparing an intercept-only MODEL R to the full MODEL G. This is best done through the anova function as follows: modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 47 2074.2 2 2918 33.06 1.087e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4 Residuals and predicted values You can obtain the prediction errors by the calling the residuals function on the fitted model. # store the residuals as errorg errorg &lt;- residuals(modg) head(errorg) ## 1 2 3 4 5 6 ## -7.618118 7.101376 -2.152059 -2.055564 -12.570794 6.382163 This returns a vector with, for each case in the data (each row in the data frame), the error term \\(\\hat{\\epsilon}_i\\). Note that we are only displaying the first six elements through the head function. You can obtain the predicted values by calling the predict function on the fitted model. # store the predictions as predictg predictg &lt;- predict(modg) head(predictg) ## 1 2 3 4 5 6 ## 70.51812 45.79862 51.65206 62.45556 45.27079 38.01784 This returns a vector with for, each case in the data, the predicted value \\(\\hat{Y}_{M,i}\\). You can use these variables to create e.g. a histogram of the errors: hist(errorg) and a predicted by residual plot # scatterplot of predicted vs residual plot(predictg, errorg, xlab = &quot;predicted&quot;, ylab = &quot;residual&quot;) # add a horizontal line (h=0 is for horizontal at 0, # and lty = 3 makes it a dotted line abline(h=0,lty=3) You can also call the plot function directly on the fitted model, which produces a range of plots to assess the model assumptions: plot(modg) 4.5 Plotting pairwise scatterplots for many variables A final tip relates to exploring relations between many variables (e.g. potential predictors and dependent variables). While you can inspect pairwise relations between variables by creating a scatterplot for each pair of variables, this quickly becomes tedious. You can save yourself some work by using a function that produces a matrix of pairwise scatterplots directly. One option for this is to use the pairs function, and supply this with a selection of variables in a data.frame. For instance, in the data set we considered now, we might be interested in the relations between hate_groups_per_million, percent_bachelors_degree_or_higher, percent_in_poverty, and percent_Trump_votes. We can obtain a matrix of all pairwise scatterplots between these variables as follows (note that rather than typing the variable names, I’m selecting column 4 to 7, which correspond to these variables): pairs(dat[,4:7]) If you don’t like the look of these base R graphics and prefer ggplot2, you can use the ggpairs function from the GGally (Schloerke et al. 2021) package to get a similar plot: library(GGally) ggpairs(dat[,4:7]) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 4.6 Multicollinearity and outlier detection The car package provides useful functions to assess multicollinearity. In particular, it provides the vif function to compute the Variance Inflation Factor (VIF) for each predictor in the data. modg_vif &lt;- car::vif(modg) modg_vif ## hate_groups_per_million percent_bachelors_degree_or_higher ## 1.093119 1.093119 To compute the tolerance, you can simply transform the results from this function, as follows: modg_tolerance &lt;- 1/modg_vif modg_tolerance ## hate_groups_per_million percent_bachelors_degree_or_higher ## 0.9148138 0.9148138 There are a large number of measures to detect potential outliers. You can get many of these with the influence.measures function, which is provided in the (base R) stats package. Let’s try this with the full data set (including the District of Columbia): modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=trump2016) influence.measures(modg) ## Influence measures of ## lm(formula = percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data = trump2016) : ## ## dfb.1_ dfb.h___ dfb.p___ dffit cov.r cook.d hat inf ## 1 -0.52688 -0.296032 0.521272 -0.56266 1.159 1.04e-01 0.1676 ## 2 0.00392 -0.061502 0.020901 0.09322 1.087 2.94e-03 0.0349 ## 3 -0.03341 0.004670 0.019253 -0.06573 1.077 1.46e-03 0.0225 ## 4 -0.03771 -0.023320 0.035989 -0.04341 1.148 6.41e-04 0.0734 ## 5 0.06591 0.148803 -0.142949 -0.32192 0.876 3.27e-02 0.0277 ## 6 -0.24979 -0.153330 0.304489 0.36014 1.029 4.26e-02 0.0705 ## 7 -0.11843 -0.105623 0.150998 0.18404 1.122 1.14e-02 0.0753 ## 8 -0.01473 -0.013061 -0.007111 -0.12198 1.038 4.99e-03 0.0201 ## 9 2.52447 -5.302567 -1.681047 -6.87659 4.759 1.38e+01 0.8587 * ## 10 -0.02247 0.000377 0.011937 -0.05068 1.080 8.72e-04 0.0211 ## 11 0.00853 -0.004034 0.000226 0.03959 1.081 5.33e-04 0.0198 ## 12 0.08150 0.367500 -0.215915 -0.52466 0.747 8.21e-02 0.0399 * ## 13 0.08875 0.096324 -0.086731 0.13557 1.106 6.22e-03 0.0559 ## 14 0.03414 0.051211 -0.063462 -0.12988 1.054 5.67e-03 0.0274 ## 15 0.00905 0.003813 -0.007741 0.01140 1.105 4.42e-05 0.0366 ## 16 -0.02482 0.014259 0.013319 -0.04802 1.089 7.84e-04 0.0273 ## 17 -0.02606 -0.087778 0.083161 0.23956 0.943 1.86e-02 0.0239 ## 18 0.09668 0.060449 -0.090373 0.11583 1.114 4.55e-03 0.0571 ## 19 -0.03092 -0.008024 0.026492 -0.03570 1.113 4.34e-04 0.0445 ## 20 -0.00993 0.025266 -0.009797 -0.07984 1.069 2.16e-03 0.0218 ## 21 0.02121 0.012855 -0.026006 -0.03119 1.139 3.31e-04 0.0656 ## 22 -0.05632 -0.040191 0.067534 0.07593 1.194 1.96e-03 0.1105 * ## 23 -0.05253 0.002615 0.031027 -0.10310 1.058 3.58e-03 0.0222 ## 24 -0.04313 -0.051157 0.063607 0.09606 1.095 3.13e-03 0.0408 ## 25 -0.19099 -0.129701 0.186786 -0.22017 1.139 1.63e-02 0.0926 ## 26 0.05673 0.023999 -0.040637 0.10148 1.062 3.47e-03 0.0235 ## 27 0.08441 0.250403 -0.093250 0.30080 1.047 3.00e-02 0.0642 ## 28 0.03238 -0.057781 0.025177 0.24179 0.917 1.88e-02 0.0208 ## 29 -0.31423 0.029236 0.242506 -0.39419 0.882 4.90e-02 0.0399 ## 30 -0.08398 -0.019730 0.109755 0.17291 1.046 1.00e-02 0.0344 ## 31 -0.09407 -0.083653 0.121603 0.15189 1.116 7.80e-03 0.0652 ## 32 -0.19154 0.123269 0.103240 -0.36583 0.836 4.16e-02 0.0292 ## 33 0.05688 0.055368 -0.081363 -0.12081 1.083 4.93e-03 0.0386 ## 34 0.00256 -0.000673 -0.000647 0.00874 1.087 2.60e-05 0.0200 ## 35 0.06753 -0.143108 0.016610 0.32284 0.852 3.27e-02 0.0251 ## 36 -0.02617 -0.003339 0.018595 -0.04122 1.088 5.78e-04 0.0250 ## 37 0.16554 -0.022263 -0.122110 0.22264 1.011 1.64e-02 0.0344 ## 38 0.01776 0.035477 -0.042254 -0.10704 1.060 3.86e-03 0.0242 ## 39 -0.00413 0.001328 0.000514 -0.01652 1.086 9.28e-05 0.0198 ## 40 0.04014 0.090825 -0.075753 -0.14660 1.067 7.23e-03 0.0367 ## 41 0.01379 -0.001198 -0.009068 0.02285 1.091 1.78e-04 0.0249 ## 42 0.16164 0.235335 -0.162909 0.30237 1.030 3.02e-02 0.0581 ## 43 0.11823 0.095192 -0.108844 0.16490 1.072 9.14e-03 0.0433 ## 44 0.00868 -0.006344 -0.002861 0.02384 1.088 1.93e-04 0.0227 ## 45 0.00224 0.007721 -0.005479 -0.01283 1.101 5.61e-05 0.0327 ## 46 0.13369 0.127883 -0.176909 -0.22824 1.075 1.74e-02 0.0595 ## 47 -0.10655 -0.025968 0.131619 0.18477 1.059 1.14e-02 0.0423 ## 48 0.04786 0.044087 -0.072617 -0.12127 1.068 4.96e-03 0.0314 ## 49 0.16158 0.033660 -0.144034 0.17103 1.125 9.88e-03 0.0745 ## 50 -0.02618 0.027968 0.006136 -0.07926 1.074 2.13e-03 0.0240 ## 51 0.27179 0.076981 -0.211548 0.38638 0.801 4.58e-02 0.0280 * You can see that this provides a quite a few measures for each case (i.e. state) in the data! The first three are so-called DFBETAS measures, which reflect the standardized difference in the estimated parameters (intercept and slopes of hate_groups_per_million and percent_bachelors_degree_or_higher, respectively) that result from deleting that particular case from the data. The dffit column refers to the DFFITS measure shows the difference in the standardized predicted value for that case, after removing the case for the estimation of the model. The value in the cov.r column is the covariance ratio measure, which assesses the influence of a case on multicollinearity. Then the Cook’s distance is provided in the cook.d column, and the hat column provides the leverage values. Finally, the inf column flags potential outliers according to any of these measures. I would not advice you to blindly delete the flagged cases. These are cases which may be outliers. Personally, I use the Cook’s distance more than the other measures. You can obtain just the Cook’s distances with the cooks.distance function: modg_cooks_distance &lt;- cooks.distance(modg) # get a quick summary of the values summary(modg_cooks_distance) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000026 0.001169 0.004989 0.284643 0.018019 13.810613 The summary shows that there is at least one case with a Cook’s distance larger than 1. We can see which states have a large Cook’s distance as follows: trump2016$state[which(modg_cooks_distance &gt; 1)] ## [1] &quot;District of Columbia&quot; which indicates that the District of Columbia is likely an outlier. References "],["moderation-and-mediation.html", "Chapter 5 Moderation and mediation 5.1 Moderation in linear models 5.2 Centering 5.3 Mediation analysis", " Chapter 5 Moderation and mediation 5.1 Moderation in linear models Including an interaction in a linear model in R is straightforward. If you have two predictors, x1 and x2, and want to include both the “simple slopes” as well as the slope for the “product predictor” (i.e. x1 \\(\\times\\) x2), then the model with y as dependent variable can be specified in formula form as y ~ x1 * x2 which evaluates to y ~ 1 + x1 + x2 + x1:x2 As discussed previously, 1 represents the intercept, which is automatically included in a model specification, unless you remove it explicitly by adding -1 to the formula. x1 represents the “simple effect” of x1, x2 the corresponding “simple effect” of x2, whilst x1:x2 represents the product-predictor for the interaction between x1 and x2 (perhaps a little confusing for those of you who know : as a division operator). Because you would generally want to include the simple effects for the predictors as well as the interaction, the authors of R have chosen to save you typing in the full model by expanding x1 * x2 in this way. This can be used to specify more complicated models, with three-way interactions. For instance, y ~ x1 * x2 * x3 evaluates to y ~ 1 + x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3 which is a model with all “simple effects”, all pairwise product-predictors, as well as a three-way product predictor which is x1 \\(\\times\\) x2 \\(\\times\\) x3. We won’t discuss such higher-order interactions until later, but it is good to be aware of this in advance. As a linear model with a product-predictors is just another linear model, that is really all there is to say about specifying linear models with interactions/moderation in R. Through the formula interface, R will create the necessary additional product predictor(s), and then estimate the parameters of the resulting linear model. Let’s have a look at how this works with the speeddate data which was also analysed in the SDAM book. The data is included in the sdamr package, and can be loaded and (partially) inspected as usual: library(sdamr) data(&quot;speeddate&quot;) head(speeddate) ## iid pid gender age date_like other_like date_want other_want match self_attr ## 1 132 137 female 27 8 7 0 1 0 8 ## 2 132 138 female 27 8 8 0 1 0 8 ## 3 132 139 female 27 5 8 0 1 0 8 ## 4 132 140 female 27 9 7 1 1 1 8 ## 5 132 141 female 27 7 7 0 1 0 8 ## 6 133 137 female 24 7 7 0 0 0 6 ## self_sinc self_intel self_fun self_amb other_attr other_sinc other_intel ## 1 10 9 10 10 8 8 9 ## 2 10 9 10 10 8 7 4 ## 3 10 9 10 10 8 NA 8 ## 4 10 9 10 10 7 7 7 ## 5 10 9 10 10 8 7 7 ## 6 8 8 7 7 6 10 10 ## other_fun other_amb other_shar date_attr date_sinc date_intel date_fun ## 1 8 9 7 7 9 7 9 ## 2 6 6 4 8 8 8 8 ## 3 8 7 NA 5 7 9 5 ## 4 7 7 7 8 9 9 9 ## 5 7 7 8 5 8 8 8 ## 6 6 7 5 7 7 7 8 ## date_amb date_shar self_imp_attr self_imp_sinc self_imp_intel self_imp_fun ## 1 6 9 16.67 16.67 16.67 16.67 ## 2 8 8 16.67 16.67 16.67 16.67 ## 3 9 5 16.67 16.67 16.67 16.67 ## 4 9 8 16.67 16.67 16.67 16.67 ## 5 7 7 16.67 16.67 16.67 16.67 ## 6 6 8 12.77 19.15 17.02 17.02 ## self_imp_amb self_imp_shar other_imp_attr other_imp_sinc other_imp_intel ## 1 16.67 16.67 17.39 17.39 15.22 ## 2 16.67 16.67 20.00 20.00 20.00 ## 3 16.67 16.67 18.75 16.67 18.75 ## 4 16.67 16.67 18.60 16.28 18.60 ## 5 16.67 16.67 20.83 20.83 16.67 ## 6 14.89 19.15 17.39 17.39 15.22 ## other_imp_fun other_imp_amb other_imp_shar ## 1 17.39 13.04 19.57 ## 2 20.00 6.67 13.33 ## 3 20.83 12.50 12.50 ## 4 18.60 11.63 16.28 ## 5 16.67 6.25 18.75 ## 6 17.39 13.04 19.57 There are rather a large number of variables in the dataset. You can obtain more information about each variable in the documentation of the dataset, by calling ?speeddate. In my humble opinion, the (generally quite) good documentation of R packages is a real benefit of R over some other systems, and I strongly recommend you to check out and read the documentation of functions and datasets before you use them. Functions in R are generally quite flexible and it is infeasible to discuss all the nuances and possibilities in introductory notes like these. In the book, we mainly focused on the variables starting with other_, which are the perceptions of the participant by their dating partner. For example, the model \\[\\begin{align} \\texttt{like}_i =&amp; \\beta_0 + \\beta_{\\texttt{attr}} \\times \\texttt{attr}_i + \\beta_{\\texttt{intel}} \\times \\texttt{intel}_i + \\beta_{\\texttt{fun}} \\times \\texttt{fun}_i \\\\ &amp;+ \\beta_{\\texttt{attr} \\times \\texttt{intel}} \\times (\\texttt{attr} \\times \\texttt{intel})_i + \\beta_{\\texttt{fun} \\times \\texttt{intel}} \\times (\\texttt{fun} \\times \\texttt{intel})_i + \\epsilon_i \\end{align}\\] referred to in the SDAM book can be estimated by calling: modg &lt;- lm(other_like ~ other_attr*other_intel + other_fun*other_intel, data=speeddate) Hypothesis tests with the \\(t\\) statistic are obtained as usual though summary(modg) ## ## Call: ## lm(formula = other_like ~ other_attr * other_intel + other_fun * ## other_intel, data = speeddate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2380 -0.6632 0.0239 0.6583 4.6484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.91118 0.44269 -2.058 0.039740 * ## other_attr 0.67123 0.09027 7.436 1.76e-13 *** ## other_intel 0.32393 0.05994 5.405 7.57e-08 *** ## other_fun 0.14331 0.08734 1.641 0.101045 ## other_attr:other_intel -0.04333 0.01171 -3.700 0.000224 *** ## other_intel:other_fun 0.03186 0.01139 2.798 0.005209 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 1472 degrees of freedom ## (84 observations deleted due to missingness) ## Multiple R-squared: 0.6227, Adjusted R-squared: 0.6214 ## F-statistic: 485.8 on 5 and 1472 DF, p-value: &lt; 2.2e-16 The equivalent tests with the \\(F\\) statistic are easily obtained through car::Anova(modg,type=3) ## Anova Table (Type III tests) ## ## Response: other_like ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 5.43 1 4.2365 0.0397398 * ## other_attr 70.93 1 55.2931 1.755e-13 *** ## other_intel 37.47 1 29.2094 7.568e-08 *** ## other_fun 3.45 1 2.6923 0.1010448 ## other_attr:other_intel 17.56 1 13.6898 0.0002236 *** ## other_intel:other_fun 10.04 1 7.8287 0.0052093 ** ## Residuals 1888.24 1472 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.1.1 A reminder about namespaces In the code above, I’m using car:: to refer to a function in the car package. Technically, a statement like package_name:: denotes the namespace of the package with the name package_name (i.e., car in this case). This allows you to use functions from a package without loading the package completely (without loading all the functions of the package in memory). This can be (and often is!) better than first loading the package through e.g. library(car) and then calling Anova. The issue is that different packages can use the same name for a function, and when you call a function, it will be the one of the package that was last loaded. When packages use the same name for functions, the function with the same name from a package that was loaded earlier will be “masked” and R will print this as a warning in the R console. For example, if you load the dplyr package (a rather useful package for data manipulation, that is a little too much to discuss here in detail), you will see the following warnings: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union The line The following objects are masked from 'package:stats' indicates that the functions filter and lag are “masked from the stats package. Whenever you call these functions, they will be the corresponding functions from the dplyr package, and not those from the stats package. This does not break the functionality of the packages themselves, as a properly written package that needs the filter function from the stats package will still use the function from that namespace (package), and not the one from the dplyr one. But when you call the a function, R will try find the definition of that function in the global namespace, and the global namespace contains the version of the last provided definition of any R object. Just like you can overwrite an R object by giving it the same name as an already existing r object (as I often do deliberately through e.g. mod &lt;- lm()), a function in R is just another object. So if I specify a function like head &lt;- function(x, na.rm = FALSE) { return(&quot;Where&#39;s your head? It&#39;s almost Halloween!&quot;) } Then next time I call that function, I will get as a result head(speeddate) ## [1] &quot;Where&#39;s your head? It&#39;s almost Halloween!&quot; and not the result from utils::head(speeddate) 5.2 Centering As discussed in the SDAM book, sometimes you might want to center variables by subtracting their (sample) mean from each value. This can be done in a number of ways. You can either create new variables in a dataframe by subtracting the single value obtained through mean() from a vector, as in speeddate$other_like_c &lt;- speeddate$other_like - mean(speeddate$other_like, na.rm = TRUE) and then using the new variable other_like_c in your lm model. Alternatively, you can do by calling the scale function. By default, the scale function creates \\(Z\\) transformed variables by subtracting the mean and then dividing this mean-deviation by the standard deviation: \\[\\text{scale}(Y_i) = \\frac{Y_i - \\overline{Y}}{S_Y}\\] The scale function has three arguments: x: the variable (or matrix of variables) which you want to scale center: a logical value indicating whether you want to subtract the mean scale: a logical value indicating whether you want to divide by the standard deviation Centering (subtracting the mean, but not dividing by the standard deviation) is thus obtained by calling scale(x, scale=FALSE). Personally, I find calling a function scale with argument scale = FALSE a little confusing. The sdamr package therefore provides the function center which is basically just a version of scale which by default sets the argument scale = FALSE: center &lt;- function(x) { scale(x, center = TRUE, scale = FALSE) } Because R is a functional programming language, you can call the scale or center function directly within the call to the lm function. This saves you having to create centered variables in a dataframe first. For instance, if you have loaded the sdamr package (or if you ran the code above defining the center function), you can obtain the results of the model with centered predictors by calling modg_c &lt;- lm(other_like ~ center(other_attr)*center(other_intel) + center(other_fun)*center(other_intel), data=speeddate) summary(modg_c) ## ## Call: ## lm(formula = other_like ~ center(other_attr) * center(other_intel) + ## center(other_fun) * center(other_intel), data = speeddate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2380 -0.6632 0.0239 0.6583 4.6484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.19613 0.03151 196.609 &lt; 2e-16 *** ## center(other_attr) 0.34539 0.01922 17.967 &lt; 2e-16 *** ## center(other_intel) 0.25791 0.02351 10.970 &lt; 2e-16 *** ## center(other_fun) 0.38292 0.02094 18.287 &lt; 2e-16 *** ## center(other_attr):center(other_intel) -0.04333 0.01171 -3.700 0.000224 *** ## center(other_intel):center(other_fun) 0.03186 0.01139 2.798 0.005209 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 1472 degrees of freedom ## (84 observations deleted due to missingness) ## Multiple R-squared: 0.6227, Adjusted R-squared: 0.6214 ## F-statistic: 485.8 on 5 and 1472 DF, p-value: &lt; 2.2e-16 5.3 Mediation analysis In the SDAM book, we looked at mediation with a different dataset called legacy2015. This is also part of the sdamr package, and we can make it available and look at the initial cases as usual: data(&quot;legacy2015&quot;) head(legacy2015) ## [1] &quot;Where&#39;s your head? It&#39;s almost Halloween!&quot; Oops! We had overwritten (“masked”) the head function before. We can get rid of our new definition of the head function by removining it from the global namespace with the rm() function (the name refers to remove): rm(&quot;head&quot;) After this, we can try again: head(legacy2015) ## id sex age legacy belief intention education income donation ## 2 2 male 23 4.38 5.4 3.57 5 5 0 ## 3 3 female 59 4.75 5.2 2.43 5 6 0 ## 4 4 male 30 5.00 5.2 2.43 5 3 1 ## 5 5 female 25 2.88 5.0 2.00 4 3 0 ## 6 6 female 32 4.88 4.4 2.57 2 1 0 ## 7 7 male 38 5.12 4.6 3.71 3 2 7 5.3.1 Causal steps The causal steps approach to assessing mediation is done through testing significance in three regression models. This can be done straightforwardly with the lm() function which we have used quite a bit already. For instance, to assess whether the relation between legacy and donation is mediated by intention, we would estimate and test the parameters of the following models: mod1 &lt;- lm(donation ~ legacy, data = legacy2015) mod2 &lt;- lm(intention ~ legacy, data = legacy2015) mod3 &lt;- lm(donation ~ intention + legacy, data = legacy2015) 5.3.2 Investigating the moderated (indirect) effect with a bootstrap test There are quite a few packages in R which will allow you to test the moderated effect of a predictor on the dependent variable “via” the mediator. I have chosen here for the mediate function from the mediation package (Tingley et al. 2019), as it is a versatile option, and doesn’t require too much additional explanation. Another very good option to conduct mediation analysis is to specify the mediation model through a generalization of linear models generally called Structural Equation Models. These are multivariate models and not something we will cover in this course. For present purposes, this can be seen as a way to link different regression models (i.e. the dependent variable of one regression model becomes a predictor in another regression model) into what are conventionally called path models. The current “go-to” and most comprehensive SEM package in R is called lavaan and if you ever need to use this kind of analysis, that is my recommendation at the moment. So let’s focus on the mediation package for now. If you haven’t done so already, you will need to install it with install.packages(&quot;mediation&quot;) If you check the documentation of mediate in this package (i.e type ?mediation::mediate), you will see there are lots of arguments to specify. We will only focus one the ones important for present purposes: model.m: the name of the R object which contains the linear model predicting the mediator from the predictor, e.g. mod2 above model.y: the name of the R object which contains the linear model predicting the dependent variable from both the mediator and predictor, e.g. mod3 above sims: the number of simulations to use for the bootstrap test boot: (logical) whether to use a bootstrap test. You should set this to TRUE boot.ci.type: the type of bootstrap confidence interval to be computed. This can either be perc, which stands for percentile, and is a simple way where the empirical 2.5th and 97.5th percentiles are calculated from the ordered outcomes. This is the option chosen by default. The other option is bca which stands for bias-corrected and accelerated. This includes a correction of the percentile method to try and reduce bias. It is generally recommended to use this option in mediation analysis. Hence, you should set boot.ci.type = \"bca\" treat: the name of the predictor in the linear models specified under model.m and model.y. This is would be e.g. legacy in the models above mediator: the name of the mediator in the linear models specified under model.m and model.y. This is would be e.g. intention in the models above To run a bootstrap mediation test with 2000 simulations, we can run the following command: set.seed(20201027) med &lt;- mediation::mediate(model.m = mod2, model.y = mod3, sims = 2000, boot = TRUE, boot.ci.type = &quot;bca&quot;, treat = &quot;legacy&quot;, mediator = &quot;intention&quot;) ## Running nonparametric bootstrap summary(med) ## ## Causal Mediation Analysis ## ## Nonparametric Bootstrap Confidence Intervals with the BCa Method ## ## Estimate 95% CI Lower 95% CI Upper p-value ## ACME 0.245 0.104 0.45 &lt;2e-16 *** ## ADE 0.488 0.186 0.82 &lt;2e-16 *** ## Total Effect 0.733 0.425 1.07 &lt;2e-16 *** ## Prop. Mediated 0.334 0.155 0.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Sample Size Used: 237 ## ## ## Simulations: 2000 Note that rather than loading the mediation package with e.g. library(mediation) I’m calling the mediate function through mediation::mediate. Loading the mediation package results in loading quite a few other R packages, which are not all necessary to perform a mediation analysis with linear models. Calling the mediate function directly through the appropriate namespace avoids loading these other add-on packages (which then will mask some functions that I don’t want masked). The output from calling the summary function on the results of the bootstrap procedure (the R object I named med) has four rows: ACME: this is the average causal mediation effect, i.e. the average of \\(\\hat{a} \\times \\hat{b}\\) in the simulations. The value under estimate is the average, and you will also see the lower and upper bound of the 95% confidence interval under 95% CI Lower and 95% CI Upper respectively. Finally, the p-value is the probability of the found ACME or more extreme given that in reality, the ACME equals 0. I.e., this is the p-value of the hypothesis test that the true mediated effect equals 0. ADE: this is the average direct effect, and reflects the effect of the predictor which is not mediated. It is the average of \\(\\hat{c}&#39;\\) in the simulations. Total Effect is the total effect of the predictor on the dependent variable, which is the sum of the ACME and ADE. Prop. Mediated is the proportion of the effect of the predictor on the dependent variable which is mediated. This is ACME divided by Total Effect. The results under ACME show that the bootstrap confidence interval of the mediated effect does not include 0. The p-value for this effect is also smaller than \\(\\alpha = .05\\). As such, the null hypothesis that \\(a \\times b = 0\\) is rejected, and we have found evidence that the effect of legacy on donation is mediated by intention. Because the confidence interval of the ADE also does not include 0, this analysis indicates that the mediation is partial. There is also a significant direct effect of legacy on donation. About 33.4% of the effect of legacy on donation is mediated by intention, so the residual direct effect is quite substantial. References "],["contrast-coding-and-oneway-anova.html", "Chapter 6 Contrast coding and oneway ANOVA 6.1 Computing contrast-coded predictors 6.2 Assigning contrasts to factors", " Chapter 6 Contrast coding and oneway ANOVA There are several ways in which you can include nominal independent variables in the General Linear Model within R. The first option is to compute the contrast-coded predictors “by hand” and then enter these as metric predictors in the lm function. The second way is to specify the nominal variable as a factor and assign an appropriate contrast to this using the contrasts function. R will then compute the contrast-coded predictors to the factor “automatically” when you enter the factor as a predictor in the lm formula. Finally, you can also use the aov (Analysis Of Variance) function, or the Anova function. These functions are more focussed on omnibus tests rather than tests of the individual contrasts. We will discuss these options using the tetris2015 data, which comes with the sdamr package. There are many variables in this dataset (for a description, see ?tetris2015). The main variables used in the book are Days_One_to_Seven_Number_of_Intrusions (the number of memory intrusions after memory reactivation) and Condition. Let’s open the data and visualize the number of intrusions for the different conditions. library(sdamr) data(&quot;tetris2015&quot;) ## as the main DV has a cumbersome name, I&#39;m creating a copy of the dataset ## with a new variable &#39;intrusions&#39; which is a copy of Days_One_to_Seven_Number_of_Intrusions dat &lt;- tetris2015 dat$intrusions &lt;- dat$Days_One_to_Seven_Number_of_Intrusions set.seed(20201104) # to replicate figure with random jitter plot_raincloud(dat, intrusions, groups = Condition) ## Warning: Removed 4 rows containing missing values (`geom_segment()`). 6.1 Computing contrast-coded predictors Let’s first focus on data from a subset of the conditions, namely the Tetris+Reactivation and Reactivation-Only condition dat &lt;- subset(dat, Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)) Note the use of the %in% operator. The statement Condition %in% c(\"Tetris_Ractivation\",\"Reactivation\") returns TRUE whenever the value of Condition is equal to one of the values in the vector c(\"Tetris_Ractivation\",\"Reactivation\"). This is shorthand to the equivalent statement dat &lt;- subset(dat, Condition == &quot;Tetris_Reactivation&quot; | Condition == &quot;Reactivation&quot;) If there are lots of “or” values, using %in% can be a lot more efficient (in terms of the code you have to type, at least). Say that we would like to use a dummy coded predictor, with the value of 0 for Tetris_Reactivation, and the value of 1 for Reactivation condition. One way to compute such a variable is as follows: dat$dummy &lt;- 0 dat$dummy[dat$Condition == &quot;Reactivation&quot;] &lt;- 1 The variable dummy is first being created as a new column in dat, by appending the name of the new variable to the data.frame with the usual “$” notation, and then assigning a value to it. On the second line, I then select a subset of the values of dat$dummy (all cases where dat$Condition == \"Reactivation\") and assign the different value 1 to this subset. A quick check using the table function shows that we now indeed have a new variable with values 1 and 0: table(dat$dummy) ## ## 0 1 ## 18 18 We can now use this dummy variable like any other (metric) predictor in a linear model. mod &lt;- lm(intrusions ~ dummy, data=dat) summary(mod) ## ## Call: ## lm(formula = intrusions ~ dummy, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8333 -1.8333 -0.8333 1.1111 10.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.8889 0.6266 3.015 0.00484 ** ## dummy 2.9444 0.8861 3.323 0.00214 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.658 on 34 degrees of freedom ## Multiple R-squared: 0.2451, Adjusted R-squared: 0.2229 ## F-statistic: 11.04 on 1 and 34 DF, p-value: 0.00214 To compute other contrast-coded predictors, you can follow the same procedure. If a contrast-coded predictor only needs two values, then you can also use the ifelse function, which is a little less typing. The ifelse function has three arguments: a logical condition, the value to return when that condition is TRUE, and the value to return when that condition is FALSE. For instance, you can create a variable effect, with the value -.5 for Tetris_Reactivation and a value .5 for Reactivation, as dat$effect &lt;- ifelse(dat$Condition == &quot;Reactivation&quot;,.5,-.5) Here, when dat$Condition == \"Reactivation\", the condition is TRUE, and hence the value .5 is returned, otherwise (when the condition is not true, so Condition != \"Reactivation\"), the value -.5 is returned. As before, we can enter this as a predictor in an linear model as usual: mod &lt;- lm(intrusions ~ effect, data=dat) summary(mod) ## ## Call: ## lm(formula = intrusions ~ effect, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8333 -1.8333 -0.8333 1.1111 10.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3611 0.4431 7.586 8.2e-09 *** ## effect 2.9444 0.8861 3.323 0.00214 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.658 on 34 degrees of freedom ## Multiple R-squared: 0.2451, Adjusted R-squared: 0.2229 ## F-statistic: 11.04 on 1 and 34 DF, p-value: 0.00214 The procedure is easily extended to multiple contrast-coded predictors. For an example, let’s consider the full dataset with all four conditions. dat &lt;- tetris2015 dat$intrusions &lt;- dat$Days_One_to_Seven_Number_of_Intrusions Say that we want a set of orthogonal contrast codes Table 6.1: A set of orthogonal contrast codes. \\(c_1\\) \\(c_2\\) \\(c_3\\) Control \\(\\tfrac{3}{4}\\) 0 0 Tetris_Reactivation \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{2}\\) Tetris \\(-\\tfrac{1}{4}\\) \\(\\tfrac{2}{3}\\) 0 Reactivation \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(\\tfrac{1}{2}\\) The first contrast code has only two values, so we can use ifelse. The second and third have three possible values, and then we can’t use that function. # use ifelse dat$c1 &lt;- ifelse(dat$Condition == &quot;Control&quot;, 3/4, -1/4) # create c2 and c3 with default values and then use subsets for other values dat$c2 &lt;- 0 dat$c2[dat$Condition == &quot;Tetris&quot;] &lt;- 2/3 dat$c2[dat$Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)] &lt;- -1/3 # use sapply and switch dat$c3 &lt;- sapply(as.character(dat$Condition), switch, &quot;Control&quot; = 0, &quot;Tetris_Reactivation&quot; = -1/2, &quot;Tetris&quot; = 0, &quot;Reactivation&quot; = 1/2) When creating c3, I used a little R wizardry. The function sapply(X, FUN, ...) can be used to apply a function FUN to each element of a vector or list X. The argument X is that vector or list. As dat$Condition is a factor, but I want to use it as a character vector here, I’m using as.character to convert the factor in a character vector. The second argument FUN is the function you want to apply to the elements of X. I’m using the switch(EXPR, ...) function here. The sapply function will take each element in X and assign that to switch as the EXPR argument. Then any arguments specified as ... in the sapply function will be passed as additional arguments to the FUN function. In this case, what is specified under ... in the sapply function will be passed on to the ... argument of switch. For switch, the ... argument should be a list of alternative values of EXPR, with a corresponding return value. For instance, if EXPR == Control, the switch function will return 0. Using a combination of sapply and e.g. switch makes R a very powerful data manipulation tool. But the ins-and-outs of such applications will require practice. Alternatively, the dplyr package (which is part of the so-called tidyverse) has powerful functionality for data manipulation and data wrangling, which, with practice, are more straightforward to use than functions such as sapply in base R. A main reason for showing you sapply here is to show you the flexibility of R. There are many ways to obtain the same result. Which way you find most intuitive is a personal judgement. Getting back to the reason why we created the new variables in the first place, we can now use them as new predictors in a linear model modg &lt;- lm(intrusions ~ c1 + c2 + c3, data=dat) summary(modg) ## ## Call: ## lm(formula = intrusions ~ c1 + c2 + c3, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1111 -1.8889 -0.8333 1.1111 10.8889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9306 0.3743 10.502 7.11e-16 *** ## c1 1.5741 0.8643 1.821 0.073 . ## c2 0.5278 0.9168 0.576 0.567 ## c3 2.9444 1.0586 2.781 0.007 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.176 on 68 degrees of freedom ## Multiple R-squared: 0.1434, Adjusted R-squared: 0.1056 ## F-statistic: 3.795 on 3 and 68 DF, p-value: 0.01409 We can also obtain the equivalent \\(F\\) tests through the Anova function in the car package car::Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: intrusions ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 1112.35 1 110.2886 7.108e-16 *** ## c1 33.45 1 3.3165 0.072989 . ## c2 3.34 1 0.3314 0.566727 ## c3 78.03 1 7.7364 0.006996 ** ## Residuals 685.83 68 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To obtain an omnibus test for Condition (i.e. a test that all these slopes in modg are equal to 0), we can create a suitable intercept-only MODEL R and perform a model comparison as follows: modr &lt;- lm(intrusions ~ 1, data=dat) anova(modr, modg) ## Analysis of Variance Table ## ## Model 1: intrusions ~ 1 ## Model 2: intrusions ~ c1 + c2 + c3 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 71 800.65 ## 2 68 685.83 3 114.82 3.7948 0.01409 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the results of this model comparison were already provided in the output of summary(modg). 6.2 Assigning contrasts to factors Creating new variables in a dataset yourself gives you full control, but it can also be a bit cumbersome. Luckily, R has functionality build-in to assign contrasts to nominal variables. These nominal variables with associated contrast codes are factors. In the tetris2015 dataset, the Condition column is already a factor. In other datasets, a nominal variable might be a character vector. You would then first need to turn this into a factor by calling e.g. # This would be useful if dat$Condition is a character vector # it is not needed here! dat$Condition &lt;- as.factor(dat$Condition) Factors have contrast codes associated to them. In R, the default contrast code is dummy coding. You can view (and set) the contrasts via the contrasts() function. First, let’s have a look at what the contrast for dat$Condition looks like: contrasts(dat$Condition) ## Tetris_Reactivation Tetris Reactivation ## Control 0 0 0 ## Tetris_Reactivation 1 0 0 ## Tetris 0 1 0 ## Reactivation 0 0 1 The contrast is a matrix with each column representing a contrast code, and each row a level of the nominal variable. Remember, when there are four levels, we need three contrast codes. The default dummy coding uses the first level as the reference group, and then each contrast code represents a comparison of a later level to the reference level. You can choose your own contrast codes by assigning a matrix with contrast-code values to e.g. contrasts(dat$Condition). For instance, we can use the orthogonal contrast code defined earlier. In the code below, I first create the contrast matrix by combining columns with the cbind() function. You can give the columns names you find intuitive with the colnames function. codes &lt;- cbind(c(3/4,-1/4,-1/4,-1/4), c(0, -1/3, 2/3,-1/3), c(0, -1/2, 0, 1/2)) colnames(codes) &lt;- c(&quot;ctrl-vs-other&quot;,&quot;tetr-vs-memory&quot;, &quot;react-vs-t+r&quot;) contrasts(dat$Condition) &lt;- codes When we now call contrasts again, we can see are new contrast codes: contrasts(dat$Condition) ## ctrl-vs-other tetr-vs-memory react-vs-t+r ## Control 0.75 0.0000000 0.0 ## Tetris_Reactivation -0.25 -0.3333333 -0.5 ## Tetris -0.25 0.6666667 0.0 ## Reactivation -0.25 -0.3333333 0.5 A nice thing about the lm function is that you can also supply factors as predictors directly. Internally, the lm function will then create the necessary contrast-coded predictors from the contrasts supplied to the factor. Let’s try this: modg &lt;- lm(intrusions ~ Condition, data=dat) summary(modg) ## ## Call: ## lm(formula = intrusions ~ Condition, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1111 -1.8889 -0.8333 1.1111 10.8889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9306 0.3743 10.502 7.11e-16 *** ## Conditionctrl-vs-other 1.5741 0.8643 1.821 0.073 . ## Conditiontetr-vs-memory 0.5278 0.9168 0.576 0.567 ## Conditionreact-vs-t+r 2.9444 1.0586 2.781 0.007 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.176 on 68 degrees of freedom ## Multiple R-squared: 0.1434, Adjusted R-squared: 0.1056 ## F-statistic: 3.795 on 3 and 68 DF, p-value: 0.01409 You can see that the output (apart from the names of the effects) is exactly the same as when we created c1, c2, and c3. So that’s pretty neat! As I said, the lm function will create the contrast-coded predictors for factors. You can view the resulting “design matrix” (the matrix with values for all predictors actually used when estimating the parameters) with the model.matrix function (as the output is rather long, I’m calling this within the head function to only show the first few rows) head(model.matrix(modg)) ## (Intercept) Conditionctrl-vs-other Conditiontetr-vs-memory ## 1 1 0.75 0 ## 2 1 0.75 0 ## 3 1 0.75 0 ## 4 1 0.75 0 ## 5 1 0.75 0 ## 6 1 0.75 0 ## Conditionreact-vs-t+r ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 As you can see, the design matrix also includes a column for the intercept. The value of this column is 1 for every case in the data. If you think about it, you can view the intercept as the slope of a predictor variable which always has the value 1: \\[\\beta_0 = \\beta_0 \\times 1\\] 6.2.1 Default coding schemes In addition to assigning your own contrast codes, there are functions to create several “default” coding matrices. These are contr.treatment: dummy coding. contr.sum: effect-coding (sum-to-zero) contr.helmert: orthogonal contrast codes comparing each level of a factor to all levels before it. contr.poly: orthogonal contrast codes, usually used for ordinal levels. You can call each of these functions by specifying how many levels the factor has. E.g. for our Condition factor with four levels, the output of these functions is contr.treatment(4) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 contr.sum(4) ## [,1] [,2] [,3] ## 1 1 0 0 ## 2 0 1 0 ## 3 0 0 1 ## 4 -1 -1 -1 contr.helmert(4) ## [,1] [,2] [,3] ## 1 -1 -1 -1 ## 2 1 -1 -1 ## 3 0 2 -1 ## 4 0 0 3 contr.poly(4) ## .L .Q .C ## [1,] -0.6708204 0.5 -0.2236068 ## [2,] -0.2236068 -0.5 0.6708204 ## [3,] 0.2236068 -0.5 -0.6708204 ## [4,] 0.6708204 0.5 0.2236068 When you look at the output of these functions, you might notice that the scale of each can be different. For instance, in the contr.helmert function, the difference between the highest and lowest value ranges from 2 to 4. In the book, I used values such that a one-unit increase on a contrast-coded predictor reflects a difference between conditions. This convention is not followed in the various contr. functions. If you want default contrast coding schemes which follow this convention, making the parameters of the model a little easier to interpret, you can use the various contrast coding schemes implemented in the codingMatrices package (Venables 2022). this package also implements several other default coding schemes not implemented in the stats package. Instead of contr., this package provides contrast codes through function names starting with code_. For example, you can obtain a Helmert contrast with a different scaling as follows: library(codingMatrices) code_helmert(4) ## H2 H3 H4 ## 1 -0.5 -0.3333333 -0.25 ## 2 0.5 -0.3333333 -0.25 ## 3 0.0 0.6666667 -0.25 ## 4 0.0 0.0000000 0.75 Another nice feature of the codingMatrices package is the mean_contrasts() function, which will show you how the intercept of the resulting model is related to the group means, and how the slope of each contrast-coded predictor is a function of the group means. For instance mean_contrasts(code_helmert(4)) ## m1 m2 m3 m4 ## Ave 1/4 1/4 1/4 1/4 ## H2 -1 1 . . ## H3 -1/2 -1/2 1 . ## H4 -1/3 -1/3 -1/3 1 shows you in the row labelled Ave that the intercept is the sum of each group mean (in the columns) multiplied by \\(\\tfrac{1}{4}\\); i.e. it is the average of averages. The next row (labelled H2) shows you how the slope of the first contrast-coded predictor can be computed from the group means (as the difference between the second mean and the first mean). By contrast, the contr.helmert() function will provide the same intercept, but slopes that are fractions of these differences mean_contrasts(contr.helmert(4)) ## m1 m2 m3 m4 ## Ave 1/4 1/4 1/4 1/4 ## -1/2 1/2 . . ## -1/6 -1/6 1/3 . ## -1/12 -1/12 -1/12 1/4 References "],["factorial-anova.html", "Chapter 7 Factorial ANOVA 7.1 Rainclouds for factorial designs 7.2 Formulating, estimating, and testing a factorial ANOVA 7.3 Type 1, 2, and 3 Sums of Squares 7.4 Planned comparisons and post-hoc tests with emmeans 7.5 Testing general contrasts with emmeans", " Chapter 7 Factorial ANOVA In this chapter, we will look at factorial ANOVA, different ways of model comparisons (different SS types), and some ways to perform multiple comparisons and post-hoc tests. We will introduce these with the same data – from a study investigating the role of experimenter belief in social priming – used in Chapter 7 of SDAM. The dataset is available in the sdamr package as expBelief. We can load it from there, and inspect the first six cases, as usual: library(sdamr) data(&quot;expBelief&quot;) head(expBelief) ## pid exptrNum age gender yearInUni ethnicity englishFluency ## 1 1001 1 18 F 1 caucasian 7 ## 2 1003 1 18 F 1 CAUCASIAN 7 ## 3 1004 1 18 F 1 MiddleEastern 7 ## 4 1005 1 19 M 1 asian 5 ## 5 1006 1 21 M 1 china 5 ## 6 1007 1 19 F 1 caucasion 7 ## experimenterBelief primeCond powerPRE powerPOST ApproachAdvantage attractive ## 1 H LPP 56.1 68.8 -149.14290 6 ## 2 H LPP 98.2 89.0 -171.48810 1 ## 3 H HPP 46.5 70.2 -27.33083 5 ## 4 H LPP 58.8 44.8 374.15660 5 ## 5 L HPP 64.7 53.6 68.55114 5 ## 6 H HPP 61.4 50.3 -92.96860 5 ## competent friendly trustworthy ## 1 7 7 7 ## 2 3 5 6 ## 3 7 7 7 ## 4 5 6 5 ## 5 6 6 6 ## 6 7 7 7 7.1 Rainclouds for factorial designs The experiment had a 2 (social prime: low-power bs high power prime) by 2 (experimenter belief: low-power vs high-power manipulation). The two experimental factors are called primeCond and experimenterBelief in the data frame. The dependent variable we looked at is called ApproachAdvantage. We can use the plot_raincloud function from sdamr for plotting the data. The function has an argument groups which allows you to plot separate rainclouds for different levels of a grouping variable. In this case, we need four rainclouds. Because there is no variable to reflect the combinations of the levels of primeCond and experimenterBelief, we should create one first. The interaction() function is a useful function to create a new factor from the combinations of a set of given factors. As I’m going to make changes to the original dataset, I like to first create a new copy of the data fro this, so that I can later still use the original data set. # create a copy of expBelief and call it &quot;dat&quot; dat &lt;- expBelief dat$condition &lt;- interaction(dat$primeCond, dat$experimenterBelief) # show the levels of the newly created factor: levels(dat$condition) ## [1] &quot;HPP.H&quot; &quot;LPP.H&quot; &quot;HPP.L&quot; &quot;LPP.L&quot; You can see that the condition factor has four levels, which concatenate the levels of primeCond (which are LPP for low-power prime, and HPP for high-power prime) and experimenterBelief (which as L for when the experimenter is made to believe the participant received the low-power prime, and H for when the experimenter believed this was the high-power prime). We can now create a raincloud plot for the four conditions as follows: plot_raincloud(dat, ApproachAdvantage, groups=condition) In the book, I first changed the labels of the two variables before calling the interaction function. If you want the same plot as in the book, you could run the following code (which is not evaluated here): # turn primeCond and experimenterBelief in factors and change the labels dat$primeCond &lt;- factor(dat$primeCond, labels=c(&quot;PH&quot;,&quot;PL&quot;)) dat$experimenterBelief &lt;- factor(dat$experimenterBelief, labels=c(&quot;EH&quot;,&quot;EL&quot;)) # now create an interaction factor, and change the separation sign to &quot;-&quot; instead of &quot;.&quot; dat$condition &lt;- interaction(dat$primeCond, dat$experimenterBelief, sep=&quot;-&quot;) plot_raincloud(dat, ApproachAdvantage, groups=condition) The raincloud plot above effectively treats the design as a oneway design. If we want the plot to more directly reflect the factorial design, we can add some functionality from the ggplot2 package. In particular, we can use so-called facets, which basically allow you to repeatedly draw a similar plot for different levels of an independent variable. Because the plot_raincloud produces a raincloud plot by calling underlying ggplot2 functions, and the result is a ggplot, you can use any function from ggplot2 to make changes to the resulting plot. For instance, we can, within a plot, separate the levels of the experimenterBelief manipulation, and then create two panels (facets) for the levels of the primeCond condition. This is done as follows: plot_raincloud(dat, ApproachAdvantage, groups = experimenterBelief) + facet_grid(~primeCond) As usual, it pays to read the documentation for the facet_grid function (try calling ?facet_grid). There is an alternative for facet_grid, called facet_wrap, which provides slightly different labelling to the panels. facet_grid is particularly useful when you have two independent variables in a factorial design for which you would like to create different panels (we will show an example of this later). As we are considering factorial designs here, I chose to use facet_grid, but you can try facet_wrap as well. 7.2 Formulating, estimating, and testing a factorial ANOVA Formulating a factorial ANOVA model, where we distinguish between main effects and interactions, is not any different from formulating a moderated regression model. We can use the formula interface to indicate that we want to include predictors, as well as the product predictors required to assess interactions. In this case, however, we will enter nominal independent variables into the formula. When these are defined as factors with associated contrast codes, R will automatically expand the model to include the contrast-coded predictors, as well as all relevant product-predictors. The first thing to do is to make sure that the variables are defined as factors # check what type the two IVs are class(dat$primeCond) ## [1] &quot;character&quot; class(dat$experimenterBelief) ## [1] &quot;character&quot; # turn each into a factor dat$primeCond &lt;- as.factor(dat$primeCond) dat$experimenterBelief &lt;- as.factor(dat$experimenterBelief) # let&#39;s check the class for one of them to make sure class(dat$primeCond) ## [1] &quot;factor&quot; # that worked :-) Now let’s define appropriate contrast codes. As usual, it is a good idea to first check the existing contrast, as this shows the order of the factor levels: contrasts(dat$primeCond) ## LPP ## HPP 0 ## LPP 1 We can see that we need to define a single contrast, with the value for HPP (high-power prime) first and then the value for LPP (low-power prime) second. As the social priming hypothesis would predict the ApproachAdvantage score to be higher for LPP than for HPP, the following contrast makes sense: contrasts(dat$primeCond) &lt;- c(1/2, -1/2) contrasts(dat$primeCond) ## [,1] ## HPP 0.5 ## LPP -0.5 We define the contrast for experimenterBelief in the same way: contrasts(dat$experimenterBelief) ## L ## H 0 ## L 1 # H comes before l contrasts(dat$experimenterBelief) &lt;- c(1/2, -1/2) contrasts(dat$experimenterBelief) ## [,1] ## H 0.5 ## L -0.5 Now we are ready to estimate the linear model. To estimate a model with the main effects and interaction, we would use: modg &lt;- lm(ApproachAdvantage ~ primeCond*experimenterBelief, data=dat) Remember that this notation will expand the formula to ApproachAdvantage ~ 1 + primeCond + experimenterBelief + primeCond:experimenterBelief i.e. to a model with an intercept, a main effect of primeCond, a main effect of experimenterBelief, and an interaction primeCond:experimenterBelief. The easiest way to obtain the parameter estimates (and t-tests for those) is to use the summary function on this fitted linear model: summary(modg) ## ## Call: ## lm(formula = ApproachAdvantage ~ primeCond * experimenterBelief, ## data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -556.7 -176.3 10.6 169.8 605.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.46 10.72 2.002 0.046 * ## primeCond1 11.98 21.44 0.559 0.577 ## experimenterBelief1 90.51 21.44 4.221 3.02e-05 *** ## primeCond1:experimenterBelief1 37.64 42.88 0.878 0.381 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 214.4 on 396 degrees of freedom ## Multiple R-squared: 0.04565, Adjusted R-squared: 0.03842 ## F-statistic: 6.314 on 3 and 396 DF, p-value: 0.0003425 Alternatively, we can use the Anova function from the car package to obtain Type 3 (omnibus) tests: car::Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 184285 1 4.0084 0.04596 * ## primeCond 14343 1 0.3120 0.57678 ## experimenterBelief 819158 1 17.8175 3.018e-05 *** ## primeCond:experimenterBelief 35410 1 0.7702 0.38069 ## Residuals 18206049 396 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can see that while the Anova function reports \\(F\\) statistics, the tests and the corresponding \\(p\\)-values are identical. In this case, each factor only has two levels, and hence one contrast code. As there is only one parameter associated to each main effect and interaction (the slope of the single contrast-coded predictor for that effect), the omnibus test is a single parameter test. We will see an example where this is not the case shortly. 7.2.1 A threeway ANOVA We can also try to assess the experimenter effects by including this as an additional factor. Experimenter has four levels, so we’ll need three contrast codes for this variable. In the data.frame, Experimenter is included as exptrNum, which is a numerical variable. So we will first convert it into a factor class(dat$exptrNum) ## [1] &quot;integer&quot; dat$exptrNum &lt;- factor(dat$exptrNum, labels=paste0(&quot;E&quot;,1:4)) I’m using factor here rather than as.factor, because the former allows me to add labels to the levels, through the labels argument. Note that I’m using the paste0 function to create a vector with labels. This function can create combinations of (character) vectors, and is quite handy. The paste0 function is very similar to the paste function, but doesn’t include a space between the combinations: paste0(&quot;E&quot;,1:4) ## [1] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E4&quot; paste(&quot;E&quot;,1:4) ## [1] &quot;E 1&quot; &quot;E 2&quot; &quot;E 3&quot; &quot;E 4&quot; Right, so let’s define a contrast for exptrNum. contrasts(dat$exptrNum) ## E2 E3 E4 ## E1 0 0 0 ## E2 1 0 0 ## E3 0 1 0 ## E4 0 0 1 contrasts(dat$exptrNum) &lt;- cbind(c(-1/2,1/2,0,0), c(-1/3,-1/3,2/3,0), c(-1/4,-1/4,-1/4,3/4)) contrasts(dat$exptrNum) ## [,1] [,2] [,3] ## E1 -0.5 -0.3333333 -0.25 ## E2 0.5 -0.3333333 -0.25 ## E3 0.0 0.6666667 -0.25 ## E4 0.0 0.0000000 0.75 Before conducting the analysis, it is always a good idea to look at the data. Let’s create a slightly different raincloud plot than the one in Chapter 8 of SDAMR, now more explicitly reflecting the factorial nature of the design: plot_raincloud(dat, ApproachAdvantage, groups = experimenterBelief) + facet_grid(primeCond ~ exptrNum) This plot is not necessarily better than the one in SDAM. It does quite clearly highlight that experimenter belief does not seem to have an effect for Experimenter 4. However, personally, I find it more difficult to assess the effect of prime condition. For that, we could create a second plot to show the effect of primeCond within each panel: plot_raincloud(dat, ApproachAdvantage, groups = primeCond) + facet_grid(experimenterBelief ~ exptrNum) This indicates quite clearly that priming condition does not seem to have much of an effect for any experimenter of experimenter belief condition. Back to the analysis, then. We can estimate a threeway factorial ANOVA by simply adding another independent variable to the formula: modg_exp &lt;- lm(ApproachAdvantage ~ primeCond*experimenterBelief*exptrNum, data=dat) This formula is expanded to ApproachAdvantage ~ 1 + primeCond + experimenterBelief + exptrNum + primeCond:experimenterBelief + primeCond:exptrNum + experimenterBelief:exptrNum + primeCond:experimenterBelief:exptrNum In other words, the model includes all the main effects, all pairwise interactions between the factors, as well as the threeway interaction. We can see the parameter estimates and associated \\(t\\)-tests as usual through the summary function: summary(modg_exp) ## ## Call: ## lm(formula = ApproachAdvantage ~ primeCond * experimenterBelief * ## exptrNum, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -507.20 -179.77 11.34 167.78 589.70 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.401 10.742 1.992 0.0471 ## primeCond1 11.908 21.485 0.554 0.5797 ## experimenterBelief1 90.441 21.485 4.210 3.19e-05 ## exptrNum1 -7.411 30.387 -0.244 0.8075 ## exptrNum2 -5.076 26.312 -0.193 0.8471 ## exptrNum3 42.309 24.807 1.706 0.0889 ## primeCond1:experimenterBelief1 37.381 42.969 0.870 0.3849 ## primeCond1:exptrNum1 -60.323 60.774 -0.993 0.3215 ## primeCond1:exptrNum2 1.765 52.625 0.034 0.9733 ## primeCond1:exptrNum3 -21.445 49.613 -0.432 0.6658 ## experimenterBelief1:exptrNum1 -17.453 60.774 -0.287 0.7741 ## experimenterBelief1:exptrNum2 11.056 52.625 0.210 0.8337 ## experimenterBelief1:exptrNum3 -116.864 49.613 -2.356 0.0190 ## primeCond1:experimenterBelief1:exptrNum1 48.352 121.548 0.398 0.6910 ## primeCond1:experimenterBelief1:exptrNum2 -8.115 105.249 -0.077 0.9386 ## primeCond1:experimenterBelief1:exptrNum3 68.237 99.227 0.688 0.4921 ## ## (Intercept) * ## primeCond1 ## experimenterBelief1 *** ## exptrNum1 ## exptrNum2 ## exptrNum3 . ## primeCond1:experimenterBelief1 ## primeCond1:exptrNum1 ## primeCond1:exptrNum2 ## primeCond1:exptrNum3 ## experimenterBelief1:exptrNum1 ## experimenterBelief1:exptrNum2 ## experimenterBelief1:exptrNum3 * ## primeCond1:experimenterBelief1:exptrNum1 ## primeCond1:experimenterBelief1:exptrNum2 ## primeCond1:experimenterBelief1:exptrNum3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 214.8 on 384 degrees of freedom ## Multiple R-squared: 0.07105, Adjusted R-squared: 0.03476 ## F-statistic: 1.958 on 15 and 384 DF, p-value: 0.01719 Wow, there are a lot of estimates and tests here (16 in total)! While these tests are informative, it is common to (at least also) consider omnibus tests. Experimenter has four levels, so three associated contrasts, and we can’t find a test of the “overall” main effect of Experimenter in the output above. For these omnibus tests, we can (as before) use the Anova function from the car package: car::Anova(modg_exp,type=3) ## Anova Table (Type III tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 183165 1 3.9689 0.04705 * ## primeCond 14178 1 0.3072 0.57972 ## experimenterBelief 817797 1 17.7205 3.19e-05 *** ## exptrNum 138582 3 1.0010 0.39238 ## primeCond:experimenterBelief 34927 1 0.7568 0.38487 ## primeCond:exptrNum 54288 3 0.3921 0.75875 ## experimenterBelief:exptrNum 262131 3 1.8933 0.13018 ## primeCond:experimenterBelief:exptrNum 29490 3 0.2130 0.88738 ## Residuals 17721491 384 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you’d just consider the output from this function, which does not provide a significant Experimenter by Belief interaction, you probably would have missed the potentially interesting experimenterBelief1:exptrNum3 interaction, which was discussed in SDAM. 7.3 Type 1, 2, and 3 Sums of Squares Whilst intended (I think!) as a factorial experiment with an equal sample size for each priming condition, experimenter belief, and experimenter combination, the sample sizes are actually slightly unbalanced. One way to count the number of cases for each combination of factor levels is through the ftable function (which stands for frequency table). The function has various interfaces, and I find the formula interface easiest to work with. On the left-hand side of the formula, you can provide the name of a factor which you want to place in the columns of the table, and on the right-hand side you can include multiple factors which make up the rows, separated by a “+” sign: ftable(exptrNum ~ primeCond + experimenterBelief, data=dat) ## exptrNum E1 E2 E3 E4 ## primeCond experimenterBelief ## HPP H 26 25 25 25 ## L 25 25 25 25 ## LPP H 25 25 25 25 ## L 25 24 25 25 As you can see, Experimenter 1 tested 26 participants in the high-power prime and high experimenter belief condition, whilst experimenter 2 tested 24 participants in the low-power prime and low experimenter belief condition. The result of unbalanced data is that the contrast-coded predictors are no longer orthogonal. As a result, different ways of performing model comparisons will give different results. The differences are likely to be rather subtle here, because the sample sizes are mostly equal. Nevertheless, let’s consider how we can obtain results for the Type 1 and Type 2 SS procedures. A Type 2 procedure is easily obtained by using the car::Anova function, now with argument type=2: car::Anova(modg_exp,type=2) ## Anova Table (Type II tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## primeCond 14682 1 0.3181 0.5731 ## experimenterBelief 819687 1 17.7615 3.125e-05 *** ## exptrNum 138507 3 1.0004 0.3926 ## primeCond:experimenterBelief 34707 1 0.7520 0.3864 ## primeCond:exptrNum 53901 3 0.3893 0.7608 ## experimenterBelief:exptrNum 261854 3 1.8913 0.1305 ## primeCond:experimenterBelief:exptrNum 29490 3 0.2130 0.8874 ## Residuals 17721491 384 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you compare the results to those of the Type 3 procedure used earlier, you can see some subtle differences. You can see that (apart from the threeway interaction), the SSR terms are slightly different, leading to small differences in the \\(F\\) statistic and associated \\(p\\)-value. Unfortunately, the car::Anova function will not work with type=1. To get the results of a Type 1 procedure, you can use the anova function from the default stats package: anova(modg_exp) ## Analysis of Variance Table ## ## Response: ApproachAdvantage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## primeCond 1 14565 14565 0.3156 0.5746 ## experimenterBelief 1 820883 820883 17.7874 3.084e-05 ## exptrNum 3 138657 46219 1.0015 0.3921 ## primeCond:experimenterBelief 1 35260 35260 0.7640 0.3826 ## primeCond:exptrNum 3 54707 18236 0.3951 0.7566 ## experimenterBelief:exptrNum 3 261854 87285 1.8913 0.1305 ## primeCond:experimenterBelief:exptrNum 3 29490 9830 0.2130 0.8874 ## Residuals 384 17721491 46150 ## ## primeCond ## experimenterBelief *** ## exptrNum ## primeCond:experimenterBelief ## primeCond:exptrNum ## experimenterBelief:exptrNum ## primeCond:experimenterBelief:exptrNum ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 again, comparing this to the results of those obtained previously, you can see differences in the SSR terms. It is important to realise that the Type 1 procedure depends on the order of the factors in the formula. For instance, if we change this order as follows: anova(lm(ApproachAdvantage ~ experimenterBelief*exptrNum*primeCond, data=dat)) ## Analysis of Variance Table ## ## Response: ApproachAdvantage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## experimenterBelief 1 820878 820878 17.7873 3.084e-05 ## exptrNum 3 138443 46148 1.0000 0.3928 ## primeCond 1 14784 14784 0.3204 0.5717 ## experimenterBelief:exptrNum 3 263137 87712 1.9006 0.1290 ## experimenterBelief:primeCond 1 34782 34782 0.7537 0.3859 ## exptrNum:primeCond 3 53901 17967 0.3893 0.7608 ## experimenterBelief:exptrNum:primeCond 3 29490 9830 0.2130 0.8874 ## Residuals 384 17721491 46150 ## ## experimenterBelief *** ## exptrNum ## primeCond ## experimenterBelief:exptrNum ## experimenterBelief:primeCond ## exptrNum:primeCond ## experimenterBelief:exptrNum:primeCond ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 you get slightly different results. I should also mention the aov function from the stats package, which will also provide Type 1 ANOVA tests. I have wrestled with this function often when I started using R a long time ago. I’ve happily not used it for some time now, so will only mention its existence here. While the differences are very subtle here, this should not lead you to believe that the methods generally provide the same results. When the design is more unbalanced, the results can change quite dramatically. Finally, I want to point out again that all three procedures test the SSR terms against the same SSE term (the SS given under Residuals). This is the SSE of the full model (the model with all effects) and this is exactly the same for all three procedures. The procedures differ in how they compute the SSR terms for the different effects, as you can see. 7.4 Planned comparisons and post-hoc tests with emmeans The emmeans package (Lenth 2022) is very useful when you want to do more comparisons than can be implemented in the contrast codes within a single model, whether these are planned comparisons or post-hoc tests. The name of the package stands for estimated marginal means. One part of the functionality of the package is to compute the (unweighted) marginal means according to different types of models, including linear models. In the SDAM book, we discussed how these marginal means can be computed using contrast codes. For example, when using orthogonal contrasts, the intercept represents the grand mean, which is a simple average of averages, where the sample means of all groups are added and then divided by the number of groups. If the groups have unequal sample sizes, this is not taken into account in computing the grand mean. That is what is meant by unweighted marginal means. You can think of the marginal means as the estimated population means assuming all groups have an equal sample size. The emmeans function (from the emmeans package with the same name) provides a simple way to compute the estimated marginal means for each condition, but also for the levels of one factor (averaging over the levels of other factors). The emmeans requires at least two arguments: an object, which is an estimated model, and specs, which is either a character vector with the names for the predictors for which the estimated marginal predictors should be computed, or a formula. Here, we will use the formula interface, as it is flexible and intuitive. Going back to our simpler 2 by 2 design (ignoring Experimenter), the estimated marginal means of the four conditions can be computed with emmeans as follows: # load the package. If you don&#39;t have it installed, you will need to run # install.packages(&quot;emmeans&quot;) first! library(emmeans) # call emmeans with modg as the object emmeans(modg, specs = ~ primeCond:experimenterBelief) ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 You can see that the emmeans function computes an estimated marginal mean for each combination of primeCond and experimenterBelief. For each mean, we get an estimate, a standard error of that estimate, the degrees of freedom (\\(n - \\text{npar}(G)\\)), and a confidence interval. The marginal means for each group in the design are just the sample means in the groups in this case, but things become more complicated when we add additional metric predictors to the design, as we will see when we discuss ANCOVA in another chapter. You can also obtain estimated marginal means for the levels of one factor, averaging over the levels of the others. These are the marginal means that are compared in the main effects of that factor. For instance, for the primeCond factor, the marginal means are emmeans(modg, specs = ~ primeCond) ## NOTE: Results may be misleading due to involvement in interactions ## primeCond emmean SE df lower.CL upper.CL ## HPP 27.5 15.1 396 -2.28 57.2 ## LPP 15.5 15.2 396 -14.41 45.4 ## ## Results are averaged over the levels of: experimenterBelief ## Confidence level used: 0.95 and for experimenterBelief they are: emmeans(modg, specs = ~ experimenterBelief) ## NOTE: Results may be misleading due to involvement in interactions ## experimenterBelief emmean SE df lower.CL upper.CL ## H 66.7 15.1 396 37.0 96.45 ## L -23.8 15.2 396 -53.7 6.09 ## ## Results are averaged over the levels of: primeCond ## Confidence level used: 0.95 In addition to computing marginal means and providing confidence intervals for each, the package has a reasonably straightforward interface for testing differences between estimated marginal means. Such differences are effectively the contrasts that we have specified with contrast codes, and tested with Type 3 tests. A benefit of using emmeans is that you can test more of these contrasts than the required number of contrast codes (i.e. 3 in this example). If you want all pairwise comparisons between the means, you can get these by entering pairwise as the left-hand side of the formula: emmeans(modg, specs = pairwise ~ primeCond:experimenterBelief) ## $emmeans ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## HPP H - LPP H 30.79 30.2 396 1.018 0.7390 ## HPP H - HPP L 109.33 30.2 396 3.614 0.0019 ## HPP H - LPP L 102.49 30.3 396 3.380 0.0044 ## LPP H - HPP L 78.53 30.3 396 2.590 0.0487 ## LPP H - LPP L 71.69 30.4 396 2.358 0.0870 ## HPP L - LPP L -6.84 30.4 396 -0.225 0.9960 ## ## P value adjustment: tukey method for comparing a family of 4 estimates which automatically uses the Tukey HSD procedure to adjust the significance level of each test to obtain a family-wise significance level of \\(\\alpha_\\text{FW} = .05\\). You can obtain other corrections through the adjust argument. Some options to enter there, which were discussed in SDAM, are: tukey scheffe bonferroni holm There are other possibilities (see ?summary.emmGrid for details). For instance, we can apply the Scheffé adjustment with: emmeans(modg, specs = pairwise ~ primeCond:experimenterBelief, adjust=&quot;scheffe&quot;) ## $emmeans ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## HPP H - LPP H 30.79 30.2 396 1.018 0.7924 ## HPP H - HPP L 109.33 30.2 396 3.614 0.0049 ## HPP H - LPP L 102.49 30.3 396 3.380 0.0103 ## LPP H - HPP L 78.53 30.3 396 2.590 0.0836 ## LPP H - LPP L 71.69 30.4 396 2.358 0.1369 ## HPP L - LPP L -6.84 30.4 396 -0.225 0.9970 ## ## P value adjustment: scheffe method with rank 3 7.4.1 Adjusted p-values One thing I should mention is that rather than showing you the corrected significance level \\(\\alpha\\) for each test, emmeans provides you with an adjusted p-value. A benefit of this is that you can just compare each \\(p-value\\) to the usual criterion level and call each test significant when \\(p&lt;.05\\). However, I personally don’t find the resulting \\(p\\) value easy to interpret as a probability. Remember that the conventional \\(p\\)-value is the probability of obtaining a test result as large or more extreme, assuming that the null hypothesis is true. This is itself already a tricky concept, but with some experience with statistical distributions and the conceptual foundations of null-hypothesis significance testing, it is a valid probability that is interpretable as such. Wright (1992) defines the adjusted p-value as the smallest family-wise significance level at which the tested null-hypothesis would be rejected. This isn’t really a probability any more, as far as I can see it. It is true that the conventional \\(p\\)-value is, by definition, also equal to the smallest significance level at which the null-hypothesis would be rejected. For instance, if a particular test provides a \\(p\\)-value of \\(p = .07\\), then you would reject the null-hypothesis by setting \\(\\alpha \\geq .07\\). Hence, \\(\\alpha = .07\\) is the smallest value of \\(\\alpha\\) which would provide a significant test result. Similarly, if the test provided a \\(p\\)-value of \\(p=.004\\), then \\(\\alpha = .004\\) is the smallest significance level for which the test would provide a significant result. Although the \\(p\\)-value is equivalent to this “minimum \\(\\alpha\\)” value, it is also a valid probability, and when you move to the domain of corrections for multiple comparisons, defining the \\(p\\)-value as the minimum family-wise significance level \\(\\alpha_\\text{FW}\\) for which the individual test would provide a significant test result, the correspondence with a proper probability is lost. For the Bonferroni correction, the adjusted \\(p\\)-value is easy to compute. Remember that the Bonferroni correction for a total of \\(q\\) tests is to set the significance level of each individual test to \\(\\alpha = \\frac{\\alpha}{q}\\). We can adjust the \\(p\\)-values correspondingly as \\(p_\\text{adj} = q \\times p\\). But if you’d perform \\(q=100\\) tests, and \\(p=.2\\), then \\(p_\\text{adj} = 100 \\times .2 = 20\\), which is obviously not a valid probability! Adjusted \\(p\\)-values are in this sense just convenience values which can be compared to e.g. \\(\\alpha_\\text{FW} = .05\\), but nothing more. 7.5 Testing general contrasts with emmeans When you have computed the required estimated marginal means, you can then use these to define a set of general contrasts that you want to test. This set can include more contrasts than \\(g-1\\), but each contrast is defined in a way that we are used to. Let’s consider an example. ems &lt;- emmeans(modg, specs = ~ primeCond:experimenterBelief) ems ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 We have four estimated marginal means, and the order that these are presented in is seen above: HPP,H, LPP,H, HPP,L, and LPP,L. We can now use the contrast function from emmeans (note that there is no “s” at the end, so this is a different function than contrasts!) to supply the ems object and a list of (named) contrasts. Suppose we want to test the following set of (somewhat arbitrary) contrasts: \\(c_1\\) \\(c_2\\) \\(c_3\\) \\(c_4\\) \\(c_5\\) \\(c_6\\) \\(c_7\\) \\(c_8\\) HPP,H \\(\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{2}{3}\\) \\(0\\) LPP,H \\(-\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{3}\\) HPP,L \\(\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{2}{3}\\) LPP,L \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{3}\\) These can be interpreted as \\(c_1\\): main effect of Prime \\(c_2\\): main effect of Belief \\(c_3\\): comparing high and low belief for low-power prime \\(c_4\\): comparing high and low belief for high-power prime \\(c_5\\): comparing high-power and low-power prime for high belief \\(c_6\\): comparing high-power and low-power prime for low belief \\(c_7\\): comparing high-power prime with high belief to low-power prime conditions \\(c_8\\): comparing high-power prime with low belief to low-power prime conditions Using the contrast function, all these comparisons can be tested simultaneously, using the Scheffé adjustment, as follows: contrast(ems, method = list(c1 = c(1/2, -1/2, 1/2, -1/2), c2 = c(1/2, 1/2, -1/2, -1/2), c3 = c(1/2, 0, -1/2, 0), c4 = c(0, 1/2, 0, -1/2), c5 = c(1/2, -1/2, 0, 0), c6 = c(0, 0, 1/2, -1/2), c7 = c(2/3, -1/3, 0, -1/3), c8 = c(0, -1/3, 2/3, -1/3)), adjust=&quot;scheffe&quot;) ## contrast estimate SE df t.ratio p.value ## c1 11.98 21.4 396 0.559 0.9577 ## c2 90.51 21.4 396 4.221 0.0006 ## c3 54.66 15.1 396 3.614 0.0049 ## c4 35.85 15.2 396 2.358 0.1369 ## c5 15.40 15.1 396 1.018 0.7924 ## c6 -3.42 15.2 396 -0.225 0.9970 ## c7 44.43 17.5 396 2.544 0.0926 ## c8 -28.46 17.5 396 -1.624 0.4518 ## ## P value adjustment: scheffe method with rank 3 If this were a set of planned comparison, and you were confident enough to not apply a correction for multiple comparison, you could leave out the adjust argument, or provide the value adjust=\"none\". References "],["repeated-measures-anova.html", "Chapter 8 Repeated-measures ANOVA 8.1 Long and wide data 8.2 Repeated-measures ANOVA with separate GLMs 8.3 Repeated-measures ANOVA with the car package 8.4 Repeated-measures ANOVA with the afex package 8.5 Contrasts with the emmeans package", " Chapter 8 Repeated-measures ANOVA In this Chapter, we will focus on performing repeated-measures ANOVA with R. We will use the same data analysed in Chapter 10 of SDAM, which is from an experiment investigating the “cheerleader effect”. The dataset is available in the sdamr package as cheerleader. We can load it from there, and inspect the first six cases, as usual: library(sdamr) data(&quot;cheerleader&quot;) head(cheerleader) ## Participant Age Sex Task LineClickAccuracy Excluded ## 1 1 47 Male Identical-Distractors 0.1361274 0 ## 3 1 47 Male Identical-Distractors 0.1361274 0 ## 4 1 47 Male Identical-Distractors 0.1361274 0 ## 6 2 19 Female Identical-Distractors -0.9752861 0 ## 8 2 19 Female Identical-Distractors -0.9752861 0 ## 9 2 19 Female Identical-Distractors -0.9752861 0 ## WhyExcluded Item Response ## 1 &lt;NA&gt; Alone 52.71289 ## 3 &lt;NA&gt; Control_Group 56.15966 ## 4 &lt;NA&gt; Distractor_Manipulation 53.27871 ## 6 &lt;NA&gt; Alone 52.47199 ## 8 &lt;NA&gt; Control_Group 55.29972 ## 9 &lt;NA&gt; Distractor_Manipulation 55.17647 This dataset is a little messy, and includes participants who were excluded by the authors. So let’s first clean it up a little: dat &lt;- cheerleader # remove participants which should be excluded dat &lt;- subset(dat, Excluded == 0) # get rid of unused factor levels in Item by dat$Item &lt;- factor(dat$Item) Another thing is that the labels of the factors don’t correspond to the ones I used in writing the SDAM chapter. Relabelling factors is somewhat tedious with base R. It’s easier to use the fct_recode function from the forcats package (Wickham 2022). This function takes a factor as its first argument, and then in the remaining arguments, you can specify a new label (unquoted) for existing labels (quoted). As usual, if you don’t have this package installed, you would first need to run install.packages(\"forcats\") before running the code below: dat$Presentation &lt;- forcats::fct_recode(dat$Item, Different = &quot;Control_Group&quot;, Similar = &quot;Distractor_Manipulation&quot;) dat$Version &lt;- forcats::fct_recode(dat$Task, Identical = &quot;Identical-Distractors&quot;, Variant = &quot;Self-Distractors&quot;) Let’s have a look at the resulting data.frame: head(dat) ## Participant Age Sex Task LineClickAccuracy Excluded ## 1 1 47 Male Identical-Distractors 0.1361274 0 ## 3 1 47 Male Identical-Distractors 0.1361274 0 ## 4 1 47 Male Identical-Distractors 0.1361274 0 ## 6 2 19 Female Identical-Distractors -0.9752861 0 ## 8 2 19 Female Identical-Distractors -0.9752861 0 ## 9 2 19 Female Identical-Distractors -0.9752861 0 ## WhyExcluded Item Response Presentation Version ## 1 &lt;NA&gt; Alone 52.71289 Alone Identical ## 3 &lt;NA&gt; Control_Group 56.15966 Different Identical ## 4 &lt;NA&gt; Distractor_Manipulation 53.27871 Similar Identical ## 6 &lt;NA&gt; Alone 52.47199 Alone Identical ## 8 &lt;NA&gt; Control_Group 55.29972 Different Identical ## 9 &lt;NA&gt; Distractor_Manipulation 55.17647 Similar Identical Looks good! You can create a raincloud plot for this data as usual: sdamr::plot_raincloud(data=dat, y=Response, groups = Presentation) + ggplot2::facet_wrap(~Version) 8.1 Long and wide data The cheerleader data, and our dat data.frame is in the so-called long format. Data in the long format is structured so that each row contains is a single meaningful observation. Here, that translates to us having multiple rows for one participant (e.g. there are three rows for Participant 1). Data in the wide format has one row for each unit of observation (e.g. Participant). For some analyses, the long format is most suitable, whilst for others the wide format. It is therefore useful to be able to transform the data from one format to the other. This used to be a real pain back in the days when I started using R. Luckily, there are now tools available that make this a lot easier. Here, we will use the pivot_longer and pivot_wider functions from the tidyr (Wickham and Girlich 2022) package. The pivot_wider function is used to transform long-format data to the wide format. In the id_cols argument, you can list variable which identify a “unit of observation” (e.g. Participant), as well as other variables which don’t vary within subjects (such as condition). In the names_from argument, you can specify a variable which identifies the within-subjects levels, which is used to name the resulting new set of dependent variables. In the values_from argument, you specify the variable which contains the values of the new set of dependent variables: wdat &lt;- tidyr::pivot_wider(dat, id_cols = c(&quot;Participant&quot;, &quot;Version&quot;), names_from = Presentation, values_from = Response) head(wdat) ## # A tibble: 6 × 5 ## Participant Version Alone Different Similar ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Identical 52.7 56.2 53.3 ## 2 2 Identical 52.5 55.3 55.2 ## 3 3 Identical 45.9 47.9 47.7 ## 4 4 Identical 43.9 50.3 44.2 ## 5 5 Identical 37.5 34.7 37.8 ## 6 6 Identical 44.4 47.9 45.1 Note that there are quite a few variables no longer from this new wide-format data. This is not a problem, as we don’t need them for the present analyses (we could also have kept these in by including them in the id_cols argument). Also note that the class of this object is not a data.frame, but a tibble. A tibble is a “modern re-imagining of the data.frame” (https://tibble.tidyverse.org/). It is a central part of the tidyverse collection of R packages, which includes the tidyr and forcats packages we have just used, as well as the ggplot2 package, and many more. When you become more familiar with R programming, you will likely adopt more of the functions and principles of the tidyverse. You can transform data from the wide format to the long format with the pivot_longer function. In the cols argument, you need to specify which columns in the wide format to transform into a single variable in the long format. In the names_to argument, you can specify the name of the resulting identifier for each value, and in the values_to argument, you can specify the name of the variable in the long format which contains the values: ldat &lt;- tidyr::pivot_longer(wdat, cols = c(&quot;Alone&quot;, &quot;Different&quot;, &quot;Similar&quot;), names_to = &quot;Presentation&quot;, values_to = &quot;Response&quot;) head(ldat) ## # A tibble: 6 × 4 ## Participant Version Presentation Response ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Identical Alone 52.7 ## 2 1 Identical Different 56.2 ## 3 1 Identical Similar 53.3 ## 4 2 Identical Alone 52.5 ## 5 2 Identical Different 55.3 ## 6 2 Identical Similar 55.2 8.2 Repeated-measures ANOVA with separate GLMs In Chapter 10 of SDAM, we focused on performing repeated-measures ANOVA by constructing within-subjects composite scores, and then performing separate GLM analyses on these. We will start with this approach, and analyse the full 2 (Version: Identical, Variant) by 3 (Presentation: Alone, Different, Similar) design. 8.2.1 Computing within-subjects composite scores The within-subjects composite scores are effectively contrasts, computed for each participant. Let’s define the following contrasts: \\(d_0\\) \\(d_1\\) \\(d_2\\) Alone 1 \\(-\\tfrac{2}{3}\\) \\(0\\) Different 1 \\(\\tfrac{1}{3}\\) \\(\\tfrac{1}{2}\\) Similar 1 \\(\\tfrac{1}{3}\\) \\(-\\tfrac{1}{2}\\) We can compute each composite score from these contrasts as: \\[W_{j,i} = \\frac{\\sum_{k=1}^g d_{j,k} Y_{i,k}}{\\sqrt{\\sum_{k=1}^g d_{j,k}^2}}\\] For \\(W_0\\) (i.e. \\(j=0\\)), the computation in R is: # compute the top part (numerator) wdat$W0 &lt;- wdat$Alone + wdat$Different + wdat$Similar # apply scaling factor to get the correct SS wdat$W0 &lt;- wdat$W0/sqrt(3) Similarly, we can compute \\(W_1\\) and \\(W_2\\) as: wdat$W1 &lt;- (1/3)*wdat$Different + (1/3)*wdat$Similar - (2/3)*wdat$Alone wdat$W1 &lt;- wdat$W1/sqrt((1/3)^2 + (1/3)^2 + (-2/3)^2) wdat$W2 &lt;- (1/2)*wdat$Different - (1/2)*wdat$Similar wdat$w2 &lt;- wdat$W2/sqrt(2/4) 8.2.2 Performing a repeated-measures ANOVA with separate models We now have three new dependent variables (\\(W_0\\), \\(W_1\\), and \\(W_2\\)), and for each we can perform a GLM analysis. To do this, we need to set a suitable contrast for Version. As in SDAM, I will use \\((\\tfrac{1}{2}, -\\tfrac{1}{2})\\) for the Identical and Variant conditions respectively: contrasts(wdat$Version) &lt;- c(0.5, -0.5) We can then estimate a linear model for each composite variable. For \\(W_0\\), we estimate: mod0 &lt;- lm(W0 ~ Version, data=wdat) summary(mod0) ## ## Call: ## lm(formula = W0 ~ Version, data = wdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -59.928 -4.593 1.922 9.059 31.259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82.5764 2.1153 39.038 &lt;2e-16 *** ## Version1 0.8647 4.2305 0.204 0.839 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.23 on 57 degrees of freedom ## Multiple R-squared: 0.0007323, Adjusted R-squared: -0.0168 ## F-statistic: 0.04177 on 1 and 57 DF, p-value: 0.8388 Note that the estimated parameters are in the scale of \\(W_0\\), not in the scale of the dependent variable (\\(Y\\)). We can get the rescaled estimates by dividing the estimates by the scaling factor (\\(\\sqrt{3}\\) in this case). The coefficients functions extracts the parameter estimates from the model. Hence, the rescaled estimates can be computed as: coefficients(mod0)/sqrt(3) ## (Intercept) Version1 ## 47.675478 0.499217 To obtain equivalent \\(F\\)-tests, we can use the Anova function from the car package: car::Anova(mod0, type=3) ## Anova Table (Type III tests) ## ## Response: W0 ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 401272 1 1523.9890 &lt;2e-16 *** ## Version 11 1 0.0418 0.8388 ## Residuals 15008 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The procedure for the within-subjects composite scores \\(W_1\\) and \\(W_2\\) is similar. For \\(W_1\\), the analysis is: # Analysis for W1 mod1 &lt;- lm(W1 ~ Version, data=wdat) summary(mod1) ## ## Call: ## lm(formula = W1 ~ Version, data = wdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8388 -1.0661 -0.0307 0.9773 3.6227 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.057739 0.190904 5.541 8.02e-07 *** ## Version1 -0.008033 0.381808 -0.021 0.983 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.464 on 57 degrees of freedom ## Multiple R-squared: 7.766e-06, Adjusted R-squared: -0.01754 ## F-statistic: 0.0004426 on 1 and 57 DF, p-value: 0.9833 coefficients(mod1)/sqrt(6/9) ## (Intercept) Version1 ## 1.29546036 -0.00983829 car::Anova(mod1, type=3) ## Anova Table (Type III tests) ## ## Response: W1 ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 65.839 1 30.6992 8.023e-07 *** ## Version 0.001 1 0.0004 0.9833 ## Residuals 122.245 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And for \\(W_2\\), it is: # Analysis for W2 mod2 &lt;- lm(W2 ~ Version, data=wdat) summary(mod2) ## ## Call: ## lm(formula = W2 ~ Version, data = wdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.02844 -0.47968 -0.06555 0.36866 2.52549 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1981 0.1184 1.673 0.0998 . ## Version1 0.5865 0.2368 2.476 0.0163 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9083 on 57 degrees of freedom ## Multiple R-squared: 0.09714, Adjusted R-squared: 0.0813 ## F-statistic: 6.133 on 1 and 57 DF, p-value: 0.01626 coefficients(mod2)/sqrt(2/4) ## (Intercept) Version1 ## 0.2801500 0.8293672 car::Anova(mod2, type=3) ## Anova Table (Type III tests) ## ## Response: W2 ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 2.309 1 2.7991 0.09980 . ## Version 5.060 1 6.1330 0.01626 * ## Residuals 47.026 57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Unfortunately, there is no simple way to obtain omnibus tests by combining these models. They can be computed “manually”, by extracting the relevant SSR, SSE, and df terms from the models. These can then be used to compute an \\(F\\)-statistic, and the \\(p\\)-value can then be computed by using the pf function. This is not the most straightforward manner to obtain omnibus tests (the following sections show how to do this in a much more convenient manner). But to show it is doable, let’s compute the omnibus test for the main effect of Presentation in this way. The relevant SSR, SSE, and df terms are stored in the objects returned by the car::Anova function. We can see the structure of this object with the str function: str(car::Anova(mod1, type=3)) ## Classes &#39;anova&#39; and &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ Sum Sq : num 6.58e+01 9.49e-04 1.22e+02 ## $ Df : num 1 1 57 ## $ F value: num 3.07e+01 4.43e-04 NA ## $ Pr(&gt;F) : num 8.02e-07 9.83e-01 NA ## - attr(*, &quot;heading&quot;)= chr [1:2] &quot;Anova Table (Type III tests)\\n&quot; &quot;Response: W1&quot; This shows that the car::Anova function returns a data.frame with the test results. The first row corresponds to the test of the intercept (which reflects the main effects of Presentation in this repeated-measures ANOVA). The last row contains the values for the error term. The structure for the mod2 analysis is the same. To get the relevant omnibus values, we can just take the appropriate elements from these data.frames. To get the omnibus SSR and \\(\\text{df}_1\\) terms, we can use: SSR &lt;- car::Anova(mod1, type=3)$&quot;Sum Sq&quot;[1] + car::Anova(mod2, type=3)$&quot;Sum Sq&quot;[1] df1 &lt;- car::Anova(mod1, type=3)$&quot;Df&quot;[1] + car::Anova(mod2, type=3)$&quot;Df&quot;[1] And for the SSE and \\(\\text{df}_2\\) terms, we can use: SSE &lt;- car::Anova(mod1, type=3)$&quot;Sum Sq&quot;[3] + car::Anova(mod2, type=3)$&quot;Sum Sq&quot;[3] df2 &lt;- car::Anova(mod1, type=3)$&quot;Df&quot;[3] + car::Anova(mod2, type=3)$&quot;Df&quot;[3] With these values, the \\(F\\)-statistic can then be computed as follows: Fstat &lt;- (SSR/df1)/(SSE/df2) Finally, the \\(p\\)-value can be obtained as 1-pf(Fstat, df1=df1, df2=df2) ## [1] 4.214718e-09 Note that we need to use 1-pf as the pf function computes the probability \\(P(F \\leq \\text{value})\\), whilst we need \\(P(F &gt; \\text{value})\\), and this equals \\(P(F &gt; \\text{value}) = 1 - P(F \\leq \\text{value})\\). The steps we have just taken is a perfectly valid manner to conduct a repeated-measures ANOVA, but it is a laborious process. An easier way to conduct repeated-measures ANOVA is provided in the car or afex package. Neither of these packages provide the tests for the individual contrasts we have just obtained. But these can be computed with the emmeans package, after conducting the omnibus tests. 8.3 Repeated-measures ANOVA with the car package When you have data in the wide format, you can obtain a repeated-measures ANOVA by using the Anova function from the car package (Fox, Weisberg, and Price 2022). As you will see later, the analysis is more straightforward with the afex package, but this requires data to be in the long format. The first step to performing a repeated-measures ANOVA with the car package is to perform a linear model for a multivariate dependent variable, which basically means providing a matrix of each repeated measurement as the DV. This is done by collating the variables within a cbind (for column-bind) argument within the model formula: mvmod &lt;- lm(cbind(Alone, Different, Similar) ~ Version, data=wdat) So here, we are modelling the Alone, Different, and Similar attractiveness ratings simultaneously as a function of the Version categorical predictor. This model is basically a set of three linear regressions, as you can see from the output: mvmod ## ## Call: ## lm(formula = cbind(Alone, Different, Similar) ~ Version, data = wdat) ## ## Coefficients: ## Alone Different Similar ## (Intercept) 46.81184 48.30539 47.90920 ## Version1 0.50578 1.08239 -0.09051 The next step is to construct an object which reflects the structure of these three measurements. This has to be done with a separate data.frame, with one row for each variable included in the cbind function specifying the multivariate DV. In this case, there is a single categorical predictor underlying all three measurements. So our data.frame can contain a single factor: idata &lt;- data.frame(Presentation = factor(c(&quot;Alone&quot;, &quot;Different&quot;, &quot;Similar&quot;))) idata ## Presentation ## 1 Alone ## 2 Different ## 3 Similar The next step is to supply a useful contrast for this within-subjects factor: contrasts(idata$Presentation) # check the levels ## Different Similar ## Alone 0 0 ## Different 1 0 ## Similar 0 1 contrasts(idata$Presentation) &lt;- cbind(c(-2/3, 1/3, 1/3), c(0,1/2, -1/2)) contrasts(idata$Presentation) ## [,1] [,2] ## Alone -0.6666667 0.0 ## Different 0.3333333 0.5 ## Similar 0.3333333 -0.5 With these elements in place, we are finally ready to perform the repeated-measures ANOVA. This involves calling the car::Anova function with the multivariate linear model as the first argument, and supplying the within-subjects structure through the idata argument. Additionally, you need to supply a right-hand-sided formula in the idesign argument in order to specify which effects to include as within-subjects factors. The type=3 argument, as usual, specifies we would like to perform Type-3 SS tests. rmaov &lt;- car::Anova(mvmod, idata=idata, idesign = ~Presentation, type=3) rmaov ## ## Type III Repeated Measures MANOVA Tests: Pillai test statistic ## Df test stat approx F num Df den Df Pr(&gt;F) ## (Intercept) 1 0.96395 1523.99 1 57 &lt; 2.2e-16 *** ## Version 1 0.00073 0.04 1 57 0.83878 ## Presentation 1 0.36601 16.17 2 56 2.872e-06 *** ## Version:Presentation 1 0.09730 3.02 2 56 0.05691 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 By default, because we have used a multivariate DV, this will show a so-called MANOVA (Multivariate ANalysis of VAriance). To obtain an ANOVA, we need to set the multivariate argument in the summary function to FALSE: summary(rmaov, multivariate=FALSE) ## Warning in summary.Anova.mlm(rmaov, multivariate = FALSE): HF eps &gt; 1 treated as ## 1 ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 401272 1 15008.3 57 1523.9890 &lt; 2.2e-16 *** ## Version 11 1 15008.3 57 0.0418 0.83878 ## Presentation 70 2 216.3 114 18.5675 1.047e-07 *** ## Version:Presentation 10 2 216.3 114 2.6670 0.07379 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Presentation 0.98188 0.59925 ## Version:Presentation 0.98188 0.59925 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Presentation 0.9822 1.314e-07 *** ## Version:Presentation 0.9822 0.07485 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Presentation 1.016935 1.046537e-07 ## Version:Presentation 1.016935 7.379140e-02 In addition to an ANOVA table which contains the omnibus tests for the within- and between-subjects effects, the output provides the Mauchly sphericity test, and subsequently the Greenhouse-Geisser and Huynh-Feldt corrected tests. The tables corresponding to these latter two corrected tests report the Greenhouse-Geisser and Huynh-Feldt estimates (as GG eps and HF eps respectively) of what I have denoted as \\(\\hat{\\zeta}\\), but is more commonly denoted as \\(\\hat{\\epsilon}\\), and the \\(p\\)-value which results from applying the correction to the degrees of freedom reported in the Univariate Type III Repeated-Measures ANOVA Assuming Sphericity table. A notable absence is the tests of the specific contrasts. We can obtain these by performing analyses on the within-subjects composite scores, as we did in the previous section. 8.4 Repeated-measures ANOVA with the afex package The afex package (Singmann et al. 2022) provides a convenient interface to the car::Anova() function, via its afex::aov_car() function. To use this function, the data needs to be in the long format. You can specify the model with the usual formula interface, and you don’t need to worry about a multivariate response and such things. There is one new thing, however: To specify a repeated-measures ANOVA, the formula needs to contain a special Error() argument. Within the Error argument, you first state the variable which identifies the “units of observations” (i.e. Participant in this case). Then, after a forward-slash (“/”), you list the repeated-measures factor(s). So, the way to perform the repeated-measures ANOVA with the afex package, and the long data (ldat) we created earlier, is: afmod &lt;- afex::aov_car(Response ~ Version*Presentation + Error(Participant/Presentation), data=ldat) ## Contrasts set to contr.sum for the following variables: Version afmod ## Anova Table (Type 3 tests) ## ## Response: Response ## Effect df MSE F ges p.value ## 1 Version 1, 57 263.30 0.04 &lt;.001 .839 ## 2 Presentation 1.96, 111.97 1.93 18.57 *** .005 &lt;.001 ## 3 Version:Presentation 1.96, 111.97 1.93 2.67 + &lt;.001 .075 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG afex provides an abridged ANOVA table, where the Greenhouse-Geisser correction is automatically applied. Note that afex automatically sets contrasts to contr.sum. That is useful here, as we haven’t set the contrast for Version in the ldat data. Because neither afex, nor the car::Anova package on which it relies, provides parameter estimates for the GLM, it doesn’t really matter whether you supply your own (sum-to-zero) contrasts, or whether you let afex pick a contr.sum() contrast for you. The afex package provides a convenient wrapper around the car::Anova() function, and saves you a lot of work if you have data in the long format. I would generally recommend storing data in the long format, as this also makes it easier to apply linear mixed-effects models. You can obtain the results as reported by the car::Anova() function by calling the summary function: summary(afmod) ## Warning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps &gt; 1 ## treated as 1 ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 401272 1 15008.3 57 1523.9890 &lt; 2.2e-16 *** ## Version 11 1 15008.3 57 0.0418 0.83878 ## Presentation 70 2 216.3 114 18.5675 1.047e-07 *** ## Version:Presentation 10 2 216.3 114 2.6670 0.07379 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Presentation 0.98188 0.59925 ## Version:Presentation 0.98188 0.59925 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Presentation 0.9822 1.314e-07 *** ## Version:Presentation 0.9822 0.07485 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Presentation 1.016935 1.046537e-07 ## Version:Presentation 1.016935 7.379140e-02 The afex package also has a function, called nice, to display the abbreviated ANOVA table we saw earlier. The arguments of this function allow you to change various aspects of the displayed results. For instance, by default, the “generalized eta-square” is used as a measure of effect size. You can change this to the partial eta-square by setting es=\"pes\". You can also change the correction to the degrees of freedom, from the default correction = \"GG\" (Greenhouse-Geisser), by setting correction = \"HF\" (Huynh-Feldt) or correction = \"none\" (no correction). So, for example, we might use afex::nice(afmod, es=&quot;pes&quot;, correction = &quot;none&quot;) ## Anova Table (Type 3 tests) ## ## Response: Response ## Effect df MSE F pes p.value ## 1 Version 1, 57 263.30 0.04 &lt;.001 .839 ## 2 Presentation 2, 114 1.90 18.57 *** .246 &lt;.001 ## 3 Version:Presentation 2, 114 1.90 2.67 + .045 .074 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 8.5 Contrasts with the emmeans package To obtain the individual contrast estimates and tests for a repeated-measures ANOVA, perhaps the most straightforward procedure is via the emmeans package (Lenth 2022). We have already discussed the use of this package in Section 7.4. The emmeans::emmeans() function calculates estimated marginal means, and it can do so for objects that are returned by the afex::aov_car() function. For example, we can obtain the marginal means for the different levels of Version as: em_version &lt;- emmeans::emmeans(afmod, specs = ~ Version) The specs argument should contain a right-sided formula with the factor(s) for which you want to compute the marginal means. You can see that the emmeans function computes the estimated marginal means, their standard error, associated degrees of freedom, and confidence intervals: em_version ## Version emmean SE df lower.CL upper.CL ## Identical 47.9 1.77 57 44.4 51.5 ## Variant 47.4 1.68 57 44.1 50.8 ## ## Results are averaged over the levels of: Presentation ## Confidence level used: 0.95 Contrasts are really just differences between (sets of) marginal means, or even more generally, linear functions of marginal means. By applying a contrast code to the marginal means, we obtain a new value (e.g. a difference between marginal means), which also comes with a standard error and associated degrees of freedom. These can be used in a one-sample \\(t\\)-test, to test whether this new values is equal to an assumed value (e.g. 0). For example, we may want to test whether the difference between the marginal mean of the Identical condition and the Variant condition is equal to 0, i.e. \\(H_0: \\mu_I - \\mu_V = 0\\). The contrast to transform the means into this difference is \\((1,-1)\\), because the sum of the means multiplied by these values is \\(1 \\times \\mu_I + (-1) \\times \\mu_V = \\mu_I - \\mu_V\\). Note that these contrasts work slightly differently than when using contrast-coded predictors in the GLM. There, we would have used the values \\((\\tfrac{1}{2}, -\\tfrac{1}{2})\\) to obtain the same contrast. That is because the slopes of orthogonal contrast-coded predictors are \\[\\beta_j = \\frac{\\sum_{k=1}^g c_{j,k} \\times \\mu_k}{\\sum_{k=1}^g c^2_{j,k}}\\] and we would aim for this slope to represent \\(\\mu_I - \\mu_V\\). Using \\((\\tfrac{1}{2}, -\\tfrac{1}{2})\\), the slope would be exactly this: \\[\\frac{ \\tfrac{1}{2} \\mu_I - \\tfrac{1}{2} \\mu_V}{(\\tfrac{1}{2})^2 + (-\\tfrac{1}{2})^2} = \\frac{ \\tfrac{1}{2} \\mu_I - \\tfrac{1}{2} \\mu_V}{\\tfrac{1}{2}} = \\mu_I - \\mu_V\\] As the contrasts in the emmeans package apply directly to the means, we don’t have to worry about the \\(\\sum_{k=1}^g c^2_{j,k}\\) term which scales the slopes of contrast-coded predictors in the GLM. That is why, when using the emmeans package, we can use \\((1,-1)\\) as our contrast, instead of \\((\\tfrac{1}{2}, -\\tfrac{1}{2})\\). The emmeans::contrast() function allows us to compute such contrasts of marginal means, and the corresponding one-sample \\(t\\)-test (against the null-hypothesis that the resulting value is equal to 0). In the methmod argument, we here supply a named list. The name is not necessary, but is helpful in identifying the contrasts when you test multiple. The key thing is that we supplied a contrast with c(1,-1). The output provides the estimate of the difference between \\(\\mu_I\\) and \\(\\mu_V\\), as well as a \\(t\\)-test and \\(p\\)-value: emmeans::contrast(em_version, method=list(&quot;I - V&quot; = c(1,-1))) ## contrast estimate SE df t.ratio p.value ## I - V 0.499 2.44 57 0.204 0.8388 ## ## Results are averaged over the levels of: Presentation We can see that this test is not significant, and hence we can’t reject the null-hypothesis that the true difference between the means is equal to 0. The results are identical to the test of the slope of Version1 for the model of W0 in Section 8.2. We can follow the same procedure to tests contrasts for the Presentation factor. In this case, the list of contrasts specified in the method argument has two elements. In the first contrast, we want to determine the difference \\[\\frac{\\mu_D + \\mu_S}{2} - \\mu_A\\] which we can do through the contrast c(-1,.5,.5) (applied to the A, D, and S conditions respectively). In the second contrast, we want to determine the difference \\[\\mu_D - \\mu_S\\] which we can do through the contrast c(0,1,-1). The following code computes the marginal means and then performs the contrast-tests on these: em_presentation &lt;- emmeans::emmeans(afmod, specs = ~ Presentation) emmeans::contrast(em_presentation, method=list(&quot;(D + S)/2 - A&quot; = c(-1,.5,.5), &quot;D - S&quot; = c(0,1,-1))) ## contrast estimate SE df t.ratio p.value ## (D + S)/2 - A 1.295 0.234 57 5.541 &lt;.0001 ## D - S 0.396 0.237 57 1.673 0.0998 ## ## Results are averaged over the levels of: Version This replicates the earlier results we obtained in Section 8.2 from the tests of the intercepts of W1 and W2. Finally, we can also consider the marginal means of the combinations of the Presentation and Version factors. We do this by specifying the full (main effects and interaction) model in the specs argument: em_pv &lt;- emmeans::emmeans(afmod, specs = ~ Presentation*Version) The marginal means are em_pv ## Presentation Version emmean SE df lower.CL upper.CL ## Alone Identical 47.1 1.78 57 43.5 50.6 ## Different Identical 48.8 1.78 57 45.3 52.4 ## Similar Identical 47.9 1.80 57 44.3 51.5 ## Alone Variant 46.6 1.69 57 43.2 49.9 ## Different Variant 47.8 1.69 57 44.4 51.1 ## Similar Variant 48.0 1.71 57 44.5 51.4 ## ## Confidence level used: 0.95 An interaction implies that a contrast for one experimental factor is moderated by the levels of another experimental factor. For example, the difference between \\[\\frac{\\mu_D + \\mu_S}{2} - \\mu_A\\] might be different in the Identical compared to the Variant condition. If that is the case, then the difference between these differences would not equal 0. Such a “difference of differences” is most clearly stated in an equation: \\[\\left(\\frac{\\mu_{I,D} + \\mu_{I,S}}{2} - \\mu_{I,A}\\right) - \\left(\\frac{\\mu_{V,D} + \\mu_{V,S}}{2} - \\mu_{V,A}\\right)\\] If we were to write this as a sum of all six means, we would do so as follows: \\[\\tfrac{1}{2} \\times \\mu_{I,D} + \\tfrac{1}{2} \\mu_{I,S} + (-1) \\times \\mu_{I,A} + (-\\tfrac{1}{2}) \\times \\mu_{V,D} + (-\\tfrac{1}{2}) \\times \\mu_{V,S} + 1 \\times \\mu_{V,A}\\] Hence, the implied contrast code is c(.5, .5, -1, -.5, -.5, 1). Following a similar logic, the implied contrast code for the second interaction is c(0, 1, -1, 0, -1, 1). Supplying these contrast codes to the emmeans::contrast function, we obtain the following results: emmeans::contrast(em_pv, method=list(&quot;c1 by d1&quot; = c(-1, .5, .5, 1, -.5, -.5), &quot;c1 by d2&quot; = c(0, 1, -1, 0, -1, 1))) ## contrast estimate SE df t.ratio p.value ## c1 by d1 -0.00984 0.468 57 -0.021 0.9833 ## c1 by d2 1.17290 0.474 57 2.476 0.0163 This replicates the results of the estimates and tests of the slopes of Version1 in the models of W1 and W2 of Section 8.2 exactly. So it doesn’t really matter all that much what form of contrast coding you use in the original analysis (as long as you use a form of sum-to-zero contrast coding). You can always perform the contrast tests afterwards by using the emmeans package (and other packages which provide similar functionality). References "],["linear-mixed-effects-models.html", "Chapter 9 Linear mixed-effects models 9.1 Formulating and estimating linear mixed-effects models with lme4 9.2 Obtaining p-values with afex::mixed 9.3 Crossed random effects 9.4 Automatically finding optimal random effects structures with the buildmer package", " Chapter 9 Linear mixed-effects models In this Chapter, we will look at how to estimate and perform hypothesis tests for linear mixed-effects models. The main workhorse for estimating linear mixed-effects models is the lme4 package (Bates et al. 2022). This package allows you to formulate a wide variety of mixed-effects and multilevel models through an extension of the R formula syntax. It is a really good package. But the main author of the package, Douglas Bates, has chosen not to provide \\(p\\)-values for the model parameters. We will therefore also consider the afex package, which provides an interface to the two main approximations (Kenward-Roger and Satterthwaite) to provide the degrees of freedom to compute \\(p\\)-values for \\(F\\) tests. While the mixed function it provides is in principle all you need to estimate the models and get the estimates, I think it is useful to also understand the underlying lme4 package (Bates et al. 2022), so we will start with a discussion of this, and then move to the afex package (Singmann et al. 2022). If you install the afex package, it will install quite a few other packages on which it relies. So to get all the required packages for this chapter, you can just type install.packages(&quot;afex&quot;) in the R console, and you should have everything you need. 9.1 Formulating and estimating linear mixed-effects models with lme4 The gold standard for fitting linear mixed-effects models in R is the lmer() (for linear mixed-effects regression) in the lme4 package. This function takes the following arguments (amongst others, for the full list of arguments, see ?lmer): formula: a two-sided linear formula describing both the fixed-effects and random-effects part of the model, with the response on the left of the ~ operator and predictors and random effects on the right-hand side of the ~ operator. data: A data.frame, which needs to be in the so-called “long” format, with a single row per observation. This may be different from what you might be used to when dealing with repeated-measures. A repeated-measures ANOVA in SPSS requires data in the “wide” format, where you use columns for the different repeated measures. Data in the “wide” format has a single row for each participants. In the “long” format, you will have multiple rows for the data from a single grouping level (e.g., participant, lab, etc.). REML: A logical variable whether to estimate the model with restricted maximum likelihood (REML = TRUE, the default) or with maximum likelihood (REML = FALSE). As correct use of the formula interface is vital, let’s first consider again how the formula interface works in general. Formulas allow you to specify a linear model succinctly (and by default, any model created with a formula will include an intercept, unless explicitly removed). Here are some examples (adapted from Singmann and Kellen (2019)): Table 9.1: Examples of R’s formula notation for fixed effects. Formula Description a + b main effects of a and b (and no interaction) a:b only interaction of a and b (and no main effects) a * b main effects and interaction of a and b (expands to: a + b + a:b) (a+b+c)^2 main effects and two-way interactions, but no three-way interaction (expands to: a + b + c + a:b + b:c + a:c) (a+b)*c all main effects and pairwise interactions between c and a or b (expands to: a + b + c + a:c + b:c) 0 + a 0 suppresses the intercept resulting in a model that has one parameter per level of a (identical to: a - 1) The lme4 package extends the formula interface to specify random effects structures. Random effects are added to the formula by writing elements between parentheses (). Within these parentheses, you provide the specification of the random effects to be included on the left-hand side of a conditional sign |. On the right-hand side of this conditional sign, you specify the grouping factor, or grouping factors, on which these random effects depend. The grouping factors need to be of class factor (i.e., they can not be numeric variables). Here are some examples of such specifications (again mostly adapted from Singmann and Kellen (2019)): Table 9.1: Examples of lme4’s formula notation for random effects. Formula Description (1|s) random intercepts for unique level of the factor s (1|s) + (1|i) random intercepts for each unique level of s and for each unique level of i (1|s/i) random intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1|s) + (1|s:i), i.e. a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools. (a|s) random intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated. (identical to (a*b|s)) (a*b|s) random intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated. (0+a|s) random slopes for a for each level of s, but no random intercepts (a||s) random intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e. they are set to 0). This expands to: (0+a|s) + (1|s)) 9.1.1 Random intercepts model Now let’s try to define a relatively simple linear mixed-effects model for the anchoring data set in the sdamr package. We will use the data from all the referrers try a few variations to get acquainted with the lmer() function. First, let’s load the packages and the data: library(sdamr) library(lme4) ## Loading required package: Matrix data(&quot;anchoring&quot;) Now let’s estimate a first linear mixed-effects model, with a fixed effect for anchor, and random intercepts, using everest_feet as the dependent variable. We will first ensure that anchor is a factor and associate a sum-to-zero contrast to it. We will also make referrer a factor; the contrast for this shouldn’t really matter, so we’ll leave it as a dummy code. We then set up the model, using (1|referrer) to specify that random intercept-effects should be included for each level of the referrer factor. Finally, we use the summary() function on the estimated model to obtain the estimates of the parameters. anchoring$anchor &lt;- factor(anchoring$anchor) contrasts(anchoring$anchor) &lt;- c(1/2, -1/2) # alphabetical order, so high before low # define a lmer mod &lt;- lme4::lmer(everest_feet ~ anchor + (1|referrer), data=anchoring) summary(mod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + (1 | referrer) ## Data: anchoring ## ## REML criterion at convergence: 98195.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.4433 -0.7178 -0.1361 0.7419 3.5546 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2717134 1648 ## Residual 93929988 9692 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22788.3 346.8 65.70 ## anchor1 23047.3 285.8 80.64 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.022 The output first shows some information about the structure of the model, and the value of the optimized “-2 log REML” (the logarithm of the minus two restricted maximum likelihood). Then some summary statistics for the standardized residuals are shown (these are the “raw” residuals divided by the estimated standard deviation of the residuals). Under the Random effects: header, you will find a table with estimates of the variance and standard deviation of the random effects terms for each grouping factor (just referrer in this case). So the estimated standard deviation of the random intercept: \\(\\hat{\\sigma}_{\\gamma_0} = 1648\\). You will also find an estimate of the variance and standard deviation of the residuals in this table: \\(\\hat{\\sigma}_\\epsilon = 9692\\)}. Under the Fixed effects: header, you will find a table with an estimate for each fixed effect, as well as the standard error of this estimate, and an associated \\(t\\) statistic. This output is much like the output of calling the summary() function on a standard model estimated with the lm function. But you won’t find the \\(p\\)-value for these estimates. This is because the author of the lme4 package, perhaps rightly, finds none of the approximations to the error degrees of freedom good enough for general usage. Opinions on this will vary. It is agreed that the true Type 1 error rate when using one of the approximations will not be exactly equal to the \\(\\alpha\\) level. In some cases, the difference may be substantial, but often the approximation will be reasonable enough to be useful in practice. For further information on this, there is a section in a very useful GLMM FAQ. You can also see ?lme4::pvalues for some information about various approaches to obtaining \\(p\\)-values. The final table, under the Correlation of Fixed Effects, shows the approximate correlation between the estimators of the fixed effects. You can think of it as the expected correlation between the estimates of the fixed effects over all different datasets that you might obtain (assuming that the predictors have the same values in each). It is not something you generally need to be concerned about. 9.1.2 Visually assessing model assumptions You can use the predict and residuals function to obtain the predicted values and residuals for a linear mixed effects model. You can then plot these, using e.g. ggplot2, as follows: library(ggplot2) tdat &lt;- data.frame(predicted=predict(mod), residual = residuals(mod)) ggplot(tdat,aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept=0, lty=3) It might be cool to colour the residuals by referrer as follows: tdat &lt;- data.frame(predicted=predict(mod), residual = residuals(mod), referrer=anchoring$referrer) ggplot(tdat,aes(x=predicted,y=residual, colour=referrer)) + geom_point() + geom_hline(yintercept=0, lty=3) If the legend gets in the way, you can remove it as follows: ggplot(tdat,aes(x=predicted,y=residual, colour=referrer)) + geom_point() + geom_hline(yintercept=0, lty=3) + theme(legend.position = &quot;none&quot;) The theme function allows for lots of functionality (check ?theme). You can also get a quick predicted vs residual plot from base R by simply calling plot(mod). We can get a nice-looking histogram of the residuals, and a QQ plot, as follows: ggplot(tdat,aes(x=residual)) + geom_histogram(bins=20, color=&quot;black&quot;) ggplot(tdat,aes(sample=residual)) + stat_qq() + stat_qq_line() 9.1.3 Random intercepts and slopes Now let’s estimate a model with random intercepts and random slopes for anchor. To do so, we can simply add anchor in the mixed effects structure specification, as follows: modg &lt;- lme4::lmer(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring) summary(modg) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: anchoring ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 2470481 1572 ## anchor1 36036021 6003 -0.80 ## Residual 87887265 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22840.8 329.4 69.34 ## anchor1 23578.8 1139.0 20.70 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 As you can see, the model now estimates a variance of the random slopes effects, as well as a correlation between the random intercept and slope effects. We could try to get a model without the correlations as follows: modr &lt;- lme4::lmer(everest_feet ~ anchor + (1 + anchor||referrer), data=anchoring) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : ## unable to evaluate scaled gradient ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : ## Model failed to converge: degenerate Hessian with 1 negative eigenvalues As you can see in the warning messages, this leads to various estimation issues. Moreover, the correlation is actually still there! summary(modr) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + ((1 | referrer) + (0 + anchor | referrer)) ## Data: anchoring ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 72369 269 ## referrer.1 anchorhigh 3835401 1958 ## anchorlow 18979098 4357 -0.77 ## Residual 87887261 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22840.8 329.4 69.34 ## anchor1 23578.8 1139.0 20.70 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 ## optimizer (nloptwrap) convergence code: 0 (OK) ## unable to evaluate scaled gradient ## Model failed to converge: degenerate Hessian with 1 negative eigenvalues As it turns out, the || notation does not work well for factors!. It only works as expected with metric predictors. We can get the desired model by defining a contrast-coded predictor for anchor explicitly, as follows: anchoring$anchor_contrast &lt;- 1/2 anchoring$anchor_contrast[anchoring$anchor == &quot;low&quot;] &lt;- -1/2 modr &lt;- lme4::lmer(everest_feet ~ anchor_contrast + (1 + anchor_contrast||referrer), data=anchoring) summary(modr) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: ## everest_feet ~ anchor_contrast + ((1 | referrer) + (0 + anchor_contrast | ## referrer)) ## Data: anchoring ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 anchor_contrast 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22773.6 326.7 69.71 ## anchor_contrast 23526.1 1141.8 20.60 ## ## Correlation of Fixed Effects: ## (Intr) ## anchr_cntrs 0.011 That is a little cumbersome, especially if you have a factor with lots of levels, in which case you would have to specify many contrast-coded predictors. The lmer_alt() function in the afex package will automatically generate the contrast-coded predictors needed, which will be convenient. You can try this by running: modr &lt;- afex::lmer_alt(everest_feet ~ anchor + (1 + anchor||referrer), set_data_arg = TRUE, data=anchoring) summary(modr) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## Data: anchoring ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 re1.anchor1 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22773.65 326.69 29.08 69.71 &lt;2e-16 *** ## anchor1 23526.08 1141.84 28.22 20.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.011 Note the use of the additional set_data_arg = TRUE argument, which is necessary to later use the object for model comparisons with the likelihood ratio test in the next section. Also note that the parameters now do come with \\(p\\)-values (using the Satterthwaite approximation). 9.1.4 Likelihood ratio test with the anova function We now have two versions of our random intercepts + slopes model, one which estimates the correlation between the random intercept and slope, and one which sets this to 0. A likelihood-ratio test comparing these two models is easily obtained as: anova(modr,modg) ## refitting model(s) with ML (instead of REML) ## Data: anchoring ## Models: ## modr: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## modg: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## modr 5 98000 98032 -48995 97990 ## modg 6 97985 98024 -48987 97973 16.339 1 5.296e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note the message refitting model(s) with ML (instead of REML). The likelihood-ratio test requires that the models are estimated by maximum likelihood, rather than restricted maximum likelihood (REML). The lme4 package is clever enough to realize this, and first re-estimates the model before computing the likelihood ratio test. Also note that the test statistic is now called “Chisq”, for Chi-squared. This is the one we want. The test result is significant, and hence we can reject the null hypothesis that the correlation between the random intercept and slope is \\(\\rho_{\\gamma_0,\\gamma_1} = 0\\). 9.1.5 Confidence intervals While the lme4 package does not provide \\(p\\)-values, it does have functionality to compute confidence intervals via the confint() function. The default option is to compute so-called profile likelihood confidence intervals for all (fixed and random) parameters: confint(modg) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## .sig01 1083.558297 2187.2825617 ## .sig02 -0.956541 -0.5042915 ## .sig03 4471.113419 7999.4737204 ## .sigma 9185.991570 9570.4663804 ## (Intercept) 22184.246089 23499.3202067 ## anchor1 21298.714977 25838.8350190 Note that .sig01 refers to the standard deviation of the random intercept (i.e. \\(\\sigma{\\gamma_0}\\)), .sig02 refers to the correlation between the random intercept and random slope (i.e. \\(\\rho_{\\gamma_0,\\gamma_1}\\)), and .sig03 to the standard deviation of the random slope (i.e. \\(\\sigma_{\\gamma_1}\\)). The value of .sig refers to the standard deviation of the error term (i.e. \\(\\sigma_\\epsilon\\)). Unfortunately, these are not the most informative labels, so it pays to check the values reported in summary(modg) to match them to the output here. Parametric bootstrap confidence (via simulation) can be obtained by setting the argument method = \"boot\". This is a very computationally intensive procedure, so you will have to wait some time for the results! Moreover, due to the random simulation involved, the results will vary (hopefully a little) every time you run the procedure: set.seed(20201201) confint(modg, method=&quot;boot&quot;) ## Computing bootstrap confidence intervals ... ## ## 14 message(s): boundary (singular) fit: see help(&#39;isSingular&#39;) ## 2.5 % 97.5 % ## .sig01 1041.469 2111.5327662 ## .sig02 -1.000 -0.5192476 ## .sig03 4369.214 7716.5436429 ## .sigma 9171.071 9573.5222532 ## (Intercept) 22138.746 23535.5279557 ## anchor1 21366.564 26111.6466861 confint(modg, method=&quot;boot&quot;) ## Computing bootstrap confidence intervals ... ## ## 12 message(s): boundary (singular) fit: see help(&#39;isSingular&#39;) ## 1 warning(s): Model failed to converge with max|grad| = 0.00760288 (tol = 0.002, component 1) ## 2.5 % 97.5 % ## .sig01 1045.6131551 2151.4335327 ## .sig02 -0.9976517 -0.5486075 ## .sig03 4296.8794715 7695.7776431 ## .sigma 9164.6786544 9584.8451898 ## (Intercept) 22141.6525509 23504.3587997 ## anchor1 21047.7038556 25819.4349753 Note the warning messages. By default, the bootstrap simulates nsim=500 datasets and re-estimates the model for each. In some of the simulated datasets, the estimation may fail, which provides the resulting warning messages. While confidence intervals and hypothesis tests, in the case of “standard” linear models give the same results, this is not necessarily the case for mixed models, as the \\(F\\) tests for the fixed effects involve approximation of the error degrees of freedom (\\(\\text{df}_2\\)), whilst the computation of the confidence intervals rely on other forms of approximation (e.g. simulation for the parametric bootstrap). As confidence intervals are included by default in lme4, it seems like the author of the package believes these are perhaps more principled than the \\(F\\) tests for the fixed effects. 9.1.6 Plotting model predictions It can be useful to plot the model predictions for each level of the random grouping factor. We can obtain such a plot by storing the model predictions with the data. By adding the group = and colour = arguments in the aes() function, you can then get separate results for all levels of the random effect factor (referrer here). For instance, we can plot the predictions for the different levels of the anchor factor with connecting lines as follows: anchoring$pred &lt;- predict(modg) ggplot(anchoring,aes(x=anchor,y=pred,colour=referrer, group=referrer)) + geom_point() + geom_line() + theme(legend.position=&quot;bottom&quot;, legend.direction = &quot;horizontal&quot;) This would also work if the variable of the x-axis is continuous, rather than categorical. 9.2 Obtaining p-values with afex::mixed Despite some of the concerns about the validity of the \\(p\\)-values for \\(F\\)-tests of the fixed effects, they are often useful (if only to satisfy reviewers of your paper). Packages such as pbkrtest (Højsgaard 2020) and lmerTest (Kuznetsova, Bruun Brockhoff, and Haubo Bojesen Christensen 2020) have been developed to provide these for mixed-effects models estimated with the lmer() function, using the Kenward-Roger or parametric bootstrap, and Satterthwaite approximations, respectively. The afex package (Singmann et al. 2022) provides a common interface to the functionality of these packages, via its mixed function. The mixed function offers some other convenient features, such as automatically using sum-to-zero contrasts (via contr.sum()), although I prefer setting my own contrasts and turn this off. Some of the main arguments to the mixed function (see ?mixed for the full overview) are: formula: a two-sided linear formula describing both the fixed-effects and random-effects part of the model, with the response on the left of the ~ operator and predictors and random effects on the right-hand side of the ~ operator. data: A data.frame, which needs to be in the so-called “long” format, with a single row per observation. This may be different from what you might be used to when dealing with repeated-measures. A repeated-measures ANOVA in SPSS requires data in the “wide” format, where you use columns for the different repeated measures. Data in the “wide” format has a single row for each participants. In the “long” format, you will have multiple rows for the data from a single grouping level (e.g., participant, lab, etc.). type: Sums of Squares type to use (1, 2, or 3). Default is 3. method: Character vector indicating which approximation method to use for obtaining the p-values. \"KR\" for the Kenward-Roger approximation, \"S\" for the Satterthwaite approximation (the default), \"PB\" for a parametric bootstrap, and \"LRT\" for the likelihood ratio test. test_intercept: Logical variable indicating whether to obtain a test of the fixed intercept (only for Type 3 SS). Default is FALSE check_contrasts: Logical variable indicating whether contrasts for factors should be checked and changed to contr.sum if they are not identical to contr.sum. Default is TRUE. You should set this to FALSE if you supply your own orthogonal contrasts. expand_re: Logical variable indicating whether random effect terms should be expanded (i.e. factors transformed into contrast-coded numerical predictors) before fitting with lmer. This allows proper use of the || notation with factors. Let’s try it out! First, let’s load the package: library(afex) ## ************ ## Welcome to afex. For support visit: http://afex.singmann.science/ ## - Functions for ANOVAs: aov_car(), aov_ez(), and aov_4() ## - Methods for calculating p-values with mixed(): &#39;S&#39;, &#39;KR&#39;, &#39;LRT&#39;, and &#39;PB&#39; ## - &#39;afex_aov&#39; and &#39;mixed&#39; objects can be passed to emmeans() for follow-up tests ## - NEWS: emmeans() for ANOVA models now uses model = &#39;multivariate&#39; as default. ## - Get and set global package options with: afex_options() ## - Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts() ## - For example analyses see: browseVignettes(&quot;afex&quot;) ## ************ ## ## Attaching package: &#39;afex&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer Note that after loading the afex package, the lmer function from lme4 will be “masked” and the corresponding function from the afex namespace will be used (it is actually the same as the one from the lmerTest namespace), which is mostly the same, but expands the class of the returned object somewhat. Afterwards, you either have to use lme4::lmer whenever you explicitly want the function from the lme4 package, or avoid loading the afex package, and always type e.g. afex::mixed. Either is fine, and mostly you wouldn’t need to worry, but sometimes the overriding of function names in the global workspace can give confusion and unexpected results, so it is good to be aware if this behaviour. In the code below, I use the mixed function to estimate the model and compute \\(p\\)-values for the fixed effect of anchor and the intercept with the \"KR\" option (note that this takes some time!): afmodg &lt;- mixed(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring, check_contrasts = FALSE, test_intercept = TRUE, method=&quot;KR&quot;) The class of the returned object saved as afmodg is different from the usual one returned by the lmer function. To get the \\(F\\) tests, you can just type in the name of the object: afmodg ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: anchoring ## Effect df F p.value ## 1 (Intercept) 1, 28.88 4759.47 *** &lt;.001 ## 2 anchor 1, 29.66 427.80 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 You can also use the summary() function to obtain the parameter estimates: summary(afmodg) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: data ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 2470481 1572 ## anchor1 36036021 6003 -0.80 ## Residual 87887265 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22840.77 329.42 27.49 69.34 &lt;2e-16 *** ## anchor1 23578.76 1138.95 27.32 20.70 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 Note that we now also get \\(p\\)-values for the effects (which are here derived from the Satterthwaite approximation to the degrees of freedom). That is, as mentioned before, because by loading the afex package, the lme4::lmer() function is masked and replaced by the lmerTest::lmer() function. This function uses the lme4::lmer() function to estimate the model, but then adds further results to compute the Satterthwaite degrees of freedom. If you like, you could force the use of the lme4::lmer() function, by specifying the appropriate namespace, as in e.g. lmermodg &lt;- lme4::lmer(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring) Continuing our example, we can also estimate the model without correlation between the random effects as follows: afmodr &lt;- mixed(everest_feet ~ anchor + (1 + anchor||referrer), data=anchoring, check_contrasts = FALSE, test_intercept = TRUE, expand_re = TRUE, method=&quot;KR&quot;) and get the \\(F\\) tests for this model: afmodr ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: everest_feet ~ anchor + (1 + anchor || referrer) ## Data: anchoring ## Effect df F p.value ## 1 (Intercept) 1, 28.86 4843.68 *** &lt;.001 ## 2 anchor 1, 29.69 424.13 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 and parameter estimates summary(afmodr) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## Data: data ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 re1.anchor1 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22773.65 326.69 29.08 69.71 &lt;2e-16 *** ## anchor1 23526.08 1141.84 28.22 20.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.011 Note that entering these two mixed models as is into the anova function will not provide the desired re-estimation of the models by maximum likelihood: anova(afmodr,afmodg) ## Data: data ## Models: ## afmodr: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## afmodg: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## afmodr 5 97970 98002 -48980 97960 ## afmodg 6 97956 97995 -48972 97944 15.781 1 7.111e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This is not clear from the output (apart from the missing refitting model(s) with ML (instead of REML) message). For the correct results, you will need to provide the lmer model, stored in the afmodr and afmodg objects under $full_model: anova(afmodr$full_model, afmodg$full_model) ## refitting model(s) with ML (instead of REML) ## Data: data ## Models: ## afmodr$full_model: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## afmodg$full_model: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## afmodr$full_model 5 98000 98032 -48995 97990 ## afmodg$full_model 6 97985 98024 -48987 97973 16.339 1 5.296e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Although the differences are small, the first test compares the “-2 log REML”, instead of the desired “-2 log ML” values. The assumptions underlying the likelihood-ratio test require the latter. 9.3 Crossed random effects The lme4::lmer() function (and the afex::mixed() function which that is built on top of that function) allows you to specify multiple sources for random effects. In the book, we discussed an example with crossed random effects, using the speeddate data. In this data, participants (identified by the iid variable) provide ratings of various attributes of different dating partners (identified by the pid variable). Here, iid refers to the Person factor, and pid to the Item factor. To estimate a model with crossed random effects, you simply include two separate statements for the random effects in the model formula, one for the effects depending on iid, and one for the random effects depending on pid: data(&quot;speeddate&quot;) dat &lt;- speeddate # center the predictors dat$other_attr_c &lt;- center(dat$other_attr) dat$other_intel_c &lt;- center(dat$other_intel) ## estimate the model crmod &lt;- mixed(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c*other_intel_c|iid) + (1 + other_attr_c*other_intel_c|pid), data=dat, test_intercept = TRUE, method=&quot;KR&quot;) ## Warning: Due to missing values, reduced number of observations to 1509 ## Numerical variables NOT centered on 0: other_attr_c, other_intel_c ## If in interactions, interpretation of lower order (e.g., main) effects difficult. ## boundary (singular) fit: see help(&#39;isSingular&#39;) Note there are some warnings. The first states that, due to missing values, the number of observations is reduced. If a row in the dataset has a missing value on either the dependent variable to one of the predictors, it is not possible to compute an error or model prediction. Hence, such rows are removed from the dataset before fitting the model. The second warning, starting with Numerical variables NOT centered on 0 we can ignore, as it is erroneous (we have centered the numeric predictors). The checks performed by afex::mixed() sometimes miss the mark. The third warning regarding boundary singular fit (boundary (singular) fit: see ?isSingular), is something that will require attention. It signals that the model may be too complex to estimate properly. Such issues often (but not always!) show in estimated variances of random effects of 0 (i.e., no variance) or correlations between random effects of 1 or -1. Before doing anything else, we should therefore inspect the random-effects estimates. We can view these estimates (as well as those of the fixed effects) as usual with the summary() function: summary(crmod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: other_like ~ other_attr_c * other_intel_c + (1 + other_attr_c * ## other_intel_c | iid) + (1 + other_attr_c * other_intel_c | pid) ## Data: data ## ## REML criterion at convergence: 4673.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.6285 -0.5182 0.0161 0.5374 4.0147 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## iid (Intercept) 0.0787745 0.28067 ## other_attr_c 0.0234085 0.15300 -0.54 ## other_intel_c 0.0230445 0.15180 0.50 -0.89 ## other_attr_c:other_intel_c 0.0008858 0.02976 -0.43 -0.27 0.54 ## pid (Intercept) 0.2848115 0.53368 ## other_attr_c 0.0423056 0.20568 -0.47 ## other_intel_c 0.0489423 0.22123 0.11 -0.63 ## other_attr_c:other_intel_c 0.0019882 0.04459 -0.36 0.27 -0.16 ## Residual 0.9630700 0.98136 ## Number of obs: 1509, groups: iid, 102; pid, 102 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 6.224027 0.069972 112.566265 88.950 &lt; 2e-16 ## other_attr_c 0.555403 0.033642 119.387075 16.509 &lt; 2e-16 ## other_intel_c 0.374503 0.039240 89.115616 9.544 2.78e-15 ## other_attr_c:other_intel_c -0.009401 0.013144 43.430420 -0.715 0.478 ## ## (Intercept) *** ## other_attr_c *** ## other_intel_c *** ## other_attr_c:other_intel_c ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) othr_t_ othr_n_ ## other_ttr_c -0.365 ## other_ntl_c 0.118 -0.557 ## othr_tt_:__ -0.261 0.011 0.079 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) In this case, none of the variances are estimated as 0, and none of the correlations are estimated as 1 or -1. But the estimated variances of the random other_attr_c:other_intel_c interactions are quite low, and close to 0. As such, it might make sense to remove these effects. crmod2 &lt;- mixed(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c + other_intel_c|iid) + (1 + other_attr_c + other_intel_c|pid), data=dat, test_intercept = TRUE, method=&quot;KR&quot;) ## Warning: Due to missing values, reduced number of observations to 1509 ## Numerical variables NOT centered on 0: other_attr_c, other_intel_c ## If in interactions, interpretation of lower order (e.g., main) effects difficult. ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning: Model failed to converge with 1 negative eigenvalue: -2.2e+03 This does not resolve the issue, as we still get the boundary (singular) fit warning. Moreover, we now get another warning: Model failed to converge with 1 negative eigenvalue. Checking the parameter estimates: summary(crmod2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: other_like ~ other_attr_c * other_intel_c + (1 + other_attr_c + ## other_intel_c | iid) + (1 + other_attr_c + other_intel_c | pid) ## Data: data ## ## REML criterion at convergence: 4717 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.4392 -0.5571 0.0336 0.5380 3.9036 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## iid (Intercept) 0.06644 0.2578 ## other_attr_c 0.02636 0.1624 -0.60 ## other_intel_c 0.02023 0.1422 0.75 -0.98 ## pid (Intercept) 0.30244 0.5499 ## other_attr_c 0.01746 0.1321 -1.00 ## other_intel_c 0.03484 0.1867 0.28 -0.28 ## Residual 1.05820 1.0287 ## Number of obs: 1509, groups: iid, 102; pid, 102 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 6.20784 0.07028 117.29792 88.331 &lt;2e-16 *** ## other_attr_c 0.53693 0.02948 118.54368 18.211 &lt;2e-16 *** ## other_intel_c 0.37939 0.03634 92.46214 10.441 &lt;2e-16 *** ## other_attr_c:other_intel_c -0.01269 0.01056 136.72908 -1.202 0.231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) othr_t_ othr_n_ ## other_ttr_c -0.530 ## other_ntl_c 0.217 -0.467 ## othr_tt_:__ -0.132 -0.022 0.058 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) shows a rather high negative correlation between the iid-wise random effects of other_attr_c and other_intel_c. Perhaps allowing the random effects to be correlated is too complex for this data. Hence, we can try a model with independent random effects: crmod3 &lt;- mixed(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c + other_intel_c||iid) + (1 + other_attr_c + other_intel_c||pid), data=dat, test_intercept = TRUE, method=&quot;KR&quot;) ## Warning: Due to missing values, reduced number of observations to 1509 ## Numerical variables NOT centered on 0: other_attr_c, other_intel_c ## If in interactions, interpretation of lower order (e.g., main) effects difficult. This eliminates the boundary singular fit issue. Inspecting the random-effects estimates shows no clear signs of further problems, as all the variances are well-above 0: summary(crmod3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: other_like ~ other_attr_c * other_intel_c + (1 + other_attr_c + ## other_intel_c || iid) + (1 + other_attr_c + other_intel_c || pid) ## Data: data ## ## REML criterion at convergence: 4722.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.4949 -0.5229 -0.0019 0.5369 3.9118 ## ## Random effects: ## Groups Name Variance Std.Dev. ## iid (Intercept) 0.064981 0.25491 ## iid.1 other_attr_c 0.016389 0.12802 ## iid.2 other_intel_c 0.009267 0.09627 ## pid (Intercept) 0.260186 0.51008 ## pid.1 other_attr_c 0.038972 0.19741 ## pid.2 other_intel_c 0.049752 0.22305 ## Residual 0.992730 0.99636 ## Number of obs: 1509, groups: iid, 102; pid, 102 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 6.23128 0.06724 115.11853 92.673 &lt; 2e-16 *** ## other_attr_c 0.55284 0.03177 130.82879 17.399 &lt; 2e-16 *** ## other_intel_c 0.36684 0.03763 90.58968 9.749 8.94e-16 *** ## other_attr_c:other_intel_c -0.01010 0.01249 314.62283 -0.808 0.42 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) othr_t_ othr_n_ ## other_ttr_c -0.021 ## other_ntl_c -0.016 -0.153 ## othr_tt_:__ -0.066 -0.055 0.129 Although the “keep it maximal” idea is good in theory, in practice, maximal models are often difficult or impossible to estimate. Determining an appropriate random-effects structure may require reducing the complexity of the model iteratively. Some guidelines for this process are provided by Matuschek et al. (2017). In this case, the results regarding the fixed effects are robust over the different random-effects structures. For instance, comparing the maximal model to the final model shows that the same effects are significant: crmod ## Warning: lme4 reported (at least) the following warnings for &#39;full&#39;: ## * boundary (singular) fit: see help(&#39;isSingular&#39;) ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: other_like ~ other_attr_c * other_intel_c + (1 + other_attr_c * ## Model: other_intel_c | iid) + (1 + other_attr_c * other_intel_c | ## Model: pid) ## Data: dat ## Effect df F p.value ## 1 (Intercept) 1, 122.81 7783.26 *** &lt;.001 ## 2 other_attr_c 1, 120.32 263.45 *** &lt;.001 ## 3 other_intel_c 1, 100.35 86.97 *** &lt;.001 ## 4 other_attr_c:other_intel_c 1, 48.28 0.42 .521 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 crmod3 ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: other_like ~ other_attr_c * other_intel_c + (1 + other_attr_c + ## Model: other_intel_c || iid) + (1 + other_attr_c + other_intel_c || ## Model: pid) ## Data: dat ## Effect df F p.value ## 1 (Intercept) 1, 120.68 8535.99 *** &lt;.001 ## 2 other_attr_c 1, 123.23 299.68 *** &lt;.001 ## 3 other_intel_c 1, 101.15 93.80 *** &lt;.001 ## 4 other_attr_c:other_intel_c 1, 353.92 0.63 .430 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The \\(F\\)-values of the last model are higher, which is consistent with the increased power that generally results from simpler random-effects structures (at a potential cost of increasing Type I error). In any case, knowing that the maximal and simpler random-effects structures both would lead to the same substantive conclusions provides some reassurance that the results are robust. The afex package also has a useful “vignette” (a longer document describing an analysis and package functionality) on mixed-effects models, which you can view through the command: vignette(&quot;afex_mixed_example&quot;, package=&quot;afex&quot;) 9.4 Automatically finding optimal random effects structures with the buildmer package A common issue with “maximal” linear mixed-effects models is that not all random effects can be estimated. These estimation issues are noted by warnings from lme4 such as boundary (singular) fit or Model failed to converge. These warnings indicate that the model is too complex for the data at hand. Matuschek et al. (2017) propose to search for a model with a parsimonious random effects structure. Starting with the maximal model, they propose to eliminate random effects from the model which don’t lead to a significant reduction in the likelihood ratio. This backwards search procedure is implemented by the buildmer() function from the buildmer package. By default, the buildmer() function will find the maximal model that can be estimated without issues, and then reduces the complexity of this model by removing random effects through model comparisons via likelihood ratio tests, removing terms in the order of their contribution to the likelihood ratio until the test is significant (i.e. until removal leads to a significantly worse fitting model). The basic usage of the buildmer() function is similar to the lme4::lmer() and afex::mixed() functions: you supply a model formula with the intended maximal model and a data.frame containing the data. The buildmer() function will then estimate all possible subsets of this model and perform model comparisons between them. The procedure can be fine-tuned via the buildmerControl argument. For example, instead of the default backwards search procedure (starting from the most complex model and eliminating terms), the direction can be changed to a forwards procedure where the starting point is the simplest model, and random effects terms are added until the increase in the likelihood ratio is not significant. It is also possible to change the criterion for elimination from a likelihood-ratio test to a model selection criterion such as the Akaike Information Criterion (AIC). In the include argument, you can specify which terms will always be included. By default, the buildmer() function will also consider removing fixed effects. This can be avoided by specifying the fixed effects in the model formula provided as the include argument. Other arguments are used to specify further calculations on the final model. For example, whether to calculate an ANOVA table (calc.anova = TRUE) and how to approximate the degrees of freedom for this (e.g. ddf = \"Satterthwaite\"). In the code below, we use the buildmer() function to find the parsimonious maximal model for the speeddate data. The (rather extensive) list of messages show all the models fitted by the buildmer() function, which includes models without random effects (estimated via the lm function) as well as models with random effects (estimated via the lmer function). library(buildmer) max_mod &lt;- buildmer(other_like ~ other_attr_c*other_intel_c + (1 + other_attr_c*other_intel_c|iid) + (1 + other_attr_c*other_intel_c|pid), data=dat, buildmerControl = buildmerControl(include = ~ other_attr_c*other_intel_c, calc.anova = TRUE, ddf = &quot;Satterthwaite&quot;)) ## Determining predictor order ## Fitting via gam, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c ## Currently evaluating LRT for: 1 | iid, 1 | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 | pid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 | pid) ## Currently evaluating LRT for: 1 | iid, other_attr_c | pid, ## other_intel_c | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 | pid) + (1 | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c | ## pid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_intel_c | ## pid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c | pid) ## Currently evaluating LRT for: 1 | iid, other_intel_c | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c | ## pid) + (1 | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c | pid) + (1 | iid) ## Currently evaluating LRT for: other_attr_c | iid, other_intel_c | iid, ## other_intel_c | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c | ## pid) + (1 + other_attr_c | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c | ## pid) + (1 + other_intel_c | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 | iid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c + other_intel_c | ## pid) + (1 | iid) ## Currently evaluating LRT for: other_attr_c | iid, other_intel_c | iid, ## other_attr_c:other_intel_c | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 + other_attr_c | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 + other_intel_c | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 | iid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c + other_intel_c | ## pid) + (1 + other_attr_c | iid) ## Currently evaluating LRT for: other_intel_c | iid, ## other_attr_c:other_intel_c | pid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 + other_attr_c + other_intel_c | iid) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c | iid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c | pid) + (1 + other_attr_c | iid) ## Currently evaluating LRT for: other_intel_c | iid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c + other_intel_c | iid) ## Updating formula: other_like ~ 1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c + (1 + other_attr_c + other_intel_c + ## other_attr_c:other_intel_c | pid) + (1 + other_attr_c + ## other_intel_c | iid) ## Currently evaluating LRT for: other_attr_c:other_intel_c | iid ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c + other_intel_c + other_attr_c:other_intel_c | iid) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Ending the ordering procedure due to having reached the maximal ## feasible model - all higher models failed to converge. The types of ## convergence failure are: Singular fit ## Fitting ML and REML reference models ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c + other_intel_c | iid) ## Fitting via lmer, with ML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c + other_intel_c | iid) ## Convergence failure. Reducing terms and retrying... The failures were: ## lme4 reports not having converged (-1) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c | iid) ## Fitting via lmer, with ML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c | pid) + (1 + ## other_attr_c | iid) ## Convergence failure. Reducing terms and retrying... The failures were: ## lme4 reports not having converged (-1) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 + other_attr_c | iid) ## Fitting via lmer, with ML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 + other_attr_c | iid) ## Convergence failure. Reducing terms and retrying... The failures were: ## lme4 reports not having converged (-1) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 | iid) ## Fitting via lmer, with ML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) + (1 | iid) ## Testing terms ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_intel_c | ## pid) + (1 | iid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c + ## other_intel_c | pid) ## Fitting via lmer, with REML: other_like ~ 1 + other_attr_c + ## other_intel_c + other_attr_c:other_intel_c + (1 + other_attr_c | ## pid) + (1 | iid) ## grouping term block score Iteration ## 1 &lt;NA&gt; 1 &lt;NA&gt; NA 1 ## 2 &lt;NA&gt; other_attr_c &lt;NA&gt; NA 1 ## 3 &lt;NA&gt; other_intel_c &lt;NA&gt; NA 1 ## 4 &lt;NA&gt; other_attr_c:other_intel_c &lt;NA&gt; NA 1 ## 9 pid 1 NA pid 1 -56.40453 1 ## 10 pid other_attr_c NA pid other_attr_c -30.96129 1 ## 5 iid 1 NA iid 1 -18.53191 1 ## 11 pid other_intel_c NA pid other_intel_c -12.44827 1 ## LRT ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 9 NA ## 10 2.507495e-17 ## 5 1.218785e-09 ## 11 3.924500e-06 ## All terms are significant ## Finalizing by converting the model to lmerTest The final model contains random intercepts and slopes for other_attr_c and other_intel_c for pid. For the iid groupings, only random intercepts are included: summary(max_mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: ## other_like ~ 1 + other_attr_c + other_intel_c + other_attr_c:other_intel_c + ## (1 + other_attr_c + other_intel_c | pid) + (1 | iid) ## Data: dat ## ## REML criterion at convergence: 4717.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.5087 -0.5416 0.0032 0.5416 3.8371 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## pid (Intercept) 0.25953 0.5094 ## other_attr_c 0.04888 0.2211 -0.46 ## other_intel_c 0.05387 0.2321 0.08 -0.60 ## iid (Intercept) 0.08821 0.2970 ## Residual 1.04247 1.0210 ## Number of obs: 1509, groups: pid, 102; iid, 102 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 6.212115 0.068184 116.886154 91.107 &lt; 2e-16 ## other_attr_c 0.556152 0.031065 105.468067 17.903 &lt; 2e-16 ## other_intel_c 0.362421 0.036769 80.839156 9.857 1.66e-15 ## other_attr_c:other_intel_c -0.007367 0.010784 466.324439 -0.683 0.495 ## ## (Intercept) *** ## other_attr_c *** ## other_intel_c *** ## other_attr_c:other_intel_c ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) othr_t_ othr_n_ ## other_ttr_c -0.279 ## other_ntl_c 0.022 -0.466 ## othr_tt_:__ -0.116 -0.054 0.116 References "],["generalized-linear-models.html", "Chapter 10 Generalized linear models 10.1 Model estimation and inference 10.2 Confidence intervals 10.3 Model predictions 10.4 Alternative link functions for logistic regression 10.5 Quasi-Poisson regression 10.6 Log-linear models 10.7 Multinomial regression 10.8 Generalized linear mixed effects models", " Chapter 10 Generalized linear models In this chapter, we will first illustrate the main methods of estimation, inference, and model checking with a logistic regression model. We will then go on to describe extensions to other generalized linear (mixed-effects) models. As we will see, most generalized linear models can be estimated with the glm() function, which works similarly to the lm() function, but contains an additional family argument to specify the distribution of the dependent variable and the link function to be used. After discussing models estimated via the glm() function, we will move on to estimating log-linear models via the loglm() function from the MASS package, multinomial regression models via the multinom() function from the nnet package, and finally generalized linear mixed effects models with the glmer() function from the lme4 package, or the mixed() function from the afex package. 10.1 Model estimation and inference Most generalized linear models can be estimated with the glm() function. The first argument of this function (formula) should be a formula specifying the predictors of the model. This part is equivalent to how you specify a model formula in the lm() function. So factors will be converted into contrast-coded predictors according to the contrasts() set for those factors. And in the formula, a + is used to include a variable “as is”, whilst separating variables by a * indicates including both main effects and interactions of these variables (see Table 9.1). What is new in the glm() function is the family argument, which is used to define both the random component of the model (i.e. the conditional distribution of the dependent variable) and the link function. For logistic regression, we should specify the family argument as binomial(). The binomial() family uses a logistic link function by default, and a call to binomial() is identical to binomial(link=\"logit\"). The following code estimates a logistic regression model to a subset of the metacognition data: library(sdamr) data(&quot;metacognition&quot;) # select the subset for participant 1 dat &lt;- subset(metacognition, id == 1) # center confidence and contrast dat$confidence &lt;- center(dat$confidence) dat$contrast &lt;- center(dat$contrast) # specify the glm model mod_logistic &lt;- glm(correct ~ confidence * contrast, family=binomial(), data=dat) You can see the parameter estimates and the results of Wald tests (testing for each parameter the null hypothesis that the true value equals 0) through the summary() function: summary(mod_logistic) ## ## Call: ## glm(formula = correct ~ confidence * contrast, family = binomial(), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7208 0.1124 0.1867 0.5837 1.4906 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.850776 0.233722 7.919 2.40e-15 *** ## confidence 0.037734 0.007389 5.106 3.28e-07 *** ## contrast 0.503369 0.108244 4.650 3.31e-06 *** ## confidence:contrast 0.008201 0.003249 2.524 0.0116 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 397.94 on 377 degrees of freedom ## Residual deviance: 273.66 on 374 degrees of freedom ## AIC: 281.66 ## ## Number of Fisher Scoring iterations: 6 Results under the header Coefficients: show the parameter estimates \\(\\hat{\\beta}_j\\) (Estimate), standard errors \\(\\text{SE}(\\hat{\\beta}_j)\\) (Std. Error), and Wald tests (z value for the test statistic, and Pr(&gt;|z|) for the \\(p\\)-value). Above this, under the header Deviance Residuals:, the output also provides a summary of the residual deviances for each observation. These deviances are also available via the residuals() function, by calling e.g. residuals(mod_logistic, type=\"deviance\"). The standardized Pearson residuals can be computed via the same function, calling residuals(mod_logistic, type=\"pearson\"). We will come back to computing residuals later, when discussing Poisson regression. Likelihood-ratio tests can be obtained via the Anova() function of the car package, setting type=3 to perform model comparisons of the full MODEL G to alternative MODEL R’s which exclude each predictor (i.e. like the Wald test testing the null-hypothesis that the true slope equals 0): car::Anova(mod_logistic, type=3) ## Analysis of Deviance Table (Type III tests) ## ## Response: correct ## LR Chisq Df Pr(&gt;Chisq) ## confidence 29.8242 1 4.731e-08 *** ## contrast 24.0379 1 9.446e-07 *** ## confidence:contrast 6.6468 1 0.009933 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that this function does not provide a likelihood-ratio test of the intercept. If you really want this test as well, you can obtain this by estimating a model without an intercept, and then using the anova() function to compare this model to the model with an intercept. To estimate a model without an intercept, you need to add -1 in the model formula: mod_logistic_0 &lt;- glm(correct ~ -1 + confidence * contrast, family=binomial(), data=dat) To get a likelihood-ratio test comparing this model to the previous one, you to use the anova() function with the argument test = \"LRT\": anova(mod_logistic_0, mod_logistic, test=&quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: correct ~ -1 + confidence * contrast ## Model 2: correct ~ confidence * contrast ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 375 368.10 ## 2 374 273.66 1 94.434 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.2 Confidence intervals Profile likelihood confidence intervals for the parameters can be obtained with the confint() function. confint(mod_logistic) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 1.420750411 2.34632015 ## confidence 0.023796104 0.05304488 ## contrast 0.298046450 0.72633736 ## confidence:contrast 0.001951436 0.01480515 By default, this computes 95% confidence intervals. You can change this via the level argument. For example, a 90% confidence interval can be computed as: confint(mod_logistic, level=.9) ## Waiting for profiling to be done... ## 5 % 95 % ## (Intercept) 1.48654679 2.26100030 ## confidence 0.02597812 0.05045421 ## contrast 0.33035069 0.68880772 ## confidence:contrast 0.00294661 0.01370484 Whilst not recommended as they are less accurate, confidence intervals based on the Normal approximation can also be computed. Wald confidence intervals are defined as \\[\\hat{\\beta}_j \\pm z_{1-\\tfrac{1}{2} \\alpha} \\text{SE}(\\hat{\\beta}_j)\\] This can be easily computed from the information available via the summary() function, which provides parameter estimates and standard errors. The summary() function returns a list, which can be stored as a new object in order to use. You can view all the elements in the list by typing e.g. str(summary(mod_logistic)) in the console. The coefficients and standard errors are part of the coefficients element in the list, which contains a matrix with parameter estimates, standard errors, and the results of the Wald tests. The confidence intervals are computed by taking the column of the matrix containing the parameter estimates, and adding and subtracting the column with standard errors, multiplied by the critical \\(z\\)-value computed via the qnorm() function: # store coefficients matrix est &lt;- summary(mod_logistic)$coefficients # compute the critical Z value (alpha = .05 for 95% confidence interval) qz &lt;- qnorm(1-(.05)/2) # compute lower and upper confidence bounds cbind(est[,1] - qz*est[,2], est[,1] + qz*est[,2]) ## [,1] [,2] ## (Intercept) 1.392689995 2.30886155 ## confidence 0.023250692 0.05221643 ## contrast 0.291214848 0.71552335 ## confidence:contrast 0.001833409 0.01456941 Compared to the confidence intervals based on the profile likelihood, the Wald confidence are a somewhat wider. If you are to use a piece of code repeatedly, it is generally a good idea to define it as a function that can be re-used. This is more efficient. It can also avoid errors, as you would only need to correct errors once in the definition of the function, and not everywhere you would otherwise perform the computation. The code below defines a new function zconfint(), which provides a simple alternative to the confint() function using the Wald approximation and a 95% confidence interval by default: z_confint &lt;- function(mod, level=.95) { est &lt;- summary(mod)$coefficients out &lt;- cbind(est[,1] - qnorm(1-(1-level)/2)*est[,2], est[,1] + qnorm(1-(1-level)/2)*est[,2]) colnames(out) &lt;- c(paste(100*(1-.95)/2,&quot;%&quot;), paste(100 - 100*(1-.95)/2,&quot;%&quot;)) return(out) } You can now use this function simply as: z_confint(mod_logistic) ## 2.5 % 97.5 % ## (Intercept) 1.392689995 2.30886155 ## confidence 0.023250692 0.05221643 ## contrast 0.291214848 0.71552335 ## confidence:contrast 0.001833409 0.01456941 and compute a 90% confidence interval by explicitly setting the level argument: z_confint(mod_logistic, level=.9) ## 2.5 % 97.5 % ## (Intercept) 1.466338069 2.23521347 ## confidence 0.025579154 0.04988797 ## contrast 0.325323639 0.68141456 ## confidence:contrast 0.002857215 0.01354560 The above is mainly meant for didactic purposes. The profile-likelihood confidence intervals computed with the confint() function are generally preferred over the Wald approximation implemented via this new z_confint() function. 10.3 Model predictions The predict() function can be used to extract the model predictions. Model predictions can be obtained on the scale of the link function (type=\"link\", the default), but also on the scale of the response (type=\"response\"). The former are just the values of the linear function of the model, i.e. \\[\\hat{y}_\\text{link} = \\hat{\\beta}_0 + \\sum_{j=1}^m \\hat{\\beta}_j \\times X_{j,i}\\] The latter are transformations of these predictions via the inverse-link function: \\[\\hat{y}_\\text{response} = h(\\hat{\\beta}_0 + \\sum_{j=1}^m \\hat{\\beta}_j \\times X_{j,i})\\] We can use the predictions to get a rough overview of the predicted probability of a correct response as a function of the (centered) confidence, for each of the levels of (centered) contrast as follows: # store model predictions in the data frame dat$pred &lt;- predict(mod_logistic, type=&quot;response&quot;) library(ggplot2) ggplot(dat,aes(x=confidence,y=pred)) + geom_point() + facet_wrap(~contrast) + ylim(c(0,1)) 10.4 Alternative link functions for logistic regression Other possible link functions for the binomial() family are probit, cauchit, log and cloglog. A model with a Probit link function can be estimated by specifying this link in the binomial() family argument as follows: mod_probit &lt;- glm(correct ~ confidence * contrast, family=binomial(&quot;probit&quot;), data=dat) summary(mod_probit) ## ## Call: ## glm(formula = correct ~ confidence * contrast, family = binomial(&quot;probit&quot;), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.77711 0.07343 0.16127 0.61259 1.39558 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.040633 0.120410 8.642 &lt; 2e-16 *** ## confidence 0.020422 0.004071 5.016 5.26e-07 *** ## contrast 0.280031 0.061099 4.583 4.58e-06 *** ## confidence:contrast 0.004224 0.001871 2.257 0.024 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 397.94 on 377 degrees of freedom ## Residual deviance: 272.29 on 374 degrees of freedom ## AIC: 280.29 ## ## Number of Fisher Scoring iterations: 7 car::Anova(mod_probit, type=3) ## Analysis of Deviance Table (Type III tests) ## ## Response: correct ## LR Chisq Df Pr(&gt;Chisq) ## confidence 27.2776 1 1.762e-07 *** ## contrast 23.4433 1 1.286e-06 *** ## confidence:contrast 5.2805 1 0.02157 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The cauchit link function is the Cauchy cumulative probability distribution: \\[g(y) = 0.5 + \\frac{\\arctan(y)}{\\pi}\\] and the cloglog the complementary-log-log link function: \\[g(y) = \\log\\left(-\\log(1 - y)\\right)\\] ## Poisson regression A Poisson regression model can be estimated by specifying the family as family=poisson(). The default link is the (canonical) log link function, and hence family=poisson() is the same as family=poisson(link=\"log\"). Other possible link functions are identity (\\(g(y) = y\\)) and sqrt (\\(g(y) = \\sqrt{y}\\)). The analysis of the gestures data in the SDAM book can be replicated with the following code, which first sets the contrasts applied to the context, language, and gender factors, and then creates a new variable for the offset in the model (which is the \\(\\log(\\texttt{duration})\\), and supplied as the offset argument to thee glm() function: # load the data data(&quot;gestures&quot;) # set the contrasts contrasts(gestures$context) &lt;- c(1,-1) contrasts(gestures$language) &lt;- c(1,-1) contrasts(gestures$gender) &lt;- c(1,-1) # create a new variable for the offset gestures$log_d &lt;- log(gestures$dur) # estimate the model, including the offset mod_poisson &lt;- glm(gestures ~ context*language*gender, data=gestures, family=poisson(), offset=log_d) As before, we can obtain the parameter estimates as Wald tests via the summary() function: summary(mod_poisson) ## ## Call: ## glm(formula = gestures ~ context * language * gender, family = poisson(), ## data = gestures, offset = log_d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.5549 -1.7616 0.0867 1.3777 3.5992 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.96553 0.01982 -48.703 &lt; 2e-16 *** ## context1 0.04595 0.01982 2.318 0.02046 * ## language1 0.02877 0.01982 1.451 0.14677 ## gender1 0.06223 0.01982 3.139 0.00169 ** ## context1:language1 -0.03608 0.01982 -1.820 0.06877 . ## context1:gender1 -0.02598 0.01982 -1.310 0.19007 ## language1:gender1 -0.04170 0.01982 -2.104 0.03542 * ## context1:language1:gender1 0.03724 0.01982 1.878 0.06031 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 326.95 on 53 degrees of freedom ## Residual deviance: 303.96 on 46 degrees of freedom ## AIC: 623.09 ## ## Number of Fisher Scoring iterations: 4 And we can obtain likelihood-ratio tests via car::Anova(): car::Anova(mod_poisson, type=3) ## Analysis of Deviance Table (Type III tests) ## ## Response: gestures ## LR Chisq Df Pr(&gt;Chisq) ## context 5.3915 1 0.020235 * ## language 2.1118 1 0.146170 ## gender 9.8764 1 0.001674 ** ## context:language 3.3206 1 0.068416 . ## context:gender 1.7183 1 0.189909 ## language:gender 4.4396 1 0.035115 * ## context:language:gender 3.5325 1 0.060178 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output of the summary() function includes a summary of the residual deviances. These residuals seem relatively large, but standardized Pearson residuals have a more straightforward interpretation. We can get a similar summary of the standardized Pearson residuals by calling the summary() function on the output of residuals(type=\"pearson\"): summary(residuals(mod_poisson, type=&quot;pearson&quot;)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -5.46630 -1.67602 0.08694 -0.04635 1.41827 3.85069 Under the null-hypothesis that the model fits the data well, the standardized Pearson residuals follow a standard Normal distribution. That implies that absolute values larger than 3 should be very unlikely. Inh this model, such unlikely residuals are often present. It is useful to plot a hisogram of the standardized Pearson residuals for generalized linear models. The following code provides a histogram and overlays a standard Normal distribution (which is the distribution of the standardized Pearson residuals under the null-hypothesis that the model in question is equal to the true model, in the sense that the probability distribution over the dependent variable equals the actual distribution of the dependent variable). A histogram depicts counts on the \\(y\\)-axis, but a density function such as the standard Normal density function depicts a probability distribution. These have different scales: the area under the curve of a probability density is equal 1, whilst the area covered by a histogram is equal to \\(n \\times w\\), where \\(n\\) is the number of observations and \\(w\\) the width of the bins in the hisotgram. To make these commensurable, we therefore need to rescale the standard Normal density function to the scale of the hisogram. In the code below, this rescaling is done via the bw (\\(w\\)) and n_obs (\\(n\\)) arguments, computed first and then used to rescale the output of the dnorm function (which provides the values of the Normal density function). The result is then provided as the fun argument to the stat_function() function of the ggplot2 package: library(ggplot2) bw &lt;- 1 # binwidth n_obs &lt;- length(residuals(mod_poisson, type=&quot;pearson&quot;)) # number of observations tdat &lt;- data.frame(residual = residuals(mod_poisson, type=&quot;pearson&quot;)) ggplot(tdat, aes(x=residual)) + geom_histogram(binwidth=bw, colour=&quot;black&quot;) + stat_function(fun=function(x) dnorm(x) * bw * n_obs, colour=&quot;red&quot;) + xlab(&quot;Standarized Pearson residual&quot;) + ylab(&quot;Frequency&quot;) You can compute the proportion of observations with an absolute standardized Pearson residual larger than 3 as follows: sum(abs(residuals(mod_poisson, type=&quot;pearson&quot;)) &gt; 3)/length(residuals(mod_poisson, type=&quot;pearson&quot;)) ## [1] 0.2222222 This shows that more than 20% of the observations have rather extreme Pearson residuals. 10.5 Quasi-Poisson regression Quasi-Poisson regression models can be estimated by setting the family argument to quasipoisson. As for the poisson() family, the default link function is the log link. mod_qpoisson &lt;- glm(gestures ~ context*language*gender, data=gestures, family=quasipoisson(), offset=log_d) summary(mod_qpoisson) ## ## Call: ## glm(formula = gestures ~ context * language * gender, family = quasipoisson(), ## data = gestures, offset = log_d) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.5549 -1.7616 0.0867 1.3777 3.5992 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.96553 0.04821 -20.028 &lt;2e-16 *** ## context1 0.04595 0.04821 0.953 0.346 ## language1 0.02877 0.04821 0.597 0.554 ## gender1 0.06223 0.04821 1.291 0.203 ## context1:language1 -0.03608 0.04821 -0.748 0.458 ## context1:gender1 -0.02598 0.04821 -0.539 0.593 ## language1:gender1 -0.04170 0.04821 -0.865 0.392 ## context1:language1:gender1 0.03724 0.04821 0.772 0.444 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 5.913283) ## ## Null deviance: 326.95 on 53 degrees of freedom ## Residual deviance: 303.96 on 46 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 The \\(F\\)-tests of this model can be obtained via the car::Anova() function, by setting the argument test.statistic=\"F\", as well as error.estimate = \"dispersion\" (the latter to indicate that the error variance is estimated from the estimated dispersion parameter of the quasi-Poisson model): car::Anova(mod_qpoisson, type=3, test.statistic=&quot;F&quot;, error.estimate=&quot;dispersion&quot;) ## Analysis of Deviance Table (Type III tests) ## ## Response: gestures ## Error estimate based on estimated dispersion ## ## Sum Sq Df F values Pr(&gt;F) ## context 5.392 1 0.9118 0.3446 ## language 2.112 1 0.3571 0.5530 ## gender 9.876 1 1.6702 0.2027 ## context:language 3.321 1 0.5616 0.4574 ## context:gender 1.718 1 0.2906 0.5924 ## language:gender 4.440 1 0.7508 0.3907 ## context:language:gender 3.532 1 0.5974 0.4435 ## Residuals 272.011 46 Note that the use of an \\(F\\)-test here is recommended by Dunn and Smyth (2018) because of the estimation of the dispersion parameter. You would not use this test for most generalized linear models, where the dispersion is taken as fixed. 10.6 Log-linear models Log-linear models can be estimated with the loglm() function from the MASS package. We will illustrate the use of this function with the rock-paper-scissors example discussed in the SDAM book. # load the rock-paper-scissors data data(&quot;rps&quot;) head(rps) ## id ai_strategy round human_action ai_action score ## 1 1 Level2 1 rock rock 0 ## 2 1 Level2 2 rock scissors 1 ## 3 1 Level2 3 rock paper -1 ## 4 1 Level2 4 paper rock 1 ## 5 1 Level2 5 scissors scissors 0 ## 6 1 Level2 6 paper paper 0 To analyse this data, we first create new variables for the previous human and previous AI actions. For this, we use the lag() function from the dlyr package. This function shifts values in a vector, by default by one element. So if you have a vector c(1,2,3), the result of lag(c(1,2,3)) = c(NA,1,2) (all elements are shifted right, and the first element is set to a missing value NA). dat &lt;- rps # create lagged variables dat$previous_human &lt;- dplyr::lag(dat$human_action) dat$previous_ai &lt;- dplyr::lag(dat$ai_action) # set the value in the first round to NA for all participants dat$previous_human[dat$round == 1] &lt;- NA dat$previous_ai[dat$round == 1] &lt;- NA # select a subset of the data with only the last half of each game # and only the &quot;level 1&quot; AI opponent dat &lt;- subset(dat, round &gt; 25 &amp; ai_strategy == &quot;Level1&quot;) We can now use the table() function to create a cross-tabulation of the three variables of interest tab &lt;- table(dat[,c(&quot;previous_human&quot;, &quot;previous_ai&quot;,&quot;human_action&quot;)]) tab ## , , human_action = paper ## ## previous_ai ## previous_human paper rock scissors ## paper 12 2 7 ## rock 9 19 40 ## scissors 94 8 11 ## ## , , human_action = rock ## ## previous_ai ## previous_human paper rock scissors ## paper 45 94 17 ## rock 5 10 5 ## scissors 24 5 15 ## ## , , human_action = scissors ## ## previous_ai ## previous_human paper rock scissors ## paper 11 10 8 ## rock 12 15 107 ## scissors 5 5 5 The loglm() function is used to estimate log-linear models, using a formula interface similar to glm() and lm(). When the supplied data is a cross-tabulation as above, the formula can contain numbers which refer to the numbers of the dimensions of the table. So, for the cross-tabulation above, 1 would refer to previous_human, 2 to previous_ai, and 3 to human_action. Independence is specified by separating the dimensions with a + sign, whilst dependence is specified by linking the dimensions with a * sign. For example, a model in which all dimensions are independent is specified as: mod_1_2_3 &lt;- MASS::loglm(~ 1 + 2 + 3, data=tab) and a model in which all dimensions are dependent as: mod_123 &lt;- MASS::loglm(~ 1*2*3, data=tab) This last model is the saturated model, and fits the data perfectly: mod_123 ## Call: ## MASS::loglm(formula = ~1 * 2 * 3, data = tab) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 0 0 1 ## Pearson 0 0 1 The independence model is the simplest model, and does not fit the data well: mod_1_2_3 ## Call: ## MASS::loglm(formula = ~1 + 2 + 3, data = tab) ## ## Statistics: ## X^2 df P(&gt; X^2) ## Likelihood Ratio 678.4482 20 0 ## Pearson 986.9995 20 0 Note that two test statistics are provided. The value under Likelihood Ratio is the \\(-2 \\log \\text{likelihood}\\) of the model compared to the saturated model. The value under Pearson contains the value of the Pearson goodness-of-fit test: \\[X^2 = \\sum_{j=1}^k \\frac{(O_j - E_j)^2}{E_j}\\] where \\(O_j\\) is the observed count for cell \\(j\\) in the multiway contingency table, and \\(E_j\\) the expected count computed according to the estimated model. As we have not discussed this test in the SDAM book, we will ignore it for now and focus on the likelihood ratio tests. The remaining other possible models are specified as follows: # human action independent from previous human and previous AI mod_12_3 &lt;- MASS::loglm(~ 1*2 + 3, data=tab) # previous AI independent from previous human and current human action mod_13_2 &lt;- MASS::loglm(~ 1*3 + 2, data=tab) # previous human action independent from previous AI and current human action mod_23_1 &lt;- MASS::loglm(~ 2*3 + 1, data=tab) mod_12_23 &lt;- MASS::loglm(~ 1*2 + 2*3, data=tab) mod_12_13 &lt;- MASS::loglm(~ 1*2 + 1*3, data=tab) mod_13_23 &lt;- MASS::loglm(~ 1*3 + 2*3, data=tab) mod_12_13_23 &lt;- MASS::loglm(~ 1*2 + 1*3 + 2*3, data=tab) By inspecting these objects, you will see that the likelihood-ratio test is significant for all of these models. Hence, the null-hypothesis that each of these models is equal to the true model is rejected. 10.7 Multinomial regression Multinomial logistic regression models can be estimated via the multinom() function of the nnet package. The function uses the usual formula interface. But there is no family() argument here. This function will always use multinomial logistic regression with a baseline logit formulation. Continuing with our example of the rock-paper-scissors data, we can estimate the multinomial logistic regression model as follows: # turn variables into factors dat$previous_ai &lt;- factor(dat$previous_ai) dat$previous_human &lt;- factor(dat$previous_human) dat$human_action &lt;- factor(dat$human_action) # set effect-coding contrasts contrasts(dat$previous_ai) &lt;- contrasts(dat$previous_human) &lt;- cbind(c(-1,1,0),c(-1,0,1)) mod_multinomial &lt;- nnet::multinom(human_action ~ previous_human + previous_ai, data=dat) ## # weights: 18 (10 variable) ## initial value 659.167373 ## iter 10 value 470.043639 ## final value 465.766168 ## converged And we can obtain the results as usual via the summary() function: summary(mod_multinomial) ## Call: ## nnet::multinom(formula = human_action ~ previous_human + previous_ai, ## data = dat) ## ## Coefficients: ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## rock 0.03037873 -1.3502186 -0.5966141 0.75578735 -0.1067017 ## scissors -0.32841978 0.6223544 -1.4315078 -0.05389995 0.7076566 ## ## Std. Errors: ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## rock 0.1356216 0.2172106 0.1796017 0.1927292 0.1940639 ## scissors 0.1505282 0.1874375 0.2240852 0.2043583 0.1727103 ## ## Residual Deviance: 931.5323 ## AIC: 951.5323 Note that there are separate rows for the logit comparing rock to the baseline paper, and the logit comparing scissors to the baseline paper. Whilst the function returns parameter estimates and standard errors, we do not get the usual Wald tests. These can be easily computed though, as the \\(z\\)-statistic for the Walk test is simply \\[z = \\frac{\\hat{\\beta}}{\\text{SE}{\\hat{\\beta}}}\\] The summary() function for multinom models returns a list. The elements of this list that we need are named coefficients and standard.errors: est &lt;- summary(mod_multinomial)$coefficients se &lt;- summary(mod_multinomial)$standard.errors z &lt;- est/se z ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## rock 0.2239963 -6.216173 -3.321874 3.9214994 -0.5498278 ## scissors -2.1817818 3.320329 -6.388230 -0.2637522 4.0973625 The corresponding \\(p\\)-values can be computed with the pnorm() function, which computes cumulative probabilities of the standard Normal distribution. For this, we take the absolute values of the \\(z\\)-statistic, and then compute the probability of obtaining a value larger than this as 1 - pnorm(abs(z)) (by default, the pnorm() function returns the probability of a value lower than its argument). Finally, for a two-sided test, we need to multiply this by 2: 2*(1-pnorm(abs(z))) ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## rock 0.82276015 5.094278e-10 8.941497e-04 8.799968e-05 5.824375e-01 ## scissors 0.02912564 8.991128e-04 1.678169e-10 7.919709e-01 4.178842e-05 Granted, whilst certainly doable, this procedure is not exactly straightforward. It is more convenient to compute likelihood-ratio tests, which are available via the car::Anova() function: car::Anova(mod_multinomial, type=3) ## Analysis of Deviance Table (Type III tests) ## ## Response: human_action ## LR Chisq Df Pr(&gt;Chisq) ## previous_human 210.124 4 &lt; 2.2e-16 *** ## previous_ai 45.408 4 3.27e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that this provides a single test for each factor, taking the effect of each contrast code for this predictor in each baseline category logit together. If you want to use a different baseline category than the default (which is the first level of the dependent variable), you can change the baseline category via the ref argument of the relevel() function: dat$human_action &lt;- relevel(dat$human_action, ref = &quot;rock&quot;) mod_multinomial2 &lt;- nnet::multinom(human_action ~ previous_human + previous_ai, data=dat) ## # weights: 18 (10 variable) ## initial value 659.167373 ## iter 10 value 468.234594 ## final value 465.766168 ## converged summary(mod_multinomial2) ## Call: ## nnet::multinom(formula = human_action ~ previous_human + previous_ai, ## data = dat) ## ## Coefficients: ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## paper -0.03035337 1.350194 0.5966090 -0.7557765 0.1066863 ## scissors -0.35881414 1.972567 -0.8347918 -0.8096717 0.8143598 ## ## Std. Errors: ## (Intercept) previous_human1 previous_human2 previous_ai1 previous_ai2 ## paper 0.1356200 0.2172092 0.1796009 0.1927278 0.1940637 ## scissors 0.1493363 0.2179507 0.2395315 0.1979328 0.1909146 ## ## Residual Deviance: 931.5323 ## AIC: 951.5323 10.8 Generalized linear mixed effects models The glmer() function of the lme4 package can be used to estimate generalized linear mixed effects models. The interface is similar to the lmer function, with an additional family argument as for glm(). For example, we can fit a mixed-effects logistic regression model for the metacognition model, with random intercepts and slopes for (centered) confidence and contrast for each participant, as follows: data(&quot;metacognition&quot;) dat &lt;- metacognition dat$confidence &lt;- center(dat$confidence) dat$contrast &lt;- center(dat$contrast) mod_mixed_logistic &lt;- lme4::glmer(correct ~ confidence * contrast + (confidence + contrast||id), family=binomial(), data=dat) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : ## Model failed to converge with max|grad| = 0.00269636 (tol = 0.002, component 1) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? summary(mod_mixed_logistic) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: correct ~ confidence * contrast + (confidence + contrast || id) ## Data: dat ## ## AIC BIC logLik deviance df.resid ## 5876.5 5925.0 -2931.2 5862.5 7553 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -28.7441 0.0402 0.1343 0.5330 1.8092 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 2.965e-01 0.544536 ## id.1 confidence 1.243e-05 0.003526 ## id.2 contrast 2.024e-02 0.142280 ## Number of obs: 7560, groups: id, 20 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.7230381 0.1340016 12.86 &lt;2e-16 *** ## confidence 0.0294809 0.0016858 17.49 &lt;2e-16 *** ## contrast 0.5332956 0.0402099 13.26 &lt;2e-16 *** ## confidence:contrast 0.0089827 0.0006359 14.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) cnfdnc cntrst ## confidence 0.208 ## contrast 0.083 0.049 ## cnfdnc:cntr 0.043 0.336 0.420 ## optimizer (Nelder_Mead) convergence code: 0 (OK) ## Model failed to converge with max|grad| = 0.00269636 (tol = 0.002, component 1) ## Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? Note the warnings about estimation issues. Estimation issues are more common in generalized linear mixed-effects models than linear mixed-effects models. A hint is provided in the warning as: - Rescale variables?. Estimation of mixed-effects models is generally easier when all predictors have the same scale, with a standard deviation equal to about 1. In this case, scaling the predictors (i.e. \\(Z\\)-transforming) provides much better results: dat$confidence &lt;- scale(dat$confidence) dat$contrast &lt;- scale(dat$contrast) mod_mixed_logistic &lt;- lme4::glmer(correct ~ confidence * contrast + (confidence + contrast||id), family=binomial(), data=dat) summary(mod_mixed_logistic) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: correct ~ confidence * contrast + (confidence + contrast || id) ## Data: dat ## ## AIC BIC logLik deviance df.resid ## 5876.5 5925.0 -2931.2 5862.5 7553 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -28.7442 0.0402 0.1343 0.5330 1.8092 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.29652 0.5445 ## id.1 confidence 0.01981 0.1408 ## id.2 contrast 0.10416 0.3227 ## Number of obs: 7560, groups: id, 20 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.72304 0.13401 12.86 &lt;2e-16 *** ## confidence 1.17701 0.06730 17.49 &lt;2e-16 *** ## contrast 1.20970 0.09121 13.26 &lt;2e-16 *** ## confidence:contrast 0.81351 0.05759 14.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) cnfdnc cntrst ## confidence 0.208 ## contrast 0.083 0.049 ## cnfdnc:cntr 0.043 0.336 0.420 Instead of the glmer() function, you can also use the mixed() function of the afex package. This has the benefit that better tests results can be computed. For generalized linear models, afex::mixed() allows method=\"LRT\" for likelihood-ratio tests, and method=\"PB\" for a parametric bootstrap test. mod_mixed_logistic2 &lt;- afex::mixed(correct ~ confidence * contrast + (confidence + contrast||id), family=binomial(), data=dat, method=&quot;LRT&quot;) ## Contrasts set to contr.sum for the following variables: id mod_mixed_logistic2 ## Mixed Model Anova Table (Type 3 tests, LRT-method) ## ## Model: correct ~ confidence * contrast + (confidence + contrast || id) ## Data: dat ## Df full model: 7 ## Effect df Chisq p.value ## 1 confidence 1 62.21 *** &lt;.001 ## 2 contrast 1 49.46 *** &lt;.001 ## 3 confidence:contrast 1 255.32 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 summary(mod_mixed_logistic2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: correct ~ confidence * contrast + (confidence + contrast || id) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 5876.5 5925.0 -2931.2 5862.5 7553 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -28.7442 0.0402 0.1343 0.5330 1.8092 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.29652 0.5445 ## id.1 confidence 0.01981 0.1408 ## id.2 contrast 0.10416 0.3227 ## Number of obs: 7560, groups: id, 20 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.72304 0.13401 12.86 &lt;2e-16 *** ## confidence 1.17701 0.06730 17.49 &lt;2e-16 *** ## contrast 1.20970 0.09121 13.26 &lt;2e-16 *** ## confidence:contrast 0.81351 0.05759 14.12 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) cnfdnc cntrst ## confidence 0.208 ## contrast 0.083 0.049 ## cnfdnc:cntr 0.043 0.336 0.420 References "],["structural-equation-modelling-with-lavaan.html", "Chapter 11 Structural Equation modelling with lavaan 11.1 Lavaan 11.2 Plotting SEM models with the semPlot package 11.3 Path models", " Chapter 11 Structural Equation modelling with lavaan There are several packages for R which allow you to estimate Structural Equation Models (SEM), including sem (Fox, Nie, and Byrnes 2022), OpenMx (Boker et al. 2022), and lavaan (Rosseel, Jorgensen, and Rockwood 2022). Here, we will focus solely on lavaan. 11.1 Lavaan The name “lavaan” stands for “latent variable analysis”. It is a package which can estimate a wide-variety of SEM models, including path models without latent variables. It has a convenient and intuitive syntax to define SEM models and it is actively developed. That is why it has perhaps become the go-to R package for SEM analysis. Although the help files in the R package are not overly comprehensive, the lavaan website has a useful tutorial, and I suggest you check this out. 11.1.1 The lavaan model syntax In lavaan, SEM models can be specified via a formulation similar to the usual syntax for models in lm() and glm(). However, there are some new relational symbols used to define (residual) covariances, and latent variables. Formula Description v =~ y latent variable v is defined and measured by y v =~ y1 + y2 + y3 latent variable v is defined and measured by y1, y2, and y3 v =~ 1 + y1 + y2 + y3 latent variable v is defined and measured by y1, y2, and y3, with the inclusion of an explicit intercept 1 to model the mean of v y ~ x y is regressed on x, i.e. a causal relation from x to y y ~ b*x y is regressed on x, with the slope of x labelled as b y ~ x1 + x2 + x3 y is regressed on x1, x2 and x3, i.e. causal relations from x1, x2, and x3 to y y ~ a*x1 + b*x2 + c*x3 regression with labelled slopes abc := a*b*c define a derived term abc as a function of the model parameters y1 ~~ y2 (residual) co-variance between y1 and y2 y1 ~~ 0*y2 fix the (residual) co-variance between y1 and y2 to 0 y ~~ y (residual) variance for y y ~~ 0*y fix the (residual) variance for y to 0 11.1.2 Model estimation Under the hood, there is really only a single function used to define and estimate SEM models in the lavaan package, namely the lavaan::lavaan() function. But the lavaan package offers several wrappers around this function to make estimation of common SEM models more convenient. Amongst these are the lavaan::sem() function, and the lavaan::cfa() function. 11.1.3 Extracting results Once a model is estimated, you can use the summary() function on the object returned to get the basic parameter estimates and tests. This function provides the most important results. You can additionally get some of the more widely used fit indices by setting the argument \"fit.measures = TRUE\". Other functions which can be called on a model fitted by lavaan are: fitMeasures(): this function returns a long list of model fit measures, only some of which (the more important ones) were discussed in the SDAM book fitted(): this function returns the model-implied variance-covariance matrix and mean vector. anova(): When provided with a single model, this function returns the results of a likelihood-ratio test of the model against the saturated model. When supplied with multiple models, the function returns likelihood-ratio tests comparing these. This assumes the models are nested! 11.2 Plotting SEM models with the semPlot package The semPlot package (R-semPlot?) package provides a convenient way to plot SEM models fitted by lavaan. In principle, all that is needed to plot a lavaan-estimated object mod is a call to semPlot::semPaths(mod). However, the default settings don’t necessarily provide the best looking plots. But the semPlot package is very flexible, and by setting several arguments, it is possible to produce agreeable plots, although it will often require multiple attempts. Important arguments to the semPlot::semPaths() function are object: for our purposes, this is the object returned by lavaan what: this allows you to change the appearance of the arrows based on the parameter estimates. The default (\"path\") just displays the arrows, but by setting this argument to \"est\" the linewidth of the arrows reflects the magnitude of the links. For more options, see ?semPlot::semPath. whatLabels: This argument sets labels for the arrows. Some of the options are \"name\" (display the name of the link) and \"est\" (display the parameter estimates). For more options, see ?semPlot::semPath. 11.3 Path models Path models can be defined and estimated with the lavaan::sem() function. Important arguments for this function are: model: A string providing the model description data: the data.frame in which all variables in the model description can be found meanstructure (passed onto the lavaan function): a logical value (TRUE or FALSE) indicating whether the means of the variables should be estimated via intercept terms conditional.x (passed onto the lavaan function): a logical value (TRUE or FALSE) indicating whether the model should just consider the conditional distribution of the endogenous variables, conditional upon the exogenous variables. fixed.x (passed onto the lavaan function): a logical value (TRUE or FALSE) indicating whether the exogenous variables should be considered as fixed (TRUE by default). estimator (passed onto the lavaan function): a string which can be \"ML\" for maximum likelihood, \"GLS\" for (normal theory) generalized least squares, \"WLS\" for weighted least squares (sometimes called ADF estimation), or other values. See ?lavOptions for other possibilities. Usually, it is fine to stick to the defaults, and just specify the model and data arguments. 11.3.1 Regression models A simple regression model can be estimated as follows: # load the data data(&quot;trump2016&quot;, package=&quot;sdamr&quot;) # exclude the outlying Columbia state dat &lt;- subset(trump2016,state != &quot;District of Columbia&quot;) # specify the model in lavaan syntax mod_spec &lt;- &#39;percent_Trump_votes ~ 1 + hate_groups_per_million&#39; # estimate the model library(&quot;lavaan&quot;) ## This is lavaan 0.6-12 ## lavaan is FREE software! Please report any bugs. fmod &lt;- lavaan::sem(mod_spec, data=dat) We can then see the estimates and tests via the summary() function: summary(fmod) ## lavaan 0.6-12 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 50 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## percent_Trump_votes ~ ## ht_grps_pr_mll 2.300 0.658 3.496 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .prcnt_Trmp_vts 42.897 2.361 18.171 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .prcnt_Trmp_vts 80.228 16.046 5.000 0.000 The output of the summary() function first provides information about the way the model was estimated (by maximum likelihood or \"ML\" by default). Next, under \"Model Test User Model\" you will find the results of a likelihood-ratio test comparing the model to a saturated model. This test was referred to as the “overall model fit” test in the SDAM book. After this come the parameter estimates, standard errors, and Wald tests. These are displayed in the following order: causal regression effects (\"Regressions\"), intercepts (\"Intercepts\"), and (residual) variances (“Variances\"). If you want to plot the model, you can use the semPaths() function from the semPlot package: library(semPlot) semPlot::semPaths(fmod) The default plot is quite basic and does not look so nice. In the following code, I set the arguments layout = tree2 and rotation = 2 to make the plot go from left to right, disable the automatic shortening of variable names by setting nCharNodes = 0, increase the size of the observed (or “manifest”) variables by setting sizeMan=7 and a relatively smaller size for the constant intercepts by setting sizeInt = 4. To display the estimated values as part of the arrows, I also set whatLabels = \"est\". There are many other tweaks possible, and it pays to play around with the semPaths() function to get the result you want. You should look at the ?semPaths help file to see all the options available. Note that it is useful to rename your variables before calling lavaan::sem() to get better names of the variables in semPlot(), as I will do in the next example. semPlot::semPaths(fmod, layout=&quot;tree2&quot;, sizeMan=7, sizeInt = 4, normalize=FALSE, whatLabels=&quot;est&quot;, width=4, height=1, rotation=2, nCharNodes = 0) The multiple regression model discussed in the SDAM book is defined in lavaan syntax as like ~ 1 + attr + sinc + intel + fun + amb This specifies that like is predicted by (observed) variables attr, sinc, intel, fun, and amb. We also include and intercept via the 1 term in the formula. The model can be specified and estimated as follows: # load the data data(&quot;speeddate&quot;, package=&quot;sdamr&quot;) dat &lt;- speeddate # the following lines are just to get shorter names for the variables # this is usefull for semPlot. There are better ways to do this though. dat$like &lt;- dat$other_like dat$attr &lt;- dat$other_attr dat$sinc &lt;- dat$other_sinc dat$intel &lt;- dat$other_intel dat$fun &lt;- dat$other_fun dat$amb &lt;- dat$other_amb # the lavaan model specification mod_spec &lt;- &#39;like ~ 1 + attr + sinc + intel + fun + amb&#39; # estimate the model fmod &lt;- lavaan::sem(mod_spec, data=dat) The results can again be obtained with the summary() function. Here we will supply the additional \"fit.measures = TRUE\" argument to get additional measures of model fit: # get the results summary(fmod, fit.measures=TRUE) ## lavaan 0.6-12 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Used Total ## Number of observations 1389 1562 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 1367.918 ## Degrees of freedom 5 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2138.891 ## Loglikelihood unrestricted model (H1) -2138.891 ## ## Akaike (AIC) 4291.783 ## Bayesian (BIC) 4328.437 ## Sample-size adjusted Bayesian (BIC) 4306.201 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## like ~ ## attr 0.338 0.020 17.007 0.000 ## sinc 0.134 0.028 4.826 0.000 ## intel 0.146 0.032 4.617 0.000 ## fun 0.353 0.023 15.445 0.000 ## amb 0.063 0.024 2.621 0.009 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .like -0.753 0.170 -4.428 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .like 1.274 0.048 26.353 0.000 Compared to the usual output, we now also get a likelihood-ratio test against a baseline model (\"Model Test Baseline Model\"), two fit indices (the CFI and TLI, the latter of which was discussed in the SDAM book), then the AIC and BIC measures, and then two other measures of model fit (the RMSEA and SRMR). As before, we can get a graphical depiction of the model by using the semPaths() function from the semPlot package. In the code below, I’m additionally setting the argument curvature=3 to make the arrows for the covariances between the exogenous predictors more spread out and easier to see: semPlot::semPaths(fmod, layout=&quot;tree&quot;, sizeMan=7, sizeInt = 4, style=&quot;ram&quot;, residuals=TRUE, rotation=1, intAtSide = FALSE, whatLabels = &quot;est&quot;, nCharNodes = 0, curvature=3) 11.3.2 Mediation models The mediation models concerned the legacy2015 data in the sdamr package, which we should load first: data(&quot;legacy2015&quot;, package=&quot;sdamr&quot;) dat &lt;- legacy2015 The Full Mediation model for the relation between legacy motive, intention, and donation, is specified through separate formulas for intention and donation, which are on separate lines of the string that will be supplied to lavaan. We can also explicitly label the regression parameters, which will allow us to compute the indirect effect of legacy on donation mod1 &lt;- &#39; intention ~ 1 + a*legacy donation ~ 1 + b*intention ab := a*b # indirect effect of legacy on donation &#39; Here, we have specified that intention is caused by legacy, and donation by intention. We have also included an intercept for both endogenous variables. The model can be estimated as usual: fmod1 &lt;- lavaan::sem(mod1, data=dat) The results are: summary(fmod1, fit.measures=TRUE) ## lavaan 0.6-12 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 237 ## ## Model Test User Model: ## ## Test statistic 5.989 ## Degrees of freedom 1 ## P-value (Chi-square) 0.014 ## ## Model Test Baseline Model: ## ## Test statistic 51.556 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.897 ## Tucker-Lewis Index (TLI) 0.692 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -883.794 ## Loglikelihood unrestricted model (H1) -880.800 ## ## Akaike (AIC) 1779.588 ## Bayesian (BIC) 1800.397 ## Sample-size adjusted Bayesian (BIC) 1781.379 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.145 ## 90 Percent confidence interval - lower 0.052 ## 90 Percent confidence interval - upper 0.266 ## P-value RMSEA &lt;= 0.05 0.047 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.048 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## legacy (a) 0.267 0.058 4.580 0.000 ## donation ~ ## intention (b) 1.065 0.205 5.185 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 1.785 0.245 7.284 0.000 ## .donation -0.391 0.619 -0.632 0.528 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.739 0.068 10.886 0.000 ## .donation 8.047 0.739 10.886 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.285 0.083 3.433 0.001 As we have defined a new parameter for the indirect effect of legacy on donation, the results also show the estimate of this new parameter under \"Defined Parameters\", named ab. We even get a Wald test for the null-hypothesis that the indirect effect is equal to 0 (i.e. no mediation), which is rejected. A longer list of fit indices can also be obtained via fitMeasures(fmod1) ## npar fmin chisq df ## 6.000 0.013 5.989 1.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.014 51.556 3.000 0.000 ## cfi tli nnfi rfi ## 0.897 0.692 0.692 0.652 ## nfi pnfi ifi rni ## 0.884 0.295 0.901 0.897 ## logl unrestricted.logl aic bic ## -883.794 -880.800 1779.588 1800.397 ## ntotal bic2 rmsea rmsea.ci.lower ## 237.000 1781.379 0.145 0.052 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.266 0.047 0.137 0.168 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.048 0.048 0.059 0.059 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.083 0.048 0.059 153.018 ## cn_01 gfi agfi pgfi ## 263.563 0.998 0.982 0.111 ## mfi ecvi ## 0.990 0.076 although this is not as nicely presented as in summary(..., fit.measures=TRUE). A graphical depiction of the model is obtained as: semPlot::semPaths(fmod1, layout=&quot;tree&quot;, sizeMan=7, sizeInt = 4, style=&quot;ram&quot;, residuals=TRUE, rotation=2, intAtSide = FALSE, whatLabels = &quot;est&quot;, nCharNodes = 0, normalize = FALSE) Note I am setting the argument \"intAtSide = FALSE\" to create a better looking plot. You can compare the result with setting \"intAtSide = TRUE\" to see the effect of this setting. The Partial Mediation model is specified again by separate formulas for intention and donation, but now we include two predictors for donation: mod2 &lt;- &#39; intention ~ 1 + a*legacy donation ~ 1 + b*intention + c*legacy ab := a*b &#39; fmod2 &lt;- lavaan::sem(mod2, data=dat) summary(fmod2, fit.measures=TRUE) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 237 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 51.556 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -880.800 ## Loglikelihood unrestricted model (H1) -880.800 ## ## Akaike (AIC) 1775.599 ## Bayesian (BIC) 1799.876 ## Sample-size adjusted Bayesian (BIC) 1777.688 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## legacy (a) 0.267 0.058 4.580 0.000 ## donation ~ ## intention (b) 0.917 0.212 4.331 0.000 ## legacy (c) 0.488 0.198 2.463 0.014 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 1.785 0.245 7.284 0.000 ## .donation -1.961 0.884 -2.220 0.026 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.739 0.068 10.886 0.000 ## .donation 7.846 0.721 10.886 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.245 0.078 3.147 0.002 semPlot::semPaths(fmod2, layout=&quot;tree&quot;, sizeMan=7, sizeInt = 5, style=&quot;ram&quot;, residuals=TRUE, rotation=2, intAtSide = FALSE, whatLabels = &quot;est&quot;, nCharNodes = 0, normalize=FALSE) Finally, the Common Cause model can be specified by three formulas. The first two lines concern the regression equations. The third line (donation ~~ 0*intention) specifies that the (residual) covariance between donation and intention should be fixed to 0. mod3 &lt;- &#39; intention ~ 1 + legacy donation ~ 1 + legacy donation ~~ 0*intention &#39; fmod3 &lt;- lavaan::sem(mod3, data=dat) summary(fmod3, fit.measures=TRUE) ## lavaan 0.6-12 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 237 ## ## Model Test User Model: ## ## Test statistic 18.048 ## Degrees of freedom 1 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 51.556 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.649 ## Tucker-Lewis Index (TLI) -0.053 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -889.824 ## Loglikelihood unrestricted model (H1) -880.800 ## ## Akaike (AIC) 1791.648 ## Bayesian (BIC) 1812.456 ## Sample-size adjusted Bayesian (BIC) 1793.438 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.268 ## 90 Percent confidence interval - lower 0.169 ## 90 Percent confidence interval - upper 0.383 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.084 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## legacy 0.267 0.058 4.580 0.000 ## donation ~ ## legacy 0.733 0.197 3.714 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention ~~ ## .donation 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 1.785 0.245 7.284 0.000 ## .donation -0.325 0.830 -0.392 0.695 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.739 0.068 10.886 0.000 ## .donation 8.467 0.778 10.886 0.000 semPlot::semPaths(fmod3, layout=&quot;tree&quot;, sizeMan=7, sizeInt = 5, style=&quot;ram&quot;, residuals=TRUE, rotation=2, intAtSide = FALSE, whatLabels = &quot;est&quot;, nCharNodes = 0, normalize=FALSE) We can obtain likelihood-ratio tests for nested models with the anova() function. For sample, we can compare the Full Mediation model to the Partial Mediation model with: anova(fmod1, fmod2) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fmod2 0 1775.6 1799.9 0.0000 ## fmod1 1 1779.6 1800.4 5.9889 5.9889 1 0.0144 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And we can compare the Common Cause model to the Partial Mediation model with: anova(fmod3, fmod2) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fmod2 0 1775.6 1799.9 0.000 ## fmod3 1 1791.7 1812.5 18.048 18.048 1 2.154e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As the Partial Mediation model is saturated, these tests are equivalent to the model fit tests reported by the summary() function for fmod1 and fmod3. 11.3.3 A more complex path model More complex path models can be estimated by including more formula’s in the model specification. For example: mod_complex &lt;- &#39; belief ~ 1 + education legacy ~ 1 + age income ~ 1 + age + education intention ~ 1 + belief + legacy + income donation ~ 1 + intention + income &#39; fmod_complex &lt;- lavaan::sem(mod_complex, data=legacy2015) summary(fmod_complex) ## lavaan 0.6-12 ended normally after 43 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 19 ## ## Used Total ## Number of observations 224 237 ## ## Model Test User Model: ## ## Test statistic 41.291 ## Degrees of freedom 11 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## belief ~ ## education 0.132 0.048 2.723 0.006 ## legacy ~ ## age -0.006 0.005 -1.120 0.263 ## income ~ ## age 0.002 0.009 0.254 0.800 ## education 0.384 0.102 3.757 0.000 ## intention ~ ## belief 0.738 0.064 11.615 0.000 ## legacy 0.123 0.049 2.522 0.012 ## income -0.080 0.030 -2.710 0.007 ## donation ~ ## intention 1.098 0.213 5.152 0.000 ## income 0.257 0.121 2.113 0.035 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .belief 4.416 0.222 19.912 0.000 ## .legacy 4.322 0.208 20.773 0.000 ## .income 1.220 0.569 2.143 0.032 ## .intention -1.077 0.386 -2.790 0.005 ## .donation -1.232 0.774 -1.592 0.111 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .belief 0.520 0.049 10.583 0.000 ## .legacy 0.906 0.086 10.583 0.000 ## .income 2.318 0.219 10.583 0.000 ## .intention 0.485 0.046 10.583 0.000 ## .donation 8.039 0.760 10.583 0.000 To plot complex models like this, it may be useful to hide the constant (intercept) terms, which is achieved by setting \"intercepts\" = FALSE in the call to semPlot::semPaths(): semPlot::semPaths(fmod_complex, layout=&quot;tree2&quot;, sizeMan=7, sizeInt = 5, style=&quot;ram&quot;, intercepts=FALSE, residuals=TRUE, rotation=2, intAtSide = FALSE, whatLabels = &quot;est&quot;, nCharNodes = 0, normalize=FALSE) References "],["bayesian-estimation.html", "Chapter 12 Bayesian estimation", " Chapter 12 Bayesian estimation "],["bayesian-hypothesis-testing-with-bayes-factors.html", "Chapter 13 Bayesian hypothesis testing with Bayes Factors 13.1 The BayesFactor package", " Chapter 13 Bayesian hypothesis testing with Bayes Factors In this chapter, we will discuss how to compute Bayes Factors for a variety of General Linear Models using the BayesFactor package (Morey and Rouder 2022). The package implements the “default” priors discussed in the SDAM book. 13.1 The BayesFactor package The BayesFactor package implements Bayesian model comparisons for General Linear Models (as well as some other models for e.g. contingency tables and proportions) using JZS-priors for the parameters, or fixing those parameters to 0. Because Bayes Factors are transitive, in the sense that a ratio of Bayes Factors is itself another Bayes factor: \\[\\begin{align} \\text{BF}_{1,2} &amp;= \\frac{p(Y_1,\\ldots,Y_n|\\text{MODEL 1})}{p(Y_1,\\ldots,Y_n|\\text{MODEL 2})} \\\\ &amp;= \\frac{p(Y_1,\\ldots,Y_n|\\text{MODEL 1})/p(Y_1,\\ldots,Y_n|\\text{MODEL 0})} {p(Y_1,\\ldots,Y_n|\\text{MODEL 2})/p(Y_1,\\ldots,Y_n|\\text{MODEL 0})} \\\\ &amp;= \\frac{\\text{BF}_{1,0}}{\\text{BF}_{2,0}} , \\end{align}\\] you can compute many other Bayes Factors which might not be immediately provided by the package, by simply dividing the Bayes factors that the package does provide. This makes the procedure of model comparison very flexible. If you haven’t installed the BayesFactor package yet, you need to do so first. Then you can load it as usual by: library(BayesFactor) 13.1.1 A Bayesian one-sample t-test A Bayesian alternative to a \\(t\\)-test is provided via the ttestBF function. Similar to the base R t.test function of the stats package, this function allows computation of a Bayes factor for a one-sample t-test or a two-sample t-tests (as well as a paired t-test, which we haven’t covered in the course). Let’s re-analyse the data we considered before, concerning participants’ judgements of the height of Mount Everest. The one-sample t-test we computed before, comparing the judgements to an assumed mean of \\(\\mu = 8848\\), was: # load the data library(sdamr) data(&quot;anchoring&quot;) # select the subset we analysed in Chapter 3 dat &lt;- subset(anchoring,(referrer == &quot;swps&quot; | referrer == &quot;swpson&quot;) &amp; anchor == &quot;low&quot;) # compute the Frequentist one-sample t-test t.test(dat$everest_meters, mu=8848) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 1.558e-13 ## alternative hypothesis: true mean is not equal to 8848 ## 95 percent confidence interval: ## 5716.848 6907.537 ## sample estimates: ## mean of x ## 6312.193 The syntax for the Bayesian alternative is very similar, namely: bf_anchor &lt;- ttestBF(dat$everest_meters, mu=8848) This code provides a test of the following models: \\[\\begin{align} H_0\\!&amp;: \\mu = 8848 \\\\ H_1\\!&amp;: \\frac{\\mu - 8848}{\\sigma_\\epsilon} \\sim \\textbf{Cauchy}(r) \\end{align}\\] After computing the Bayes factor and storing it in an object bf_anchor, we just see the print-out of the result by typing in the name of the object: bf_anchor ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 46902934288 ±0% ## ## Against denominator: ## Null, mu = 8848 ## --- ## Bayes factor type: BFoneSample, JZS This output is quite sparse, which is by no means a bad thing. It shows a few important things. Under Alt. (which stands for the alternative hypothesis), we first see the scaling factor \\(r\\) used for the JZS prior distribution on the effect size. We then see the value of the Bayes Factor, which is “extreme” (&gt;100), showing that the data increases the posterior odds ratio for the alternative model over the null model by a factor of 46,902,934,288. Quite clearly, the average judgements differed from the true height of Mount Everest! After the computed value of the Bayes factor, you will find a proportional error for the estimate of the Bayes factor. In general, the marginal likelihoods that constitute the numerator (“top model”) and denominator (“bottom model”) of the Bayes factor cannot be computed exactly, and have to be approximated by numerical integration routines or simulation. This results in some (hopefully small) error in computation, and the error estimate indicates the extend to which the true Bayes factor might differ from the computed one. In this case, the error is (proportionally) very small, and hence we can be assured that our conclusion is unlikely to be affected by error in the approximation. As we didn’t set the scaling factor explicitly, the default value is used, which is the “medium” scale \\(r = \\frac{\\sqrt{2}}{2} = 0.707\\). Note that this is actually different from the default value of \\(r=1\\) proposed in Rouder et al. (2009), which first introduced this version of the Bayesian \\(t\\)-test to a psychological audience, and the one used to illustrate the method in the SDAM book. Whilst reducing the default value to \\(r=0.707\\) is probably reasonable given the effect sizes generally encountered in psychological studies, a change in the default prior highlights the subjective nature of the prior distribution in the Bayesian model comparison procedure. You should also realise that different analyses, such as t-tests, ANOVA, and regression models, use different default values for the scaling factor. As shown in the SDAM book, the value of the Bayes factor depends on the choice for the scaling factor. Although the default value may be deemed reasonable, the choice should really be based on a consideration of the magnitude of the effect sizes you (yes, you!) expect in a particular study. This is not always easy, but you should pick one (the default value, for instance, if you can’t think of a better one) before conducting the analysis. If you feel that makes the test too subjective, you may may want to check the robustness of the result for different choices of the scaling factor. You can do this by computing the Bayes factor for a range of choices of the scaling factor, and then inspecting whether the strength of the evidence is in line with your choice for a reasonable range of values around your choice. The code below provides an example of this: # create a vector with different values for the scaling factor r rscale &lt;- seq(.001, 3, length=100) # create an empty vector to store the resulting Bayes factors BFs &lt;- vector(&quot;double&quot;, length=100) # compute the Bayes factor for each value of r for(i in 1:100) { temp_bf &lt;- ttestBF(dat$everest_meters, mu=8848, r = rscale[i]) # the object returned by the ttestBF function is a so-called S4 object # this has &quot;slots&quot; which can be accessed through the &quot;@&quot; operator. # The actual value of the BF is extracted by @bayesFactor$bf # which is stored in a logarithmic scale, so we need to use &quot;exp&quot; # to obtain the value of the actual Bayes factor: BFs[i] &lt;- exp(temp_bf@bayesFactor$bf) } # use ggplot to plot the values of the Bayes factor against the choice of r library(ggplot2) ggplot(data.frame(r=rscale,BF=BFs),aes(x=r,y=BF)) + geom_line() Given the scale of the \\(y\\)-axis (e.g., the first tick mark is at 1e+10 = 10,000,000,000), there is overwhelming evidence against the null-hypothesis for most choices of the scaling factor. Hence, the results seem rather robust to the exact choice of prior. 13.1.2 A Bayesian two-sample t-test To compare the means of two groups, we can revisit the Tetris study, where we considered whether the number of memory intrusions is reduced after playing Tetris in combination with memory reactivation, compared to just memory reactivation by itself. The ttestBF function allows us to provide the data for one group as the x argument, and the data for the other group as the y argument, so we can perform our model comparison, by subsetting the dependent variable appropriately, as follows: data(&quot;tetris2015&quot;, message=FALSE) ## Warning in data(&quot;tetris2015&quot;, message = FALSE): data set &#39;FALSE&#39; not found bf_tetr &lt;- ttestBF(x=subset(tetris2015, Condition == &quot;Reactivation&quot;)$Days_One_to_Seven_Number_of_Intrusions, y = subset(tetris2015, Condition == &quot;Tetris_Reactivation&quot;)$Days_One_to_Seven_Number_of_Intrusions) bf_tetr ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 16.78482 ±0% ## ## Against denominator: ## Null, mu1-mu2 = 0 ## --- ## Bayes factor type: BFindepSample, JZS Which shows strong evidence for the alternative hypothesis over the null hypothesis that the means are identical (i.e. that the difference between the means is zero, \\(\\mu_1 - \\mu_2 = 0\\)), as the alternative model is 2.82 times more likely than the null model, which sets the difference between the means to exactly \\(\\mu_1 - \\mu_2 = 0\\), rather than allowing different values of this difference through the prior distribution. A two-sample t-test should really be identical to a two-group ANOVA model, as both concern the same General Linear Model (a model with a single contrast-coding predictor, with e.q. values of \\(-\\tfrac{1}{2}\\) and \\(\\tfrac{1}{2}\\)). Before fully discussing the way to perform an ANOVA-type analysis with the BayesFactor package, let’s just double-check this is indeed the case: dat &lt;- subset(tetris2015, Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)) # remove levels of Condition that are no longer needed due to subsetting dat$Condition &lt;- droplevels(dat$Condition) # use the anovaBF function bf2_tetr &lt;- anovaBF(Days_One_to_Seven_Number_of_Intrusions ~ Condition, data = dat) bf2_tetr ## Bayes factor analysis ## -------------- ## [1] Condition : 16.78482 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The results are indeed identical. Note that this is because both the ttestBF and anovaBF function use the same prior distribution for the effect. 13.1.3 A Bayesian ANOVA More general ANOVA-type models can be tested though the anovaBF function. This function takes the following important arguments: formula: a formula like in the lm function, specifying which factors to include as well as their interactions (by e.g. using an * operator to specify you want to include the main effects as well as their interactions. Note that unlike in the lm function, all terms must be factors. data: a data.frame containing data for all factors in the formula. whichRandom: a character vector specifying which factors are random. Random factors can be used to obtain a model similar to a repeated-measures ANOVA, or a (restricted) set of linear-mixed effects models (with only factors for the fixed effects). whichModels: a character vector specifying which models to compare. The allowed values are \"all\", \"withmain\" (the default), \"top\", and \"bottom\". Setting whichModels to \"all\" will test all models that can be created by including or not including a main effect or interaction. \"top\" will test all models that can be created by removing or leaving in a main effect or interaction term from the full model. \"bottom\" creates models by adding single factors or interactions to the null model. \"withmain\" will test all models, with the constraint that if an interaction is included, the corresponding main effects are also included. Setting the argument to top produces model comparisons similar to the Type 3 procedure, comparing the full model to a restricted model with each effect removed. Setting the argument to withMain produces model comparisons similar to the Type 2 procedure, with model comparisons that respect the “principle of marginality”, such that tests of the main effects do not consider higher-order interactions, whilst a test of any interaction includes the main effects that constitute the elements in the interactions. rscaleFixed: prior scale for fixed effects. The value can be given numerically, or as one of three strings: \"medium\" (\\(r = \\tfrac{1}{2}\\)), \"wide\": \\(r = \\tfrac{\\sqrt{2}}{2}\\), or \"ultrawide\" (\\(r=1\\)). The default is “medium\". rscaleRandom: prior scale for random effects. Accepts the same strings as rscaleFixed, and in addition \"nuisance\" (\\(r = 1\\)). The default is \"nuisance\". rscaleEffects: a named vector with prior scales for individual factors. The anovaBF function will (as far as I can gather) always use contr.sum() contrasts for the factors. So setting your own contrasts will have no effect on the results. The exact contrast should not really matter for omnibus tests, and sum-to-zero are a reasonable choice in general (contr.sum implements what we called effect-coding before).3 While the anovaBF function always uses the JZS prior for any effects, it allows you to specify exactly which scaling factor to use for every effect, if so desired. One perhaps confusing thing is that effect-sizes for ANOVA designs (as far as I can gather) are based on standardized treatment-effects, whilst those for the t-test designs are based on Cohens-\\(d\\) effect sizes. Hence, the values of the scaling factor \\(r\\) for “medium”, “wide”, and “ultrawide” are different for the Bayesian \\(t\\)-test and ANOVA models (whilst they provide the same results for models with two conditions). Let’s see what happens when we use a Bayesian ANOVA-type analysis for the data on experimenter beliefs in social priming. First, let’s load the data, and turn the variables reflecting the experimental manipulations into factors: data(&quot;expBelief&quot;) dat &lt;- expBelief dat$primeCond &lt;- factor(dat$primeCond) dat$experimenterBelief &lt;- factor(dat$experimenterBelief) We can now use the anovaBF function to compute the Bayes factors: # this uses a sampling based approach # set the seed to make it reproducible set.seed(202111208) bf_expB &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat) bf_expB ## Bayes factor analysis ## -------------- ## [1] experimenterBelief : 537.3879 ±0% ## [2] primeCond : 0.1282136 ±0.12% ## [3] experimenterBelief + primeCond : 69.86988 ±0.98% ## [4] experimenterBelief + primeCond + experimenterBelief:primeCond : 15.62005 ±5.59% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS A main thing to note here is that the comparisons of different versions of MODEL G are against the same MODEL R, which is an intercept-only model. We can see that all models which include experimenterBelief receive strong evidence against the intercept-only model, apart from the model which only includes primeCond, which has less evidence than the intercept-only model. Although this indicates that the primeCond effect might be ignorable, the comparisons are different from comparing reduced models to the general MODEL G with all effects included. We can obtain these Type 3 comparisons by setting the whichModels argument to `top``: set.seed(202111208) bf_expB2 &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat, whichModels = &quot;top&quot;) bf_expB2 ## Bayes factor top-down analysis ## -------------- ## When effect is omitted from experimenterBelief + primeCond + experimenterBelief:primeCond , BF is... ## [1] Omit experimenterBelief:primeCond : 4.703731 ±1.7% ## [2] Omit primeCond : 7.674953 ±2.19% ## [3] Omit experimenterBelief : 0.00188326 ±1.79% ## ## Against denominator: ## ApproachAdvantage ~ experimenterBelief + primeCond + experimenterBelief:primeCond ## --- ## Bayes factor type: BFlinearModel, JZS It is very important to realise that the output now concerns the comparison of the reduced model (in the numerator, i.e. the “top model”) against the full model (in the denominator, i.e. the “bottom model”), as is stated in the Against denimonator part of the output. So these are \\(\\text{BF}_{0,1}\\) values, rather than \\(\\text{BF}_{1,0}\\) values. That means that low values of the Bayes factor now indicate evidence for the alternative hypothesis that an effect is different from 0. As we find a very low \\(\\text{BF}_{0,1}\\) value for the experimenterBelief effect, this thus shows strong evidence that this effect is different than 0. The \\(\\text{BF}_{0,1}\\) values for the other effects are larger than 1, which indicate more support for the null hypothesis than for the alternative hypothesis. We can change the output from a \\(\\text{BF}_{0,1}\\) value to a \\(\\text{BF}_{1,0}\\) value by simply inverting the Bayes factors, as follows: 1/bf_expB2 ## denominator ## numerator experimenterBelief + primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 0.2125972 ## denominator ## numerator experimenterBelief + experimenterBelief:primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 0.130294 ## denominator ## numerator primeCond + experimenterBelief:primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 530.994 As we noted before, we again see strong evidence for the effect of experimenterBelief when we remove it from the full model, but not for the other effects. The transitivity of the Bayes factor means that we can also obtain some of these results through a ratio of the Bayes factors obtained earlier. For instance, a Type 3 test of the effect of experimenterBelief:primeCond interaction can be obtained by comparing a model with all effects included to a model without this interaction. In the analysis stored in bf_expB, we compared a number of the possible models to an intercept-only model. By comparing the Bayes factors of the model which excludes the interaction to a model which includes it, we can obtain the same Bayes factor of that interaction as follows. In the output of bf_expB, the fourth element compared the full model to the intercept-only model, whilst in the third element, a model with only the main effects of experimenterBelief and primeCond are compared to an intercept-only model. The Type 3 test of the interaction can then be obtained through the ratio of these two Bayes factors: bf_expB[4]/bf_expB[3] ## Bayes factor analysis ## -------------- ## [1] experimenterBelief + primeCond + experimenterBelief:primeCond : 0.2235591 ±5.68% ## ## Against denominator: ## ApproachAdvantage ~ experimenterBelief + primeCond ## --- ## Bayes factor type: BFlinearModel, JZS which indicates evidence for the null hypothesis that there is no moderation of the effect of experimenterbelief by primeCond, as the Bayes factor is well below 1. We cannot replicate all Type 3 analyses with the results obtained earlier, unless we ask the function to compare every possible model against the intercept-only model, by specifying whichModels = \"all\": set.seed(202111208) bf_expB3 &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat, whichModels = &quot;all&quot;) bf_expB3 ## Bayes factor analysis ## -------------- ## [1] experimenterBelief : 537.3879 ±0% ## [2] primeCond : 0.1282136 ±0.12% ## [3] experimenterBelief:primeCond : 0.1613884 ±0.1% ## [4] experimenterBelief + primeCond : 69.86988 ±0.98% ## [5] experimenterBelief + experimenterBelief:primeCond : 114.0048 ±1.69% ## [6] primeCond + experimenterBelief:primeCond : 0.02797422 ±1.13% ## [7] experimenterBelief + primeCond + experimenterBelief:primeCond : 14.85414 ±1.38% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS For instance, we can now obtain a Type 3 test for experimenterBelief by comparing the full model (the 7th element in the output) to a model which just excludes this effect (i.e. the 6th element): bf_expB3[7]/bf_expB3[6] ## Bayes factor analysis ## -------------- ## [1] experimenterBelief + primeCond + experimenterBelief:primeCond : 530.994 ±1.79% ## ## Against denominator: ## ApproachAdvantage ~ primeCond + experimenterBelief:primeCond ## --- ## Bayes factor type: BFlinearModel, JZS which reproduces mostly the result we obtained by setting whichModel = \"top\" before. 13.1.4 Bayesian regression and ANCOVA Apart from different default values of the scaling factor \\(r\\) in the scaled-Cauchy distribution, the BayesFactor package works in the same way for models which include metric predictors. In a multiple regression model with only metric predictors, we can use the convenience function regressionBF. If you want to mix metric and categorical predictors, as in an ANCOVA model, you will have to use the generalTestBF function. All functions discussed so far are really just convenience interfaces to the generalTestBF, which implements Bayes factors for the General Linear Model. These convenience functions are used to determine an appropriate scaling factor for the different terms in the model, but not much else of concern, so you can replicate all the previous analyses through the generalTestBFfunction, if you’d like. 13.1.5 Bayesian repeated-measures ANOVA An analysis similar to a repeated-measures ANOVA can also be obtained. Just like the afex package, the BayesFactor package requires data in the long format. Let’s first prepare the data of the Cheerleader-effect experiment: data(&quot;cheerleader&quot;) dat &lt;- cheerleader # remove participants which should be excluded dat &lt;- subset(dat, Excluded == 0) # get rid of unused factor levels in Item by dat$Item &lt;- factor(dat$Item) dat$Presentation &lt;- forcats::fct_recode(dat$Item, Different = &quot;Control_Group&quot;, Similar = &quot;Distractor_Manipulation&quot;) dat$Version &lt;- forcats::fct_recode(dat$Task, Identical = &quot;Identical-Distractors&quot;, Variant = &quot;Self-Distractors&quot;) The way the BayesFactor package deals with repeated-measures designs is a little different from how we treated repeated-measures ANOVA. Rather than computing within-subjects composite variables, the package effectively deals with individual differences by adding random intercepts (like in a linear mixed-effects model). To do this, we add Participant as an additive effect, and then classify it as a random effect through the whichRandom argument. To obtain Type-3 comparisons, we again set whichModels to top: set.seed(202111208) bf_rmanova &lt;- anovaBF(Response ~ Task*Item + Participant, whichRandom = &quot;Participant&quot;, data=dat, whichModels = &quot;top&quot;) bf_rmanova ## Bayes factor top-down analysis ## -------------- ## When effect is omitted from Task + Item + Task:Item + Participant , BF is... ## [1] Omit Item:Task : 1.340752 ±9.36% ## [2] Omit Item : 7.465094e-06 ±9.62% ## [3] Omit Task : 1.439629 ±7.75% ## ## Against denominator: ## Response ~ Task + Item + Task:Item + Participant ## --- ## Bayes factor type: BFlinearModel, JZS In this case, the proportional errors of the results may be deemed to high. We can get more precise results by obtaining more samples (for these complex models, the estimation of the Bayes factor is done with a sampling-based approximation). We can do this, without the need to respecify the model, with the recompute function, where we can increase the number of sampling iterations from the default (10,000 iterations) to something higher: set.seed(1234) bf_rmanova2 &lt;- recompute(bf_rmanova, iterations=100000) bf_rmanova2 ## Bayes factor top-down analysis ## -------------- ## When effect is omitted from Task + Item + Task:Item + Participant , BF is... ## [1] Omit Item:Task : 1.230971 ±2.77% ## [2] Omit Item : 7.194855e-06 ±2.85% ## [3] Omit Task : 1.364189 ±2.46% ## ## Against denominator: ## Response ~ Task + Item + Task:Item + Participant ## --- ## Bayes factor type: BFlinearModel, JZS This provides somewhat better results, although it would be better to increase the number of iterations even more. As before, the Bayes Factors are for the reduced model compared to the full model and we can get more easily interpretable results by computing the inverse value 1/bf_rmanova2 ## denominator ## numerator Task + Item + Participant ## Task + Item + Task:Item + Participant 0.812367 ## denominator ## numerator Task + Task:Item + Participant ## Task + Item + Task:Item + Participant 138988.2 ## denominator ## numerator Item + Task:Item + Participant ## Task + Item + Task:Item + Participant 0.7330362 We can see that we obtain “extreme” evidence for the main effect of Item. For the other effects, the evidence is more in favour of the null-hypothesis. 13.1.6 Parameter estimates By default, the Bayes Factor objects just provide the values of the Bayes Factor. We don’t get estimates of the parameters. To get (approximate) posterior distributions for the parameters, we can first estimate the general MODEL G with the lmBF function. This function is meant to compute a specific General Linear Model (rather than a set of such models). For example, for the Social Priming example, we can estimate the ANOVA model with lmBF as: dat &lt;- expBelief dat$primeCond &lt;- factor(dat$primeCond) dat$experimenterBelief &lt;- factor(dat$experimenterBelief) set.seed(202111208) bflm_expB &lt;- lmBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat) We can then use this estimated model to obtain samples from the posterior distribution over the model parameters. This is done with the posterior function of the Bayesfactor package. We can determine the number of samples through the iterations argument. This should generally be a high number, to get more reliable estimates: set.seed(1234) post_samples &lt;- posterior(bflm_expB, iterations=100000) The post_samples object can be effectively treated as a matrix, with columns corresponding to the different parameters, and in the rows the samples. So we can obtain posterior means as the column-wise averages: colMeans(post_samples) ## mu primeCond-HPP ## 21.440783 5.836911 ## primeCond-LPP experimenterBelief-H ## -5.836911 43.978650 ## experimenterBelief-L primeCond:experimenterBelief-HPP.&amp;.H ## -43.978650 8.725977 ## primeCond:experimenterBelief-HPP.&amp;.L primeCond:experimenterBelief-LPP.&amp;.H ## -8.725977 -8.725977 ## primeCond:experimenterBelief-LPP.&amp;.L sig2 ## 8.725977 45920.258584 ## g_primeCond g_experimenterBelief ## 1.928212 2.384292 ## g_primeCond:experimenterBelief ## 1.435865 Here, mu corresponds to the “grand mean” (i.e. the average of averages), which is is the intercept in a GLM with sum-to-zero contrasts. The next mean corresponds to the posterior mean of the treatment effect of the high-power prime condition (primeCond-HPP). I.e., this is the marginal mean of the high-power prime conditions, compared to the grand mean. The second effect is the posterior mean of the treatment effect of the low-power prime condition (primeCond-LPP). As there are only two power-prime conditions, this is exactly the negative value of the posterior mean of the high-power prime treatment effect (the grand mean is exactly halfway between these two treatment effects). We get similar treatment effects for the main effect of experimenter belief, and the interaction between power prime and experimenter belief. The posterior mean labelled sig2 is an estimate of the error variance. The columns after this are values related to the specification of the prior, which we will ignore for now. We can do more than compute means for these samples from the posterior distribution. For instance, we can plot the (approximate) posterior distributions as well. For example, we can plot the posterior distribution of the high-power prime treatment effect as: ggplot(data.frame(x=as.numeric(post_samples[,c(&quot;primeCond-HPP&quot;)])), aes(x=x)) + geom_density() and for the experimenter-belief-high treatment effect as ggplot(data.frame(x=as.numeric(post_samples[,c(&quot;experimenterBelief-H&quot;)])), aes(x=x)) + geom_density() The first plot indicates that the posterior probability is quite high at the value of 0. In the second plot, the mass of the posterior distribution is clearly quite far removed from 0. This is in line with earlier results from the Bayes Factors, which showed little evidence for the main effect of power prime, but strong evidence for the effect of experimenter belief. 13.1.7 Highest-density interval A convenient way to obtain highest-density intervals, is by using the hdi function from the HDInterval package (Meredith and Kruschke 2020). This function is defined for a variety of objects, including those returned by the BayesFactor::posterior() function. The function has, in addition to the object, one more argument called credMass, which specifies the width of the credible interval (credMass = .95 is the default). For example, the 95% HDI interval for the two parameters that we plotted above are obtained as follows: HDInterval::hdi(post_samples[,c(&quot;primeCond-HPP&quot;)], credMass=0.95) ## var1 ## lower -14.74079 ## upper 26.47933 ## attr(,&quot;credMass&quot;) ## [1] 0.95 # (don&#39;t have to specify credMass, as we want the default value) HDInterval::hdi(post_samples[,c(&quot;experimenterBelief-H&quot;)]) ## var1 ## lower 23.57900 ## upper 65.49569 ## attr(,&quot;credMass&quot;) ## [1] 0.95 The output shows the lower and upper bound for each HDI. We see that the 95% HDI for power prime effect includes 0, whilst the 95% HDI for the experimenter belief effect does not. Again, this corresponds to what we observed earlier, that there is strong evidence for the experimenter belief effect, but not for the power prime effect. References "],["reproducible-reports-with-rmarkdown.html", "Chapter 14 Reproducible reports with RMarkdown 14.1 YAML headers 14.2 Markdown syntax 14.3 R code chunks and inline R code 14.4 APA documents with papaja", " Chapter 14 Reproducible reports with RMarkdown One of the many things that makes R extremely useful as a data analysis platform is its ability to generate high-quality reproducible reports and documents via R Markdown. Markdown itself is a lightweight markup language, with a plain-text formatting syntax. Markdown documents can be “parsed” to produce documents in a variety of formats, including HTML, PDF, Open Document Type, Microsoft Word, you name it… R Markdown integrates Markdown with R, allowing you to use R to include figures, tables, and R output (as well as the corresponding code, if you wish) directly in the document. A big benefit of this is that can keep your analysis and write-up in one place, so you don’t have to copy-paste results from one place to another (which often results in errors and issues). Including all analysis in the same document allows for completely reproducible research. Anyone with the R Markdown file would be able to reproduce everything in a scientific paper. R Markdown is extremely flexible and useful. For instance, this book was completely written in R Markdown, with help from the additional bookfown package (https://bookdown.org/). This chapter provides a brief introduction to R Markdown, as well as the papaja package to produce output in the APA style. As an introduction, there are many things left unsaid, so you will quite likely have to consult other sources to get properly acquainted with all the possibilities of R Markdown. A good reference to R Markdown is the R Markdown cookbook. Another source showcasing the many possibilities of R Markdown is R Markdown: The Definitive Guide. This chapter includes parts of the latter book. To get started with R Markdown, you can create a template from within RStudio, by clicking on the menu File &gt; New File &gt; R Markdown. You can fill in the title of your document, your name, and choose the output format (HTML, PDF, or Word). A new R Markdown document will be generated. If you click on the knit button, just above the file, you will be prompted to save the file, and then the file will be parsed and the output document generated. 14.1 YAML headers Headers in R Markdown files contain some metadata about your document, which you can customize to your liking. They use a syntax called YAML. Below is a simple example that purely states the title, author name(s), date, and output format. --- title: &quot;My title&quot; author: &quot;Mr My full Name&quot; date: &quot;December 21, 2020&quot; output: html_document --- You can change the output option to pdf_document, or word_document. Some other arguments you may want to provide in the YAML header are for the bibliography and the style of the output. For example, the previous header can be expanded to --- title: &quot;My title&quot; author: &quot;Mr My full Name&quot; date: &quot;December 21, 2020&quot; output: html_document: toc: true toc_float: true number_sections: true theme: cerulian bibliography: references.bib biblio-style: apalike --- This would produce an HTML output with a floating table of contents (toc: true, toc_float: true), numbered sections, and the “cerulian” theme (see e.g. https://www.datadreaming.org/post/r-markdown-theme-gallery/ for some of the available themes). In addition, it tells the R Markdown compiler to look for BibTex references in the references.bib file, and use an APA style for references. There are many things that can be specified in the header file. To make this process easier, you can install the ymlthis package, which also provides an RStudio plugin with a graphical user interface for some common options (see https://ymlthis.r-lib.org/). Note that if you want to create PDF documents you additionally need a TeX distribution. If you have no use for TeX beyond rendering R Markdown documents, I recommend you use TinyTex. TinyTex can be installed from within R as follows. if(!&quot;tinytex&quot; %in% rownames(installed.packages())) install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() Other, more full-fledged LaTeX MikTeX for Windows, MacTeX for Mac, or TeX Live for Linux. 14.2 Markdown syntax The text in an R Markdown document is written with the Markdown syntax. Precisely speaking, it is Pandoc’s Markdown. There are many flavours of Markdown invented by different people, and Pandoc’s flavour is the most comprehensive one to our knowledge. You can find the full documentation of Pandoc’s Markdown at https://pandoc.org/MANUAL.html. We strongly recommend that you read this page at least once to know all the possibilities with Pandoc’s Markdown, even if you will not use all of them. 14.2.1 Inline formatting Inline text will be italic if surrounded by underscores or asterisks, e.g., _text_ or *text*. Bold text is produced using a pair of double asterisks (**text**). A pair of tildes (~) turn text to a subscript (e.g., H~3~PO~4~ renders H3PO4). A pair of carets (^) produce a superscript (e.g., Cu^2+^ renders Cu2+). To mark text as inline code, use a pair of backticks, e.g., `code`. To include \\(n\\) literal backticks, use at least \\(n+1\\) backticks outside, e.g., you can use four backticks to preserve three backtick inside: ```` ```code``` ````, which is rendered as ```code```. Hyperlinks are created using the syntax [text](link), e.g., [RStudio](https://www.rstudio.com). The syntax for images is similar: just add an exclamation mark, e.g., ![alt text or image title](path/to/image). Footnotes are put inside the square brackets after a caret ^[], e.g., ^[This is a footnote.]. There are multiple ways to insert citations, and we recommend that you use BibTeX databases, because they work better when the output format is LaTeX/PDF. Section 2.8 of Xie (2016) explains the details. The key idea is that when you have a BibTeX database (a plain-text file with the conventional filename extension .bib) that contains entries like: @Manual{R-base, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2017}, url = {https://www.R-project.org/}, } You may add a field named bibliography to the YAML metadata, and set its value to the path of the BibTeX file. Then in Markdown, you may use @R-base (which generates “R Core Team (2022)”) or [@R-base] (which generates “(R Core Team 2022)”) to reference the BibTeX entry. Pandoc will automatically generated a list of references in the end of the document. 14.2.2 Block-level elements Section headers can be written after a number of pound signs, e.g., # First-level header ## Second-level header ### Third-level header If you do not want a certain heading to be numbered, you can add {-} or {.unnumbered} after the heading, e.g., # Preface {-} Unordered list items start with *, -, or +, and you can nest one list within another list by indenting the sub-list, e.g., - one item - one item - one item - one more item - one more item - one more item The output is: one item one item one item one more item one more item one more item Ordered list items start with numbers (you can also nest lists within lists), e.g., 1. the first item 2. the second item 3. the third item - one unordered item - one unordered item The output does not look too much different with the Markdown source: the first item the second item the third item one unordered item one unordered item Blockquotes are written after &gt;, e.g., &gt; &quot;I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.&quot; &gt; &gt; --- Mark Twain The actual output (we customized the style for blockquotes in this book): “I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.” — Mark Twain Plain code blocks can be written after three or more backticks, and you can also indent the blocks by four spaces, e.g., ``` This text is displayed verbatim / preformatted ``` Or indent by four spaces: This text is displayed verbatim / preformatted In general, you’d better leave at least one empty line between adjacent but different elements, e.g., a header and a paragraph. This is to avoid ambiguity to the Markdown renderer. For example, does “#” indicate a header below? In R, the character # indicates a comment. And does “-” mean a bullet point below? The result of 5 - 3 is 2. Different flavours of Markdown may produce different results if there are no blank lines. 14.2.3 Math expressions You can write mathematical expressions using LaTeX syntax. LaTeX is a system for scientific typesetting widely used in academia. It has excellent support for mathematical notation. Below, you will find some examples. For more extensive information on mathematical notation, you can consult e.g. https://en.wikibooks.org/wiki/LaTeX/Mathematics Inline LaTeX equations can be written in a pair of dollar signs using the LaTeX syntax, e.g., $f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$ (actual output: \\(f(k)={n \\choose k}p^{k}(1-p)^{n-k}\\)); math expressions of the display style can be written in a pair of double dollar signs, e.g., $$f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$$, and the output looks like this: \\[f\\left(k\\right)=\\binom{n}{k}p^k\\left(1-p\\right)^{n-k}\\] You can also use math environments inside $ $ or $$ $$, e.g., $$\\begin{array}{ccc} x_{11} &amp; x_{12} &amp; x_{13}\\\\ x_{21} &amp; x_{22} &amp; x_{23} \\end{array}$$ \\[\\begin{array}{ccc} x_{11} &amp; x_{12} &amp; x_{13}\\\\ x_{21} &amp; x_{22} &amp; x_{23} \\end{array}\\] $$X = \\begin{bmatrix}1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3} \\end{bmatrix}$$ \\[X = \\begin{bmatrix}1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3} \\end{bmatrix}\\] $$\\Theta = \\begin{pmatrix}\\alpha &amp; \\beta\\\\ \\gamma &amp; \\delta \\end{pmatrix}$$ \\[\\Theta = \\begin{pmatrix}\\alpha &amp; \\beta\\\\ \\gamma &amp; \\delta \\end{pmatrix}\\] $$\\begin{vmatrix}a &amp; b\\\\ c &amp; d \\end{vmatrix}=ad-bc$$ \\[\\begin{vmatrix}a &amp; b\\\\ c &amp; d \\end{vmatrix}=ad-bc\\] 14.3 R code chunks and inline R code In R Studio, you can insert an R code chunk either using the RStudio toolbar (the Insert button) or the keyboard shortcut Ctrl + Alt + I (Cmd + Option + I on macOS). There are a lot of things you can do in a code chunk: you can produce text output, tables, or graphics. You have fine control over all these output via chunk options, which can be provided inside the curly braces (between ```{r and }). For example, you can choose hide text output via the chunk option results = 'hide', or set the figure height to 4 inches via fig.height = 4. Chunk options are separated by commas, e.g., ```{r, chunk-label, results=&#39;hide&#39;, fig.height=4} The value of a chunk option can be an arbitrary R expression, which makes chunk options extremely flexible. For example, the chunk option eval controls whether to evaluate (execute) a code chunk, and you may conditionally evaluate a chunk via a variable defined previously, e.g., ```{r} # execute code if the date is later than a specified day do_it = Sys.Date() &gt; &#39;2018-02-14&#39; ``` ```{r, eval=do_it} x = rnorm(100) ``` There are a large number of chunk options in knitr documented at https://yihui.name/knitr/options. We list a subset of them below: eval: Whether to evaluate a code chunk. echo: Whether to echo the source code in the output document (someone may not prefer reading your smart source code but only results). results: When set to 'hide', text output will be hidden; when set to 'asis', text output is written “as-is”, e.g., you can write out raw Markdown text from R code (like cat('**Markdown** is cool.\\n')). By default, text output will be wrapped in verbatim elements (typically plain code blocks). collapse: Whether to merge text output and source code into a single code block in the output. This is mostly cosmetic: collapse = TRUE makes the output more compact, since the R source code and its text output are displayed in a single output block. The default collapse = FALSE means R expressions and their text output are separated into different blocks. warning, message, and error: Whether to show warnings, messages, and errors in the output document. Note that if you set error = FALSE, rmarkdown::render() will halt on error in a code chunk, and the error will be displayed in the R console. Similarly, when warning = FALSE or message = FALSE, these messages will be shown in the R console. include: Whether to include anything from a code chunk in the output document. When include = FALSE, this whole code chunk is excluded in the output, but note that it will still be evaluated if eval = TRUE. When you are trying to set echo = FALSE, results = 'hide', warning = FALSE, and message = FALSE, chances are you simply mean a single option include = FALSE instead of suppressing different types of text output individually. cache: Whether to enable caching. If caching is enabled, the same code chunk will not be evaluated the next time the document is compiled (if the code chunk was not modified), which can save you time. However, I want to honestly remind you of the two hard problems in computer science (via Phil Karlton): naming things, and cache invalidation. Caching can be handy but also tricky sometimes. fig.width and fig.height: The (graphical device) size of R plots in inches. R plots in code chunks are first recorded via a graphical device in knitr, and then written out to files. You can also specify the two options together in a single chunk option fig.dim, e.g., fig.dim = c(6, 4) means fig.width = 6 and fig.height = 4. out.width and out.height: The output size of R plots in the output document. These options may scale images. You can use percentages, e.g., out.width = '80%' means 80% of the page width. fig.align: The alignment of plots. It can be 'left', 'center', or 'right'. dev: The graphical device to record R plots. Typically it is 'pdf' for LaTeX output, and 'png' for HTML output, but you can certainly use other devices, such as 'svg' or 'jpeg'. fig.cap: The figure caption. child: You can include a child document in the main document. This option takes a path to an external file. Chunk options in knitr can be surprisingly powerful. You are encouraged to read the knitr documentation to discover the possibilities. You may also read Xie, Allaire, and Grolemund (2018), which is freely available online at https://bookdown.org/yihui/rmarkdown/. There is an optional chunk option that does not take any value, which is the chunk label. It should be the first option in the chunk header. Chunk labels are mainly used in filenames of plots and cache. If the label of a chunk is missing, a default one of the form unnamed-chunk-i will be generated, where i is incremental. I strongly recommend that you only use alphanumeric characters (a-z, A-Z and 0-9) and dashes (-) in labels, because they are not special characters and will surely work for all output formats. Other characters, spaces and underscores in particular, may cause trouble in certain packages, such as bookdown. If a certain option needs to be frequently set to a value in multiple code chunks, you can consider setting it globally in the first code chunk of your document, e.g., ```{r, setup, include=FALSE} knitr::opts_chunk$set(fig.width = 8, collapse = TRUE) ``` Besides code chunks, you can also insert values of R objects inline in text. For example: ```{r} x = 5 # radius of a circle ``` For a circle with the radius `r x`, its area is `r pi * x^2`. 14.3.1 Figures By default, figures produced by R code will be placed immediately after the code chunk they were generated from. For example: ```{r} plot(cars, pch = 18) ``` You can provide a figure caption using fig.cap in the chunk options. If the document output format supports the option fig_caption: true (e.g., the output format rmarkdown::html_document), the R plots will be placed into figure environments. In the case of PDF output, such figures will be automatically numbered. If you also want to number figures in other formats (such as HTML), please see the bookdown package in https://bookdown.org/yihui/rmarkdown/books.html. PDF documents are generated through the LaTeX files generated from R Markdown. A highly surprising fact to LaTeX beginners is that figures float by default: even if you generate a plot in a code chunk on the first page, the whole figure environment may float to the next page. This is just how LaTeX works by default. It has a tendency to float figures to the top or bottom of pages. Although it can be annoying and distracting, we recommend that you refrain from playing the “Whac-A-Mole” game in the beginning of your writing, i.e., desparately trying to position figures “correctly” while they seem to be always dodging you. You may wish to fine-tune the positions once the content is complete using the fig.pos chunk option (e.g., fig.pos = 'h'). See https://www.overleaf.com/learn/latex/Positioning_images_and_tables for possible values of fig.pos and more general tips about this behavior in LaTeX. In short, this can be a difficult problem for PDF output. To place multiple figures side-by-side from the same code chunk, you can use the fig.show='hold' option along with the out.width option. Figure 14.1 shows an example with two plots, each with a width of 50%. par(mar = c(4, 4, .2, .1)) plot(cars, pch = 19) plot(pressure, pch = 17) Figure 14.1: Two plots side-by-side. If you want to include a graphic that is not generated from R code, you may use the knitr::include_graphics() function, which gives you more control over the attributes of the image than the Markdown syntax of ![alt text or image title](path/to/image) (e.g., you can specify the image width via out.width). ```{r, out.width=&#39;25%&#39;, fig.align=&#39;center&#39;, fig.cap=&#39;...&#39;} knitr::include_graphics(&#39;images/cute-cat-picture.png&#39;) ``` 14.3.2 Tables The easiest way to include tables is by using knitr::kable(), which can create tables for HTML, PDF and Word outputs.4 Table captions can be included by passing caption to the function, e.g., ```{r tables-mtcars} knitr::kable(iris[1:5, ], caption = &#39;A caption&#39;) ``` Tables in non-LaTeX output formats will always be placed after the code block. For LaTeX/PDF output formats, tables have the same issue as figures: they may float. If you want to avoid this behavior, you will need to use the LaTeX package longtable, which can break tables across multiple pages. This can be achieved by adding \\usepackage{longtable} to your LaTeX preamble, and passing longtable = TRUE to kable(). If you are looking for more advanced control of the styling of tables, you are recommended to use the kableExtra package, which provides functions to customize the appearance of PDF and HTML tables. Formatting tables can be a very complicated task, especially when certain cells span more than one column or row. It is even more complicated when you have to consider different output formats. For example, it is difficult to make a complex table work for both PDF and HTML output. We know it is disappointing, but sometimes you may have to consider alternative ways of presenting data, such as using graphics. 14.4 APA documents with papaja To help you write documents according to the APA guidelines, you can use the papaja package. This section contains materials adapted from the papaja readme. 14.4.1 Installation To use papaja you need either an up-to-date version of RStudio or pandoc. papaja is not yet available on CRAN but you can install it from this repository: # Install devtools package if necessary if(!&quot;devtools&quot; %in% rownames(installed.packages())) install.packages(&quot;devtools&quot;) # Install the stable development verions from GitHub devtools::install_github(&quot;crsh/papaja&quot;) # Install the latest development snapshot from GitHub devtools::install_github(&quot;crsh/papaja@devel&quot;) 14.4.2 How to use papaja Once papaja is installed, you can select the APA template when creating a new Markdown file through the RStudio menus. APA template selection If you want to add citations specify your BibTeX-file in the YAML front matter of the document (bibliography: my.bib) and you can start citing. If necessary, have a look at R Markdown’s overview of the citation syntax. You may also be interested in citr, an R Studio addin to swiftly insert Markdown citations. 14.4.2.1 Helper functions to report analyses The functions apa_print() and apa_table() facilitate reporting results of your analyses. Take a look at the R Markdown-file of the example manuscript in the folder example and the resulting PDF. Drop a supported analysis result, such as an htest- or lm-object, into apa_print() and receive a list of possible character strings that you can use to report the results of your analysis. ## Loading required package: tinylabels my_lm &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Width + Petal.Length, data = iris) apa_lm &lt;- apa_print(my_lm) One element of this list is apa_lm$table that, in the case of an lm-object, will contain a complete regression table. Pass apa_lm$table to apa_table() to turn it into a proper table in your PDF or Word document. apa_table(apa_lm$table, caption = &quot;Iris regression table.&quot;) Table 14.1: Iris regression table. Predictor \\(b\\) 95% CI \\(t\\) \\(\\mathit{df}\\) \\(p\\) Intercept 1.04 [0.51, 1.58] 3.85 146 &lt; .001 Sepal Length 0.61 [0.48, 0.73] 9.77 146 &lt; .001 Petal Width 0.56 [0.32, 0.80] 4.55 146 &lt; .001 Petal Length -0.59 [-0.71, -0.46] -9.43 146 &lt; .001 papaja currently provides methods for the following object classes: A-B D-L L-S S-Z afex_aov default lsmobj* summary.aovlist anova emmGrid* manova summary.glht* anova.lme glht* merMod summary.glm Anova.mlm glm mixed summary.lm aov htest papaja_wsci summary.manova aovlist list summary_emm* summary.ref.grid* BFBayesFactor* lm summary.Anova.mlm BFBayesFactorTop* lme summary.aov * Not fully tested, don’t trust blindly! 14.4.2.2 Plot functions Be sure to also check out apa_barplot(), apa_lineplot(), and apa_beeplot() (or the general function apa_factorial_plot()) if you work with factorial designs: apa_factorial_plot( data = npk , id = &quot;block&quot; , dv = &quot;yield&quot; , factors = c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;) , ylim = c(0, 80) , level = .34 , las = 1 , ylab = &quot;Yield&quot; , plot = c(&quot;swarms&quot;, &quot;lines&quot;, &quot;error_bars&quot;, &quot;points&quot;) ) If you prefer creating your plots with ggplot2 try theme_apa(). 14.4.3 Getting help For an in-depth introduction to papaja, check out the current draft of the manual. 14.4.4 Other related R packages By now, there are a couple of R packages that provide convenience functions to facilitate the reporting of statistics in accordance with APA guidelines. apa: Format output of statistical tests in R according to APA guidelines APAstats: R functions for formatting results in APA style and other stuff apaTables: Create American Psychological Association (APA) Style Tables pubprint: This package takes the output of several statistical tests, collects the characteristic values and transforms it in a publish-friendly pattern schoRsch: Tools for Analyzing Factorial Experiments sigr: Concise formatting of significances in R Obviously, not all journals require manuscripts and articles to be prepared according to APA guidelines. If you are looking for other journal article templates, the following list of rmarkdown/pandoc packages and templates may be helpful. rticles: LaTeX Journal Article Templates for R Markdown chi-proc-rmd-template: ACM CHI Proceedings R Markdown Template Michael Sachs’ pandoc journal templates: Pandoc templates for the major statistics and biostatistics journals References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
