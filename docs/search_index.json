[["index.html", "An R companion to Statistics: data analysis and modelling Preface 0.1 Acknowledgements", " An R companion to Statistics: data analysis and modelling Maarten Speekenbrink 2020-10-21 Preface This is a companion to the book “Statistics: Data analysis and modelling”. It covers R is a programming language and environment specifically designed for data analysis. It is flexible, relatively fast, and has a large number of users and contributors. However, R is known to have a somewhat steep learning curve, so if you want to learn R, you will have to put in some extra effort (compared to e.g. JASP or SPSS). This effort will certainly pay off in the end, but it is up to you to decide whether you want to make this investment. This companion is meant to show you how to use R to do the types of analyses covered in “Statistics: Data analysis and modelling”. It is certainly not meant as a complete course on R. There are lots of good resources on R available on the internet and I suggest that, if you are serious about learning R, you also look elsewhere. Some sources you might find useful are: Beginner’s guide to R (Computer World) Interactive introduction to R programming (DataCamp) Try R (another interactive tutorial by codeschool) A freely downloadable book on R and statistics specifically focused on psychology students (especially part II and III are relevant): Learning statistics with R (Danielle Navarro) 0.1 Acknowledgements Parts of these notes were adapted from other sources (if there is a licence allowing that). I acknowledge these sources in footnotes. # automatically create a bib database for R packages knitr::write_bib(c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39; ), &#39;packages.bib&#39;) "],["introduction.html", "Chapter 1 Introduction 1.1 What is R?1 1.2 Getting started 1.3 Working with RStudio 1.4 Installing packages 1.5 Getting help 1.6 First steps: R as a calculator 1.7 Data 1.8 Exploring data: Descriptive statistics 1.9 Exploring data: Creating plots 1.10 Scatterplot 1.11 Raincloud plot", " Chapter 1 Introduction 1.1 What is R?1 R is a statistical programming language that has rapidly gained popularity in many scientific fields. It was developed by Ross Ihaka and Robert Gentleman as an open source implementation of the “S” programming language. (Next time you need a fun fact, you can say “Did you know that S came before R?”) R is also the name of the software that uses this language for statistical computing. With a huge online support community and dedicated packages that provide extra functionality for virtually any application and field of study, there’s hardly anything you can’t do in R. If you already know your way around statistical software like JASP or SPSS, the main difference is that R has no graphical user interface, which means there are no buttons to click and no dropdown menus. R can be run entirely by typing commands into a text interface (welcome to the Matrix!). This may seem a little daunting, but it also means a whole lot more flexibility, as you are not relying on a pre-determined toolkit for your analyses. If you need any more convincing, why are we using R and not one of the many other statistical packages like JASP, SPSS, MATLAB, Minitab, or even Microsoft Excel? Well, R is great because: R is free and open source, and always will be! Anybody can use the code and see exactly how it works. Because R is a programming language rather than a graphical interface, the user can easily save scripts as small text files for use in the future, or share them with collaborators. R has a very active and helpful online community - normally a quick search is all it takes to find that somebody has already solved the problem you’re having. 1.2 Getting started If you want to use R and RStudio, you should first install R, and after that install RStudio. R and RStudio are separate programs, and RStudio requires R to be installed. 1.2.1 Download R You can download R from CRAN (The Comprehensive R Archive Network). Select the link appropriate for your operating system and follow the instructions. You will want to download the installer for the latest release (currently version 4.0.2) of the base R software. As you can see, the CRAN website has a rather distinctive “old-school” look. Don’t let that fool you though. R itself is anything but old school. 1.2.2 Download R Studio R does not come with a graphical interface by default. Most people nowadays interact with R through second-party graphical platforms that provide extra functionality. Probably the most popular graphical front-end to R is RStudio. This is actually a full “integrated development environment” (IDE), but mostly, we will use it as a place where we can keep scripts, plots, and R output together in one place. Like R, RStudio is open source software and free to download for anyone that wants to. You can download RStudio from the RStudio website (select the free open source desktop version). 1.3 Working with RStudio When you open RStudio, you will see something like Figure 1.1. You will probably not see exactly the same layout, but once you click on File in the top menu, and then New File &gt; R Script, you should be pretty close. You can get direct access to the R environment itself in the console panel. If you type in commands here, they will be interpreted by R, and possibly some output is given. Working directly within the R console is handy if you want to do simple calculations, or try out different commands and functions. When you are conducting analyses, you will want to keep the commands that produce useful results somewhere, so you don’t have to type in everything again if you want to rerun the analyses, or if you need to change something. That is where R scripts come in handy. Basically, these are text files where you store a collection of commands that can be interpreted by R. Within RStudio, you can select lines of the script, and by clicking on Run, those lines will get pasted to the R console. Figure 1.1: The RStudio interface consists of four main panels. The source panel (top left) is where you can keep scripts. The console panel (bottom left) is where the R commands and text output go; this is where R truly lives. The environment and history panel (top right) shows which R objects are currently available in memory and a history of all the R commands the R console received. The files, plots, packages, etc panel contains a file browser, a display panel for all plots, a list of installed R packages, and a browser for help files. Another useful way to store R commands is in a different file format, called RMarkdown. Rmarkdown allows you to combine text, plots, and R commands all in a single file. This file can then be “parsed” to produce a variety of document formats, such as HTML, pdf, and even Microsoft Word. If you click on File &gt; New File &gt; R Markdown, you can see an example of such a file. Using a package like bookdown, you can even write whole books in R Markdown (like this one)! We will discuss R Markdown more at a later point. 1.4 Installing packages Part of the popularity of R stems from the thousands of packages that extent the basic capabilities of R. 1.4.1 Installing the sdamr package The “Statistics: Data analysis and modelling” book has an associated R package which contains the data sets used as examples in the book, as well as some additional functions. As it is still in development as I’m writing, it is not hosted on CRAN yet. That doesn’t mean you can’t install the package, but it requires a tiny bit more work. The source code of the sdamr package is hosted in GitHub, and the package can be installed from there with help of the remotes package. So you will first need to install that package, and then you can use the install_github function to install the sdamr package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;mspeekenbrink/sdam-r&quot;) Note that by typing remotes:: before the function call, we are telling R that the function is available in the remotes package. This avoids you having to load the package (i.e. by library(remotes)) first. To check whether the package is installed, type library(sdamr) 1.5 Getting help R may be tricky to master, especially at the start, but help is never far away: From within R If you want more information on a specific function, use a question mark before it (e.g., ?plot) or the help function (e.g., help(plot)) If you don’t know the function name, use two question marks (e.g., ??plot) or the help.search function (e.g., help.search(\"plot\")) If you know a function is in a package, use search help on the package (e.g., ?ggplot2) The RSiteSearch(\"keyword\") function will will search for “keyword” in all functions available in R, associated packages, and the R-Help News groups (if desired). Online Stack Overflow is a platform in which you can ask questions about R and its many packages. Many questions will already have been asked, so its archive of questions and answers is particularly useful. The meta-search engine at www.rseek.org may also be handy. R has an active help mailing list as well, but when asking questions there, make sure you read the posting guide, as some people on there sometimes get a little grumpy. 1.6 First steps: R as a calculator R can be used as a console-based calculator. Here are some examples. 2 + 11 # addition ## [1] 13 2 * 11 # multiplication ## [1] 22 2 / 11 # division ## [1] 0.1818182 2^(11) # exponentiation ## [1] 2048 sqrt(2) # square root ## [1] 1.414214 2^(1/2) # another way to compute the square root ## [1] 1.414214 # the order matters! 2 + 11*3 ## [1] 35 (2 + 11)*3 # R follows the usual rules of arithmetic ## [1] 39 Note that the hash symbol (“#”) is used for comments, such that anything following a “#” is not evaluated. 1.7 Data You can load in data files that come with R packages by using the data function, with as argument the name of the dataset you want to load (as a string, so make sure you use quote signs). For instance, you can load the dataset fifa2010teams from the sdamr package as follows: library(sdamr) data(&quot;fifa2010teams&quot;) A loaded dataset will show up in the Environment panel in RStudio. If you click on the name of the dataset, you can then see the data as a table in the Source panel. You can also view the data in the R console by simply typing the name of the dataset. This will often produce a lot of output. If you just want to view a part of the dataset, you can use the head function, which will show the first 6 rows: head(fifa2010teams) ## nr team matches_played goals_for goals_scored goals_against ## 1 1 Germany 7 16 16 5 ## 2 2 Netherlands 7 12 11 6 ## 3 3 Uruguay 7 11 11 8 ## 4 4 Argentina 5 10 9 6 ## 5 5 Brazil 5 9 9 4 ## 6 6 Spain 7 8 8 2 ## penalty_goal own_goals_for yellow_cards indirect_red_cards direct_red_cards ## 1 0 0 13 0 0 ## 2 0 0 24 0 0 ## 3 1 0 11 0 1 ## 4 0 0 7 0 0 ## 5 0 0 9 0 1 ## 6 0 0 8 0 0 You can also get a quick summary of the characteristics of the variables in the data through the summary function: summary(fifa2010teams) ## nr team matches_played goals_for ## Min. : 1.00 Length:32 Min. :3.00 Min. : 0.000 ## 1st Qu.: 8.75 Class :character 1st Qu.:3.00 1st Qu.: 2.000 ## Median :16.50 Mode :character Median :3.50 Median : 3.000 ## Mean :16.50 Mean :4.00 Mean : 4.531 ## 3rd Qu.:24.25 3rd Qu.:4.25 3rd Qu.: 5.250 ## Max. :32.00 Max. :7.00 Max. :16.000 ## goals_scored goals_against penalty_goal own_goals_for ## Min. : 0.000 Min. : 1.000 Min. :0.0000 Min. :0 ## 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.:0.0000 1st Qu.:0 ## Median : 3.000 Median : 5.000 Median :0.0000 Median :0 ## Mean : 4.469 Mean : 4.531 Mean :0.2812 Mean :0 ## 3rd Qu.: 5.250 3rd Qu.: 5.250 3rd Qu.:0.2500 3rd Qu.:0 ## Max. :16.000 Max. :12.000 Max. :2.0000 Max. :0 ## yellow_cards indirect_red_cards direct_red_cards ## Min. : 2.000 Min. :0 Min. :0.0000 ## 1st Qu.: 6.000 1st Qu.:0 1st Qu.:0.0000 ## Median : 7.500 Median :0 Median :0.0000 ## Mean : 8.156 Mean :0 Mean :0.2812 ## 3rd Qu.: 9.000 3rd Qu.:0 3rd Qu.:0.2500 ## Max. :24.000 Max. :0 Max. :2.0000 1.7.1 Data types Data in R is generally stored in vectors, which are fixed-length collections of values of a particular data type. Common data types are logical: values which can either be TRUE or FALSE numeric: numbers of all kinds, such as 1, 356, and 34.5782 character: characters and strings, such as q and Hello You can combine values of a data type in a vector by using the c() function (which stands for “combine”). For instance c(TRUE, FALSE, TRUE, TRUE) ## [1] TRUE FALSE TRUE TRUE c(3,4,802.376) ## [1] 3.000 4.000 802.376 c(&quot;Coffee&quot;,&quot;now&quot;,&quot;please&quot;) ## [1] &quot;Coffee&quot; &quot;now&quot; &quot;please&quot; If you combine elements of different data types, then R will convert them to the most “general” type necessary. Combining a logical value with a numeric one, for instance, will convert logical value TRUE to 1, and FALSE to 0. Combining a character element with other elements, will convert everything to character elements: c(TRUE, FALSE, 12) ## [1] 1 0 12 c(TRUE, 5.67788, &quot;let&#39;s see what happens&quot;) ## [1] &quot;TRUE&quot; &quot;5.67788&quot; &quot;let&#39;s see what happens&quot; 1.7.2 Objects Objects are named things that are stored in memory and available to functions etc. Objects can be vectors, such as discussed above, but also more general types, such as matrices, factors, and data frames. If you want to create an object, you use the assignment operator &lt;-, with on the left side the name you want to give to the object, and on the right side the content of the object. For instance, we can store a numeric vector as the object my_vector as follows: my_vector &lt;- c(1,2,10:20) Note a little trick above, where 10:20 stands for a sequence of integers, i.e. \\(10, 11, 12, \\ldots, 20\\). my_vector is now an object in R memory (you should see it show up in the Environment panel), and can be called by name, as in: my_vector ## [1] 1 2 10 11 12 13 14 15 16 17 18 19 20 A matrix is a collection of vectors of the same length, joined as columns or rows. mat &lt;- matrix(1:10,ncol=2) mat # matrices are filled column-wise ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 mat[,2] # select the second column (the result is a vector) ## [1] 6 7 8 9 10 mat[3,1] # select the value in the third row and first column ## [1] 3 A factor is useful for nominal and ordinal data. A factor is a vector with integers, where each integer is provided with a unique label. For instance # construct a factor by giving integer values and specifying the accompanying # labels fact &lt;- factor(c(1,2,2,3),labels=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) fact # display it ## [1] red green green blue ## Levels: red green blue fact == &quot;green&quot; # determine which elements equal (==) &#39;green&#39; ## [1] FALSE TRUE TRUE FALSE A list is a collection of different R objects. This is a very general type of object, and the elements of a list can even be lists themselves. A list allows you to keep different types of information together, but you probably won’t need to use it much for the content discussed here. But let’s quickly look at some aspects of a list: lst &lt;- list(a=mat, b=fact) # construct a named list with a matrix and factor lst ## $a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 ## ## $b ## [1] red green green blue ## Levels: red green blue If a list is named, meaning the elements have names, like above, you can select elements from the list by using a dollar sign and then the name of the elements: lst$a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 lst$b ## [1] red green green blue ## Levels: red green blue You can also select elements by an index number, which should go in between double square brackets. For instance, if you want to select the first element, you can type lst[[1]] A data.frame is probably one of the most useful features of R for data analysis. A data.frame is like a matrix, in that it is a rectangular collection of data, but the columns are variables which can be of a different type (e.g., numeric, factors, or characters). You can construct data frames through the data.frame function, for instance as my_data_frame &lt;- data.frame(var1 = 1:10, var2 = 10:1, var3 = rep(c(&quot;a&quot;,&quot;b&quot;),times=5)) my_data_frame ## var1 var2 var3 ## 1 1 10 a ## 2 2 9 b ## 3 3 8 a ## 4 4 7 b ## 5 5 6 a ## 6 6 5 b ## 7 7 4 a ## 8 8 3 b ## 9 9 2 a ## 10 10 1 b Constructing a data frame looks similar to constructing a list, but all the (named) arguments should have the same length. If you for instance try data.frame(var1 = 1:10, var2 = 1:11) you will get an error. You won’t always get an error. If the length of longer elements are multiples of the length of shorter elements, R will fill in the missing values by repeating the shorter elements until they are of the same length as the longer elements. For instance data.frame(var1 = 1:10, var2 = 1:5, var3 = 1) ## var1 var2 var3 ## 1 1 1 1 ## 2 2 2 1 ## 3 3 3 1 ## 4 4 4 1 ## 5 5 5 1 ## 6 6 1 1 ## 7 7 2 1 ## 8 8 3 1 ## 9 9 4 1 ## 10 10 5 1 This can be handy, but also risky, as sometimes you might not realise that R is filling in values for you, and your analyses might give rather unexpected results. I would therefore always ensure that you create a data frame with elements of the same length. Most of the time, you won’t create data frames yourself within R, but you will load in external data as a data frame. 1.7.3 Importing data R can load data in many formats. Personally, I mainly use data stored in “comma separated value” (CSV) format. This is one of the most portable ways of storing data, so that it can be used in a variety of programs like R, SPSS, JASP, Excel, etc. Data in a comma-separated value (CSV) format can be read through the read.csv function. A nice thing about R is that it can read data directly from the World Wide Web. So you don’t need to first download data, and then open it from within R. [TODO: example] At some point, you will probably also come across data stored in Excel or SPSS format. regarding Excel, it is safest to first save a spreadsheet as a CSV file, and then load this file into R. Alternatively, the xlsx package provides the function read.xlsx to directly read a spreadsheet into R. To load data in SPSS format, the package foreign package provides the read.spss function. 1.8 Exploring data: Descriptive statistics Measures of location and spread can be computed through specialized functions, namely mean, median, IQR (inter-quartile range), var (variance), and sd (standard deviation). E.g. mean(fifa2010teams$goals_for) ## [1] 4.53125 median(fifa2010teams$goals_for) ## [1] 3 will give you the mean and median of variable goals_for in data.frame fifa2010teams. You can obtain the inter-quartile range as IQR(fifa2010teams$goals_for,type=1) ## [1] 3 Note the use of the type argument here. There are many ways in which to compute and estimate percentiles and quantiles. Using type=1 gives you the same result as the way I explained how to compute the IQR in the book. By default, R will use type = 7, which gives different results (type ?quantile for more information). The var and sd functions from base R do not actually provide the sample variance_ and sample standard deviation. Rather, they give unbiased estimates of the “true” (population) variance and standard deviation. To compute the variance and standard deviation of the sample data, you can use the sample_var and sample_sd functions in the sdamr package: sample_var(fifa2010teams$goals_for) ## [1] 13.24902 sample_sd(fifa2010teams$goals_for) ## [1] 3.639921 There is no function in base R to compute the mode2, but the sdamr package provides the function sample_mode to do just that: sample_mode(fifa2010teams$goals_for) ## [1] 3 1.9 Exploring data: Creating plots There are two common ways to plot data with R. Base R has various plotting functions, such as plot, hist, boxplot, which are useful for quick plots to explore your data. The resulting plots are not always the most aesthetically pleasing. The R package ggplot2 provides means to create a wide range of useful and beautiful plots. It is based on the idea of a “grammar of graphics”, which makes it extremely flexible, but also a little difficult to get your head around. In the following, I will show you how to use both base R and ggplot2. 1.9.1 Histogram R has many built-in plotting functions. These tend to be a little basic, and much prettier plots can be made with packages such as ggplot2 (my current favourite!). But for quick data exploration, the built-in plotting functions are faster. A histogram is plotted through the hist function. In the following example, I first generate some random data, store it in an object called dat and then plot a histogram: hist(fifa2010teams$goals_for) There are many parameters you can change. In the following, I give the plot a new title and x-axis labels, as well as request the number of bins to be 20: hist(fifa2010teams$goals_for,main=&quot;Histogram of points scored by teams in the FIFA 2010 World Cup&quot;, xlab=&quot;Goals for&quot;, breaks=20) To create a nicer looking plot, you can use ggplot2. library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram() Well, that’s not actually so pretty. We can make it better by changing some of the defaults: library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram(bins=10, colour=&quot;black&quot;, fill=&#39;#8C8279&#39;) + xlab(&quot;Goals scored&quot;) Note that within the geom_histogram function, I’ve specified to use 10 bins, and draw a black line around the bars, and fill the bars with colour specified by the hexadecimal colour code ‘#8C8279’. Finally,, I’m using the xlab function to generate a better label for the x-axis. ggplot2 is very powerful and flexible, so there are many such adjustments you can make. 1.9.2 Boxplot For a quick boxplot, you can use the base R function with the same name: boxplot(fifa2010teams$goals_for) ggplot2 also provides a boxplot through the geom_boxplot function. Note that in the aes specification, I’m now using goals_for as the y-axis. ggplot(fifa2010teams,aes(y=goals_for)) + geom_boxplot() Not very pretty! A somewhat better version can be obtained by: ggplot(fifa2010teams,aes(x=&quot;&quot;,y=goals_for)) + geom_boxplot(width=.2) + xlab(&quot;&quot;) 1.10 Scatterplot plot(x=fifa2010teams$matches_played, y=fifa2010teams$goals_for) ggplot(fifa2010teams, aes(x=matches_played, y=goals_for)) + geom_point() 1.11 Raincloud plot A basic (but reasonably flexible) function to create a raincloud plot is provided in the sdamr package through the plot_raincloud function. The data argument expects a data frame, and the y argument expects the name of the variable for which you want to create the plot. Note that as the jitter applied to the plot is random, to get exactly the same plot again, you need to set the random number seed through set.seed before. This is only necessary if you want to recreate a plot exactly. We’ll talk more about random number generation later. set.seed(467) plot_raincloud(data=fifa2010teams, y=goals_for) This section contains material adapted from https://ourcodingclub.github.io/tutorials/intro-to-r/↩︎ There is a function mode, but this does something rather different!↩︎ "],["statistical-modelling.html", "Chapter 2 Statistical modelling 2.1 Distributions 2.2 Estimation 2.3 Hypothesis testing directly with the binomial distribution", " Chapter 2 Statistical modelling As R was written by statisticians for statisticians, it naturally has very good support for statistical modelling. 2.1 Distributions R includes functions to calculate probabilities and generate random data from a wide variety of distributions (type ?distributions for an overview). Distribution R name Parameters Binomial binom size (\\(n\\)), prob (\\(\\theta\\)) Chi-squared chisq df F f df1, df2 Normal norm mean (\\(\\mu\\)), sd (\\(\\sigma\\)) Student’s t t df The statistical distribution functions all have a similar interface: you can generate random data by calling the R name with an r (for random) before it (e.g. rbinom) you can compute the probability (or density function value) of a particular value by calling the function with a d (for density) before it (e.g. dbinom) you can compute a cumulative probability (the probability of a particular value and anything lower than it) by calling the function with a p (for probability) before it (e.g. pbinom) you can compute a quantile (the value such that there is a particular probability of sampling a value equal to or lower that it) by calling the function with a q (for probability) before it (e.g. qbinom) 2.1.1 Generating random data let’s use a coin tossing model to generate lots of replications of an experiment in which Paul is asked to provide 8 predictions. The dependent variable in each experiment is \\(Y_i = k\\), where \\(k\\) is the number of correct predictions (out of \\(n=8\\)). To generate 100 replications of such an experiment (e.g. 100 “alternate universes” where Paul might have made different predictions), we can use the rbinom function: set.seed(4638) Y &lt;- rbinom(n=100, size=8, prob=0.5) Y ## [1] 5 4 5 2 5 2 4 5 3 4 3 5 4 5 3 4 6 3 2 3 0 3 5 5 1 2 5 4 6 3 5 4 4 1 3 5 3 ## [38] 5 6 8 5 5 3 2 3 3 3 5 4 5 4 2 2 4 5 3 5 4 4 5 3 5 5 5 2 3 4 8 1 6 4 3 4 4 ## [75] 3 6 3 4 5 4 1 3 1 5 3 5 4 5 5 4 5 3 3 6 4 4 3 5 1 6 You can see quite some variability in the outcomes, perhaps easier in a histogram hist(Y, breaks=8) Computers can’t actually generate real random data. Computers are deterministic, performing computations according to instructions. But clever algorithms have been designed that produce sequences of numbers that are (almost) indistinguishable from purely random sequences. These algorithms are called random number generators and the default in R is the so-called “Mersenne-Twister” algorithm. You don’t have to worry about the details of such algorithms. We can just pretend that they produce truly random numbers. One thing I do want to point out is that before you use one of such algorithms, they need to be initialized with a special number, called the random seed. A number like this basically sets the algorithm in motion. Starting it with a different seed will produce a different sequence of random numbers. If you start it with the same random seed, you get exactly the same sequence of random numbers. If you don’t supply a value, R will use the current time as the random seed the first time you ask it to produce a random number in a session. For the purposes of replicating things, it may be useful to set the seed explicitly (so that a subsequent call to a random number generator will produce exactly the same results.) Therefore, you will now and then see calls such as set.seed(4638) in these notes, which ensures that these notes have the same content every time I run R. 2.2 Estimation 2.2.1 The likelihood function If we take a particular result, e.g. \\(Y=8\\) correct, it is straightforward to compute the probability of that result for different values of \\(\\theta\\). Earlier, we already used 1:10 to obtain a sequence of integers from 1 to 10. Not we ae going to use a more general function seq, to obtain a sequence of possible values for \\(\\theta\\) between 0 and 1. We will then supply this sequence as the prob argument to calculate the probability of the result according to each. possible_theta &lt;- seq(0,1,length=100) lik_theta &lt;- dbinom(8,size=8,possible_theta) plot(x=possible_theta,y=lik_theta,type=&quot;l&quot;) Instead of the first argument in dbinom being 8, you can try this for different values. The maximum likelihood estimate \\(\\hat{\\theta} = \\frac{k}{n}\\) is very simple to compute. Moreover, computations with R are vectorized. For instance, if Y is a vector, then Y/8 divides each element of Y by 8. And if Y and X are vectors of the same length, then Y/X divides each element of Y by the corresponding element of X. So, to compute the maximum likelihood estimate of \\(\\theta\\) for each of the simulated replications of the experiment, we can simply run: theta_est &lt;- Y/8 which stores the estimates in the object theta_est. 2.2.2 Calculating the likelihood ratio Having computed the estimate \\(\\hat{\\theta}\\) for each simulated dataset, we can also straightforwardly compute the likelihood ratio for each data set, comparing e.g. our MODEL R where we assume \\(\\theta = 0.5\\) to MODEL G where we use \\(\\theta = \\hat{\\theta}\\) instead. First, let’s caclulate the likelihood of each simulated dataset according to MODEL R. Again, we will make use of the fact that many functions in R are vectorized, such that if we ask R to calculate the probability for a vector of outcomes Y, the result is a vector with the probabilities for each element in Y likelihood_R &lt;- dbinom(Y, size=8, prob=.5) Remember, if you want to see what is in an R object, you can just type in the name: likelihood_R ## [1] 0.21875000 0.27343750 0.21875000 0.10937500 0.21875000 0.10937500 ## [7] 0.27343750 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 ## [13] 0.27343750 0.21875000 0.21875000 0.27343750 0.10937500 0.21875000 ## [19] 0.10937500 0.21875000 0.00390625 0.21875000 0.21875000 0.21875000 ## [25] 0.03125000 0.10937500 0.21875000 0.27343750 0.10937500 0.21875000 ## [31] 0.21875000 0.27343750 0.27343750 0.03125000 0.21875000 0.21875000 ## [37] 0.21875000 0.21875000 0.10937500 0.00390625 0.21875000 0.21875000 ## [43] 0.21875000 0.10937500 0.21875000 0.21875000 0.21875000 0.21875000 ## [49] 0.27343750 0.21875000 0.27343750 0.10937500 0.10937500 0.27343750 ## [55] 0.21875000 0.21875000 0.21875000 0.27343750 0.27343750 0.21875000 ## [61] 0.21875000 0.21875000 0.21875000 0.21875000 0.10937500 0.21875000 ## [67] 0.27343750 0.00390625 0.03125000 0.10937500 0.27343750 0.21875000 ## [73] 0.27343750 0.27343750 0.21875000 0.10937500 0.21875000 0.27343750 ## [79] 0.21875000 0.27343750 0.03125000 0.21875000 0.03125000 0.21875000 ## [85] 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 0.27343750 ## [91] 0.21875000 0.21875000 0.21875000 0.10937500 0.27343750 0.27343750 ## [97] 0.21875000 0.21875000 0.03125000 0.10937500 We can follow the same procedure to calculate the likelihood for each simulated dataset according to the estimated MODEL G. In this case though, we are going to use two vectorized arguments simultaneously. For each simulated dataset, we have an observed number of correct predictions (in our object Y), as well as a corresponding estimate of the probability of a correct response (in our object theta_est). If we supply each as arguments to the dbinom function, R will match each element in Y with the corresponding element of theta_est, i.e. Y[1] is matched with theta_est[1], y[2] with theta_est[2], etc. You will have to be quite certain that the indices (i.e., the number between the brackets, such as [1] and [2]) of one argument correspond to the indices of another to get the correct results. In this case, we computed theta_est directly from Y, and therefore we know each value in Y corresponds to the value of theta_est = y/8. Sowe can safely compute the likelihood of MODEL G with twp vectorised arguments as: likelihood_G &lt;- dbinom(Y, size=8, prob = theta_est) To compute the likelihood ratio for all these simulated datasets, we can now simply divide each element in likelihood_R by the corresponding element in likelihood_G: likelihood_ratio &lt;- likelihood_R/likelihood_G Great! We now have a distribution of likelihood ratio values, simulated from MODEL R. We can get closer and closer to the sampling distribution of the likelihood ratio values by increasing the number of simulated datasets. Here, I’m only simulating 100 datasets, but you can easily simulate many more. Let’s have a look at the distribution of likelihood ratio values with a histogram: hist(likelihood_ratio) Compared to an infinite number of datasets, 100 is quite a small number, but nevertheless the histogram already looks quite a bit like the true distribution of the likelihood ratio under MODEL R. 2.2.3 Working out the theoretical distribution of the likelihood ratio (TODO: will add this later) 2.3 Hypothesis testing directly with the binomial distribution To perform hypothesis tests directly with the binomial distribution, you can use the binom.test function. This function has 5 arguments: x: the number of “successes” (e.g., the number of correct guesses). n: the number of trials (e.g. the total number of guesses) p: the probability of success assumed under the null hypothesis (e.g. \\(\\underline{\\theta}\\)). The default value is p = 0.5. alternative: the range of values for p (i.e. \\(\\theta\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\theta &gt; \\underline{\\theta}\\) allowed), or less (only values \\(\\theta &lt; \\underline{\\theta}\\) allowed). The default value is alternative = \"two.sided\". conf.level: the confidence level for the returned confidence interval. Should be specified as a probability (i.e., to get the 95% confidence interval, you should specify conf.level = 0.95). The default value is conf.level = 0.95. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(0 \\leq \\theta \\leq 1\\) corresponds to a two-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta \\neq 0.5\\). This test can be performed as: binom.test(x=8, n = 8, p = 0.5) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.007812 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.6305834 1.0000000 ## sample estimates: ## probability of success ## 1 The output provides the estimated \\(\\theta\\) for MODEL G under sample estimates: probability of success. Remember that the p-value of the test reflects the probability of the provided number of successes or more extreme ones under the null hypothesis. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(\\theta &gt; 0.5\\) corresponds to a one-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta &gt; 0.5\\). This test can be performed by binom.test(x=8, n = 8, p = 0.5, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.003906 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.687656 1.000000 ## sample estimates: ## probability of success ## 1 "],["the-one-sample-t-test.html", "Chapter 3 The one-sample t-test 3.1 Missing values 3.2 Selecting subsets of data 3.3 One-sample t-test", " Chapter 3 The one-sample t-test You can open the anchoring data as follows: library(sdamr) data(anchoring) head(anchoring) ## session_id sex age citizenship referrer us_or_international lab_or_online ## 1 2400853 f 18 US abington US In-lab ## 2 2400856 f 19 CN abington US In-lab ## 3 2400860 f 18 US abington US In-lab ## 4 2400868 m 18 US abington US In-lab ## 5 2400914 f 18 US abington US In-lab ## 6 2400916 f 18 US abington US In-lab ## anchor everest_feet everest_meters ## 1 high 30000 NA ## 2 high 30000 NA ## 3 low 5000 NA ## 4 low 2400 NA ## 5 low 10000 NA ## 6 high 25000 NA 3.1 Missing values Sometimes a data set has missing values. In R, a missing value is shown as NA (for Not Available). For example, you can see missing in the everest_meters variable. If a variable has missing values, functions such as mean and sd will return a missing value, rather than a numeric value. For instance: mean(anchoring$everest_meters) ## [1] NA sd(anchoring$everest_meters) ## [1] NA You could get rid of all the missing values from your data.frame by using a subset function, but generally, I think it is better to keep the dataset as is, and use other means to avoid problems with missing values. Luckily, many functions have arguments to deal with missing values. For instance, the two functions above have an na.rm argument (for not-available-remove). Setting this to true will call the function only on the non-missing-values: mean(anchoring$everest_meters,na.rm=TRUE) ## [1] 6636.786 sd(anchoring$everest_meters,na.rm=TRUE) ## [1] 3942.714 3.2 Selecting subsets of data There are two main ways to select a subset of observations in R. You can either use an “indexing variable”, or use the subset function. ### Indexing variables An indexing variable is used to specify the rows in a data.frame that you want to use. Generally, an indexing variable is a logical variable, which takes the value TRUE for cases (rows) that you want to include, and FALSE for cases that you want to exclude. Using an index variable, we will treat the data.frame as a matrix, which allows us to use square brackets, as in data[row,column] to either select rows or columns. For example, anchoring[,\"age\"] selects the column named “age” and returns it, while anchoring[1:10,] selects rows 1 to 10. The nice thing about R is that instead of providing row row numbers, we can create a logical variable based on the data itself to select rows. To do so, we can use the logical comparators and operators: == “equal to” != “not equal to” &gt; “greater than” &gt;= “greater than or equal to” &lt; “smaller than” &lt;= “smaller than or equal to” &amp; “and” | “or” Some examples of using index variables are as follows. An index variable which is TRUE for males and FALSE for females can be computed as follows: index &lt;- anchoring$sex == &quot;m&quot; Let’s see what this variable looks like: head(index) ## [1] FALSE FALSE FALSE TRUE FALSE FALSE It is indeed a logical variable which is TRUE whenever sex is equal to \"m\", and FALSE otherwise. You can use it to select all the males in the anchoring data by: dat &lt;- anchoring[index,] Note that you don’t have to create an index variable separately. You can obtain the same result by computing the index variable within the brackets, like so: dat &lt;- anchoring[anchoring$sex == &quot;m&quot;,] You can select all males over 30 years of age, and check the number of observations in this subset by the nrow function, as follows: dat &lt;- anchoring[anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;,] nrow(dat) ## [1] 361 You can select all participants who are male and over 30 years of age, or females who are female and over 30 years of age by: dat &lt;- anchoring[(anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;) | (anchoring$age &gt; 30 &amp; anchoring$sex == &quot;f&quot;),] 3.2.1 The subset function The subset function is quite similar to using index variables, but it doesn’t require the treatment of the data.frame as a matrix and it looks for variable names in the data.frame so you don’t have to use e.g. anchoring$ before the variable name. This makes the subset function a bit easier to use than using indexing variables. The subset function has the following arguments: * x: the object (e.g. the data.frame) for which you want to select a subset of cases * subset: a logical expression indicating elements or rows to keep * select: an optional expression which indicates which columns to select from a data frame I generally use just the first two arguments. We can replicate the selections above using the subset function as follows: dat &lt;- subset(anchoring, sex == &quot;m&quot;) dat &lt;- subset(anchoring, age &gt; 30 &amp; sex == &quot;m&quot;) dat &lt;- subset(anchoring, (age &gt; 30 &amp; sex == &quot;m&quot;) | (age &gt; 30 &amp; sex == &quot;f&quot;)) For more information on indexing and subsetting, have a look at e.g. http://www.cookbook-r.com/Basics/Getting_a_subset_of_a_data_structure/ The data analysed in the SDAM book was selected as follows: dat &lt;- subset(anchoring,(referrer == &quot;swps&quot; | referrer == &quot;swpson&quot;) &amp; anchor == &quot;low&quot;) 3.3 One-sample t-test R has a t.test function which allows you to compute a variety of t-tests. For a one-sample t-test, you would use the following arguments * x: the variable for which to compute the t-test * mu: the assumed value of the mean, i.e. \\(\\underline{\\mu}\\) * alternative: similar as in binom.test, the range of values for mu (i.e. \\(\\mu\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\mu &gt; \\underline{\\mu}\\) allowed), or less (only values \\(\\mu &lt; \\underline{\\mu}\\) allowed). The default value is alternative = \"two.sided\". For instance, we can run the two-sided t-test also reported in the SDAM book by t.test(dat$everest_meters, mu=8848) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 1.558e-13 ## alternative hypothesis: true mean is not equal to 8848 ## 95 percent confidence interval: ## 5716.848 6907.537 ## sample estimates: ## mean of x ## 6312.193 A one-sided test where MODEL R assumes \\(\\mu = 8848\\) and MODEL G assumes that \\(\\mu &lt; 8848\\), is obtained by t.test(dat$everest_meters, mu=8848, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 7.791e-14 ## alternative hypothesis: true mean is less than 8848 ## 95 percent confidence interval: ## -Inf 6810.498 ## sample estimates: ## mean of x ## 6312.193 "],["regression.html", "Chapter 4 Regression 4.1 Estimating an testing a simple regression model 4.2 Model comparisons 4.3 Estimating and testing a multiple regression model 4.4 Residuals and predicted values 4.5 Plotting pairwise scatterplots for many variables", " Chapter 4 Regression 4.1 Estimating an testing a simple regression model Regression analysis is done through the lm function, with the following syntax: lm(formula, data,...). The first argument is called formula and expects a symbolic description of your model. I will tell you more about how to specify models with the formula syntax later, when we discuss moderation. For now, a few simple examples will suffice. To specify a simple regression model where you predict a dependent variable y by a predictor x, you would use the formula y ~ x On the left-hand side of the formula you need to provide the name of the dependent variable. After the name of the dependent variable, you need to put a tilde (~) (which you can read is “is modelled as a function of”). On the right-hand side, you then provide the name of the predictor. R will automatically include an intercept in the model. In R, the intercept is actually represented as a special predictor which always (for every row in the data set) has the value 1. The formula above is actually interpreted as y ~ x + 1 Because the intercept is included in most models, the authors of R have decided to save the you trouble of typing + 1 in each formula, by making this part of the formula implicit. You can fit a model without an intercept (which is the same as fixing the value of the intercept to 0), by instead of + 1, putting - 1 in the formula, as in y ~ x - 1 The second argument of the lm function is called data and expects the name of the data.frame in which the variables are stored. To see how the lm function works in practice, let’s open the trump2016 data provided in the sdamr package. This is the data analysed in Chapter 4 and 5. We open and inspect the data as usual: library(sdamr) # load the Trump data data(&quot;trump2016&quot;) # remove the data from the District of Columbia (Washintgon D.C.) dat &lt;- subset(trump2016,state != &quot;District of Columbia&quot;) head(dat) ## state hate_groups population hate_groups_per_million ## 1 Alabama 27 4863300 5.55 ## 2 Alaska 0 741894 0.00 ## 3 Arizona 18 6931071 2.60 ## 4 Arkansas 16 2988248 5.35 ## 5 California 79 39250017 2.01 ## 6 Colorado 16 5540545 2.89 ## percent_bachelors_degree_or_higher percent_in_poverty percent_Trump_votes ## 1 15.4 18.5 62.9 ## 2 29.7 10.4 52.9 ## 3 27.7 17.4 49.5 ## 4 21.8 18.7 60.4 ## 5 32.3 15.4 32.7 ## 6 39.2 11.5 44.4 You can see that there are a number of variables in the dataset (and not all of these were analysed in the book). For more information on the variables in the dataset, you can call the help file with ?trump2016. Now let’s estimate a simple regression model to predict the percentage of votes for Trump by the number of hate groups per million citizens: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million, data=dat) modg ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Coefficients: ## (Intercept) hate_groups_per_million ## 42.9 2.3 I’ve named the resulting object modg for MODEL G. You can pick any name you like for R objects. Note that when you just print a fitted linear model (by e.g., typing the name of the object modg), R will show the parameter estimates, but nothing else. You can get the important statistics by calling the summary function on the fitted model: summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.8206 -5.8570 -0.0529 5.2632 20.7883 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.8968 2.4094 17.804 &lt; 2e-16 *** ## hate_groups_per_million 2.3004 0.6715 3.426 0.00127 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.142 on 48 degrees of freedom ## Multiple R-squared: 0.1965, Adjusted R-squared: 0.1797 ## F-statistic: 11.74 on 1 and 48 DF, p-value: 0.001265 This provides quite a lot of useful information. The output of the summary function consists of four parts: Call simply shows you the call to the lm function used to fit the model (including the model formula) Residuals shows you some summary statistics for the prediction errors of the estimated model (which are often referred to as residuals) Coefficients shows you a table with: the name of variable for which the parameter was estimated Estimate: the estimated parameters Std. Error: the standard error of the estimates (this is the standard deviation of the sampling distribution of the estimates) t value: the t statistic of the hypothesis test that the true value of the parameter is equal to 0. Pr(&gt;|t|): the p-value, which is the probability that, given that the null hypothesis is true (i.e. the true value of the parameter is equal to 0), you would find a t-statistic at least as extreme as the one computed found for this data. Some overall model statistics: Residual standard error: this is and unbiased estimate of the standard deviation of the errors. Multiple R-squared: the \\(R^2\\) or proportion of variance of the dependent variable “explained” by the model. Adjusted R-squared: an unbiased estimate of the true value of \\(R^2\\) F-statistic: the results of a model comparison comparing the estimated model (MODEL G) to a MODEL R which only includes an intercept. 4.2 Model comparisons Comparing regression models and computing the \\(F\\) statistic can be done through the anova() function. Let’s first estimate a restricted version of MODEL G above where we fix the slope of hate_groups_per_million to 0. This MODEL R is identical to a model with only an intercept. We can estimate this by not providing any predictor names, but now explicitly providing the intercept term 1. # fit a MODEL R with only an intercept modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) We can then compute the \\(F\\) test by entering this MODEL R, and the MODEL G we estimated earlier, as arguments in the anova function: anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 48 4011.4 1 980.78 11.736 0.001265 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 the output lists the formula’s of the models we compare, and then provides a table with test results. The columns in this table are * Res.Df: the denominator degrees of freedom, i.e. \\(n=-\\text{npar}(M)\\) * RSS: the “residual sum of squares” or Sum of Squared Error of the model, i.e. \\(\\text{SSE}(M)\\) * Df: the numerator degrees of freedom,,i.e. $(G) - (R) * Sum of Sq: the reduction in the Sum of Squared Error, i.e. \\(\\text{SSE}(R) - \\text{SSE}(R)\\) * F: the \\(F\\) statistic of the test * Pr(&gt;F): the p-value of the test. We can obtain a test for the intercept by fitting a different MODEL R, now without an intercept, and comparing it to MODEL G # fit a MODEL R without an intercept (through &quot; - 1&quot;) modr &lt;- lm(percent_Trump_votes ~ hate_groups_per_million - 1, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ hate_groups_per_million - 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 30501.7 ## 2 48 4011.4 1 26490 316.98 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output of the anova function isn’t particularly pretty. Also, if you want to do multiple model comparisons, first estimating models and then comparing them with the anova function becomes a little cumbersome. An easier way to obtain all the model comparisons is to use the Anova function from the car package to automatically construct different possible versions of MODEL R, each being one particular restriction of MODEL G which fixes the relevant parameter to 0. If you don’t have the car package installed yet, you need to install it first (e.g. by install.packages(\"car\"). You can then call: library(car) Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 26490.3 1 316.981 &lt; 2.2e-16 *** ## hate_groups_per_million 980.8 1 11.736 0.001265 ** ## Residuals 4011.4 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that it is important to give the type = 3 argument in the Anova function. This will construct MODEL R by fixing a single parameter to 0 in turn (i.e. first fixing \\(\\beta_0=0\\) and estimating all other parameters, then another model fixing \\(\\beta_1 = 0\\) for estimating all other parameters), etc. 4.3 Estimating and testing a multiple regression model To specify a multiple regression model for a dependent variable named y and with three predictors, named x1, x2, and x3, you would use the formula y ~ x1 + x2 + x3 This is similar to the earlier formula, but you now need to provide the names of all the predictors, separated by a + sign. For instance, we can fit a model with two predictors (which we will call modg, for MODEL G), as follows: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million + ## percent_bachelors_degree_or_higher, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.7268 -4.0621 0.0426 2.8937 15.8359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81.9945 6.1555 13.321 &lt; 2e-16 *** ## hate_groups_per_million 1.3138 0.5102 2.575 0.0132 * ## percent_bachelors_degree_or_higher -1.2187 0.1839 -6.625 3.03e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.643 on 47 degrees of freedom ## Multiple R-squared: 0.5845, Adjusted R-squared: 0.5668 ## F-statistic: 33.06 on 2 and 47 DF, p-value: 1.087e-09 the output of the summary function contains the same elements as before, but the table of coefficients now includes an additional row for percent_bachelors_degree_or_higher. Also, note that all the estimates are different, because the slopes reflect unique effects, and these differ compared to models with other predictors. Finally, I’d like to point out that the last row of the output contains the “whole model test”, which compares the estimated model to a model with only an intercept. Recall that the estimate of the intercept in this latter model equals the sample mean. So we are now comparing a model with two predictors to a model which predicts all values as the sample mean. The difference in the number of estimated parameters for this comparison is \\(\\text{npar}(G) - \\text{npar}(R) = 3 - 1 = 2\\). Hence, the degrees of freedom are \\(\\text{df}_1 = 2\\) and \\(\\text{df}_2 = n - \\text{npar}(G) = 50 - 3 = 47\\). We can also get all the model comparisons for this MODEL G through: Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 7830.7 1 177.4381 &lt; 2.2e-16 *** ## hate_groups_per_million 292.7 1 6.6318 0.01322 * ## percent_bachelors_degree_or_higher 1937.2 1 43.8956 3.028e-08 *** ## Residuals 2074.2 47 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Finally, we can also obtain a “whole model test”, by comparing an intercept-only MODEL R to the full MODEL G. This is best done through the anova function as follows: modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 47 2074.2 2 2918 33.06 1.087e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4 Residuals and predicted values You can obtain the prediction errors by the calling the residuals function on the fitted model. # store the residuals as errorg errorg &lt;- residuals(modg) head(errorg) ## 1 2 3 4 5 6 ## -7.618118 7.101376 -2.152059 -2.055564 -12.570794 6.382163 This returns a vector with, for each case in the data (each row in the data frame), the error term \\(\\hat{\\epsilon}_i\\). Note that we are only displaying the first six elements through the head function. You can obtain the predicted values by calling the predict function on the fitted model. # store the predictions as predictg predictg &lt;- predict(modg) head(predictg) ## 1 2 3 4 5 6 ## 70.51812 45.79862 51.65206 62.45556 45.27079 38.01784 This returns a vector with for, each case in the data, the predicted value \\(\\hat{Y}_{M,i}\\). You can use these variables to create e.g. a histogram of the errors: hist(errorg) and a predicted by residual plot # scatterplot of predicted vs residual plot(predictg, errorg, xlab = &quot;predicted&quot;, ylab = &quot;residual&quot;) # add a horizontal line (h=0 is for horizontal at 0, # and lty = 3 makes it a dotted line abline(h=0,lty=3) You can also call the plot function directly on the fitted model, which produces a range of plots to assess the model assumptions: plot(modg) 4.5 Plotting pairwise scatterplots for many variables A final tip relates to exploring relations between many variables (e.g. potential predictors and dependent variables). While you can inspect pairwise relations between variables by creating a scatterplot for each pair of variables, this quickly becomes tedious. You can save yourself some work by using a function that produces a matrix of pairwise scatterplots directly. One option for this is to use the pairs function, and supply this with a selection of variables in a data.frame. For instance, in the data set we considered now, we might be interested in the relations between hate_groups_per_million, percent_bachelors_degree_or_higher, percent_in_poverty, and percent_Trump_votes. We can obtain a matrix of all pairwise scatterplots between these variables as follows (note that rather than typing the variable names, I’m selecting column 4 to 7, which correspond to these variables): pairs(dat[,4:7]) If you don’t like the look of these base R graphics and prefer ggplot2, you can use the ggpairs function from the GGally package to get a similar plot: library(GGally) ggpairs(dat[,4:7]) "]]
