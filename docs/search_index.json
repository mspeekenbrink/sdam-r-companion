[["index.html", "An R companion to Statistics: data analysis and modelling Preface 0.1 Acknowledgements", " An R companion to Statistics: data analysis and modelling Maarten Speekenbrink 2021-09-29 Preface This is a companion to the book Statistics: Data analysis and modelling. It covers how to perform the analyses discussed in that book, mostly using “base” R and a relatively small selection of add-on packages. R is a programming language and environment specifically designed for data analysis. It is flexible, relatively fast, and has a large number of users and contributors. However, R is known to have a somewhat steep learning curve, so if you want to learn R, you will have to put in some extra effort (compared to e.g. JASP or SPSS). This effort will certainly pay off in the end, but it is up to you to decide whether you want to make this investment. This companion is meant to show you how to use R to do the types of analyses covered in “Statistics: Data analysis and modelling.” It is certainly not meant as a complete course on R. There are lots of good resources on R available on the internet and I suggest that, if you are serious about learning R, you also look elsewhere. Some sources you might find useful are: Beginner’s guide to R (Computer World) Interactive introduction to R programming (DataCamp) Try R (another interactive tutorial by codeschool) A freely downloadable book on R and statistics specifically focused on psychology students (especially part II and III are relevant): Learning statistics with R (Danielle Navarro) 0.1 Acknowledgements Parts of these notes were adapted from other sources (if there is a licence allowing that). I acknowledge these sources in the text or footnotes. # automatically create a bib database for R packages knitr::write_bib(c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;, &#39;car&#39;, &#39;lme4&#39;, &#39;mediation&#39;, &#39;ggplot2&#39;, &#39;afex&#39;, &#39;lmerTest&#39;, &#39;BayesFactor&#39;, &#39;GGally&#39;, &#39;codingMatrices&#39;, &#39;emmeans&#39; ), &#39;packages.bib&#39;) "],["introduction.html", "Chapter 1 Introduction 1.1 What is R?1 1.2 Getting started 1.3 Working with RStudio 1.4 Installing packages 1.5 Getting help 1.6 First steps: R as a calculator 1.7 Data 1.8 Exploring data: Descriptive statistics 1.9 Exploring data: Creating plots 1.10 Scatterplot 1.11 Raincloud plot", " Chapter 1 Introduction 1.1 What is R?1 R is a statistical programming language that has rapidly gained popularity in many scientific fields. It was developed by Ross Ihaka and Robert Gentleman as an open source implementation of the “S” programming language. (Next time you need a fun fact, you can say “Did you know that S came before R?”) R is also the name of the software that uses this language for statistical computing. With a huge online support community and dedicated packages that provide extra functionality for virtually any application and field of study, there’s hardly anything you can’t do in R. If you already know your way around statistical software like JASP or SPSS, the main difference is that R has no graphical user interface, which means there are no buttons to click and no dropdown menus. R can be run entirely by typing commands into a text interface (welcome to the Matrix!). This may seem a little daunting, but it also means a whole lot more flexibility, as you are not relying on a pre-determined toolkit for your analyses. If you need any more convincing, why are we using R and not one of the many other statistical packages like JASP, SPSS, MATLAB, Minitab, or even Microsoft Excel? Well, R is great because: R is free and open source, and always will be! Anybody can use the code and see exactly how it works. Because R is a programming language rather than a graphical interface, the user can easily save scripts as small text files for use in the future, or share them with collaborators. R has a very active and helpful online community - normally a quick search is all it takes to find that somebody has already solved the problem you’re having. 1.2 Getting started If you want to use R and RStudio, you should first install R, and after that install RStudio. R and RStudio are separate programs, and RStudio requires R to be installed. 1.2.1 Download R You can download R from CRAN (The Comprehensive R Archive Network). Select the link appropriate for your operating system and follow the instructions. You will want to download the installer for the latest release (currently version 4.0.2) of the base R software. As you can see, the CRAN website has a rather distinctive “old-school” look. Don’t let that fool you though. R itself is anything but old school. 1.2.2 Download R Studio R does not come with a graphical interface by default. Most people nowadays interact with R through second-party graphical platforms that provide extra functionality. Probably the most popular graphical front-end to R is RStudio. This is actually a full “integrated development environment” (IDE), but mostly, we will use it as a place where we can keep scripts, plots, and R output together in one place. Like R, RStudio is open source software and free to download for anyone that wants to. You can download RStudio from the RStudio website (select the free open source desktop version). 1.3 Working with RStudio When you open RStudio, you will see something like Figure 1.1. You will probably not see exactly the same layout, but once you click on File in the top menu, and then New File &gt; R Script, you should be pretty close. You can get direct access to the R environment itself in the console panel. If you type in commands here, they will be interpreted by R, and possibly some output is given. Working directly within the R console is handy if you want to do simple calculations, or try out different commands and functions. When you are conducting analyses, you will want to keep the commands that produce useful results somewhere, so you don’t have to type in everything again if you want to rerun the analyses, or if you need to change something. That is where R scripts come in handy. Basically, these are text files where you store a collection of commands that can be interpreted by R. Within RStudio, you can select lines of the script, and by clicking on Run, those lines will get pasted to the R console. R scripts should only contain working R commands. You can comment on your code by preceding a line (or the end of a line) by a hash-symbol (“#”). Anything after the has symbol is not evaluated by the R interpreter. Figure 1.1: The RStudio interface consists of four main panels. The source panel (top left) is where you can keep scripts. The console panel (bottom left) is where the R commands and text output go; this is where R truly lives. The environment and history panel (top right) shows which R objects are currently available in memory and a history of all the R commands the R console received. The files, plots, packages, etc panel contains a file browser, a display panel for all plots, a list of installed R packages, and a browser for help files. Another useful way to store R commands is in a different file format, called R Markdown. R Markdown allows you to combine text, plots, and R commands all in a single file. This file can then be “parsed” to produce a variety of document formats, such as HTML, pdf, and even Microsoft Word. If you click on File &gt; New File &gt; R Markdown in RStudio, you can see an example of such a file. As the name suggests, R Markdown is a combination of R and Markdown. Markdown is a lightweight markup language for creating formatted text documents with a plain-text editor. A markup language is, roughly put, a system which defines elements in a document by their role, for instance defining certain elements as titles or headers, and others as quoted text or test that should be emphasised. Common examples of markup languages are HTML and XML. If you know a little HTML, you might know that in modern implementations, it separates content and markup (HTML) from style (CSS). This separation allows you to easily create a variety of documents which are visually very different from the same HTML source file. When you use a word processor such as Microsoft Word, it creates a single document which specifies both content and style, and in a way which is specific to the word processor used. Markdown aims to provide a way to define a software-agnostic markup language, separating content from style, which can be used to produce a variety of output formats from the same source file. R markdown add to this an integration with R. Effectively, R Markdown first evaluates all the R code in an R Markdown file to create a “plain” markdown file, which can then be parsed into a variety of output formats. The great thing about this is that you can create automatically reproducible documents, and you don’t have to copy-paste results of analyses between R and your word processor, avoiding the common mistakes that this brings. And R Markdown is really flexible. For example, using a package like bookdown, you can even write whole books in R Markdown (like this one)! We will discuss R Markdown in more detail at a later point. If you want to get started already, a very useful resource is R Markdown: The Definitive Guide. 1.4 Installing packages Part of the popularity of R stems from the thousands of packages that extend the basic capabilities of R. 1.4.1 Installing the sdamr package The “Statistics: Data analysis and modelling” book has an associated R package which contains the data sets used as examples in the book, as well as some additional functions. It should be available on CRAN soon. Once it is available there, you can install it simply by typing install.packages(&quot;sdamr&quot;) If it is not available there, you will see a warning (package ‘sdamr’ is not available for this version of R). If that happens, you can install the development version from GitHub (see below). The source code of the sdamr package is hosted in GitHub, and the package can be installed from there as well with help of the remotes package. So you will first need to install that package, and then you can use the install_github function to install the sdamr package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;mspeekenbrink/sdam-r&quot;) Note that by typing remotes:: before the function call, we are telling R that the function is available in the remotes package. This avoids you having to load the package (i.e. by library(remotes)) first. To check whether the package is installed, type library(sdamr) If you can’t get this to work on your system, wait until the sdamr package becomes available on CRAN. 1.5 Getting help R may be tricky to master, especially at the start, but help is never far away: From within R If you want more information on a specific function, use a question mark before it (e.g., ?plot) or the help function (e.g., help(plot)) If you don’t know the function name, use two question marks (e.g., ??plot) or the help.search function (e.g., help.search(\"plot\")) If you know a function is in a package, use search help on the package (e.g., ?ggplot2) The RSiteSearch(\"keyword\") function will will search for “keyword” in all functions available in R, associated packages, and the R-Help News groups (if desired). Online Stack Overflow is a platform in which you can ask questions about R and its many packages. Many questions will already have been asked, so its archive of questions and answers is particularly useful. The meta-search engine at www.rseek.org may also be handy. R has an active help mailing list as well, but when asking questions there, make sure you read the posting guide, as some people on there sometimes get a little grumpy. 1.6 First steps: R as a calculator R can be used as a console-based calculator. Here are some examples. 2 + 11 # addition ## [1] 13 2 * 11 # multiplication ## [1] 22 2 / 11 # division ## [1] 0.1818182 2^(11) # exponentiation ## [1] 2048 sqrt(2) # square root ## [1] 1.414214 2^(1/2) # another way to compute the square root ## [1] 1.414214 # the order matters! 2 + 11*3 ## [1] 35 (2 + 11)*3 # R follows the usual rules of arithmetic ## [1] 39 Note that the hash symbol (“#”) is used for comments, such that anything following a “#” is not evaluated. 1.7 Data You can load in data files that come with R packages by using the data function, with as argument the name of the dataset you want to load (as a string, so make sure you use quote signs). For instance, you can load the dataset fifa2010teams from the sdamr package as follows: library(sdamr) data(&quot;fifa2010teams&quot;) A loaded dataset will show up in the Environment panel in RStudio. If you click on the name of the dataset, you can then see the data as a table in the Source panel. You can also view the data in the R console by simply typing the name of the dataset. This will often produce a lot of output. If you just want to view a part of the dataset, you can use the head function, which will show the first 6 rows: head(fifa2010teams) ## nr team matches_played goals_for goals_scored goals_against ## 1 1 Germany 7 16 16 5 ## 2 2 Netherlands 7 12 11 6 ## 3 3 Uruguay 7 11 11 8 ## 4 4 Argentina 5 10 9 6 ## 5 5 Brazil 5 9 9 4 ## 6 6 Spain 7 8 8 2 ## penalty_goal own_goals_for yellow_cards indirect_red_cards direct_red_cards ## 1 0 0 13 0 0 ## 2 0 0 24 0 0 ## 3 1 0 11 0 1 ## 4 0 0 7 0 0 ## 5 0 0 9 0 1 ## 6 0 0 8 0 0 You can also get a quick summary of the characteristics of the variables in the data through the summary function: summary(fifa2010teams) ## nr team matches_played goals_for ## Min. : 1.00 Length:32 Min. :3.00 Min. : 0.000 ## 1st Qu.: 8.75 Class :character 1st Qu.:3.00 1st Qu.: 2.000 ## Median :16.50 Mode :character Median :3.50 Median : 3.000 ## Mean :16.50 Mean :4.00 Mean : 4.531 ## 3rd Qu.:24.25 3rd Qu.:4.25 3rd Qu.: 5.250 ## Max. :32.00 Max. :7.00 Max. :16.000 ## goals_scored goals_against penalty_goal own_goals_for ## Min. : 0.000 Min. : 1.000 Min. :0.0000 Min. :0 ## 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.:0.0000 1st Qu.:0 ## Median : 3.000 Median : 5.000 Median :0.0000 Median :0 ## Mean : 4.469 Mean : 4.531 Mean :0.2812 Mean :0 ## 3rd Qu.: 5.250 3rd Qu.: 5.250 3rd Qu.:0.2500 3rd Qu.:0 ## Max. :16.000 Max. :12.000 Max. :2.0000 Max. :0 ## yellow_cards indirect_red_cards direct_red_cards ## Min. : 2.000 Min. :0 Min. :0.0000 ## 1st Qu.: 6.000 1st Qu.:0 1st Qu.:0.0000 ## Median : 7.500 Median :0 Median :0.0000 ## Mean : 8.156 Mean :0 Mean :0.2812 ## 3rd Qu.: 9.000 3rd Qu.:0 3rd Qu.:0.2500 ## Max. :24.000 Max. :0 Max. :2.0000 1.7.1 Data types Data in R is generally stored in vectors, which are fixed-length collections of values of a particular data type. Common data types are logical: values which can either be TRUE or FALSE numeric: numbers of all kinds, such as 1, 356, and 34.5782 character: characters and strings, such as q and Hello You can combine values of a data type in a vector by using the c() function (which stands for “combine”). For instance c(TRUE, FALSE, TRUE, TRUE) ## [1] TRUE FALSE TRUE TRUE c(3,4,802.376) ## [1] 3.000 4.000 802.376 c(&quot;Coffee&quot;,&quot;now&quot;,&quot;please&quot;) ## [1] &quot;Coffee&quot; &quot;now&quot; &quot;please&quot; If you combine elements of different data types, then R will convert them to the most “general” type necessary. Combining a logical value with a numeric one, for instance, will convert logical value TRUE to 1, and FALSE to 0. Combining a character element with other elements, will convert everything to character elements: c(TRUE, FALSE, 12) ## [1] 1 0 12 c(TRUE, 5.67788, &quot;let&#39;s see what happens&quot;) ## [1] &quot;TRUE&quot; &quot;5.67788&quot; &quot;let&#39;s see what happens&quot; 1.7.2 Objects Objects are named things that are stored in memory and available to functions etc. Objects can be vectors, such as discussed above, but also more general types, such as matrices, factors, and data frames. If you want to create an object, you use the assignment operator &lt;-, with on the left side the name you want to give to the object, and on the right side the content of the object. For instance, we can store a numeric vector as the object my_vector as follows: my_vector &lt;- c(1,2,10:20) Note a little trick above, where 10:20 stands for a sequence of integers, i.e. \\(10, 11, 12, \\ldots, 20\\). my_vector is now an object in R memory (you should see it show up in the Environment panel), and can be called by name, as in: my_vector ## [1] 1 2 10 11 12 13 14 15 16 17 18 19 20 A matrix is a collection of vectors of the same length, joined as columns or rows. mat &lt;- matrix(1:10,ncol=2) mat # matrices are filled column-wise ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 mat[,2] # select the second column (the result is a vector) ## [1] 6 7 8 9 10 mat[3,1] # select the value in the third row and first column ## [1] 3 A factor is useful for nominal and ordinal data. A factor is a vector with integers, where each integer is provided with a unique label. For instance # construct a factor by giving integer values and specifying the accompanying # labels fact &lt;- factor(c(1,2,2,3),labels=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)) fact # display it ## [1] red green green blue ## Levels: red green blue fact == &quot;green&quot; # determine which elements equal (==) &#39;green&#39; ## [1] FALSE TRUE TRUE FALSE A list is a collection of different R objects. This is a very general type of object, and the elements of a list can even be lists themselves. A list allows you to keep different types of information together, but you probably won’t need to use it much for the content discussed here. But let’s quickly look at some aspects of a list: lst &lt;- list(a=mat, b=fact) # construct a named list with a matrix and factor lst ## $a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 ## ## $b ## [1] red green green blue ## Levels: red green blue If a list is named, meaning the elements have names, like above, you can select elements from the list by using a dollar sign and then the name of the elements: lst$a ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 lst$b ## [1] red green green blue ## Levels: red green blue You can also select elements by an index number, which should go in between double square brackets. For instance, if you want to select the first element, you can type lst[[1]] A data.frame is probably one of the most useful features of R for data analysis. A data.frame is like a matrix, in that it is a rectangular collection of data, but the columns are variables which can be of a different type (e.g., numeric, factors, or characters). You can construct data frames through the data.frame function, for instance as my_data_frame &lt;- data.frame(var1 = 1:10, var2 = 10:1, var3 = rep(c(&quot;a&quot;,&quot;b&quot;),times=5)) my_data_frame ## var1 var2 var3 ## 1 1 10 a ## 2 2 9 b ## 3 3 8 a ## 4 4 7 b ## 5 5 6 a ## 6 6 5 b ## 7 7 4 a ## 8 8 3 b ## 9 9 2 a ## 10 10 1 b Constructing a data frame looks similar to constructing a list, but all the (named) arguments should have the same length. If you for instance try data.frame(var1 = 1:10, var2 = 1:11) you will get an error. You won’t always get an error. If the length of longer elements are multiples of the length of shorter elements, R will fill in the missing values by repeating the shorter elements until they are of the same length as the longer elements. For instance data.frame(var1 = 1:10, var2 = 1:5, var3 = 1) ## var1 var2 var3 ## 1 1 1 1 ## 2 2 2 1 ## 3 3 3 1 ## 4 4 4 1 ## 5 5 5 1 ## 6 6 1 1 ## 7 7 2 1 ## 8 8 3 1 ## 9 9 4 1 ## 10 10 5 1 This can be handy, but also risky, as sometimes you might not realise that R is filling in values for you, and your analyses might give rather unexpected results. I would therefore always ensure that you create a data frame with elements of the same length. Most of the time, you won’t create data frames yourself within R, but you will load in external data as a data frame. 1.7.3 Importing data R can load data in many formats. Personally, I mainly use data stored in “comma separated value” (CSV) format. This is one of the most portable ways of storing data, so that it can be used in a variety of programs like R, SPSS, JASP, Excel, etc. Data in a comma-separated value (CSV) format can be read through the read.csv function. A nice thing about R is that it can read data directly from the World Wide Web. So you don’t need to first download data, and then open it from within R. [TODO: example] At some point, you will probably also come across data stored in Excel or SPSS format. regarding Excel, it is safest to first save a spreadsheet as a CSV file, and then load this file into R. Alternatively, the xlsx package provides the function read.xlsx to directly read a spreadsheet into R. To load data in SPSS format, the package foreign package provides the read.spss function. 1.8 Exploring data: Descriptive statistics Measures of location and spread can be computed through specialized functions, namely mean, median, IQR (inter-quartile range), var (variance), and sd (standard deviation). E.g. mean(fifa2010teams$goals_for) ## [1] 4.53125 median(fifa2010teams$goals_for) ## [1] 3 will give you the mean and median of variable goals_for in data.frame fifa2010teams. You can obtain the inter-quartile range as IQR(fifa2010teams$goals_for,type=1) ## [1] 3 Note the use of the type argument here. There are many ways in which to compute and estimate percentiles and quantiles. Using type=1 gives you the same result as the way I explained how to compute the IQR in the book. By default, R will use type = 7, which gives different results (type ?quantile for more information). The var and sd functions from base R do not actually provide the sample variance_ and sample standard deviation. Rather, they give unbiased estimates of the “true” (population) variance and standard deviation. To compute the variance and standard deviation of the sample data, you can use the sample_var and sample_sd functions in the sdamr package: sample_var(fifa2010teams$goals_for) ## [1] 13.24902 sample_sd(fifa2010teams$goals_for) ## [1] 3.639921 There is no function in base R to compute the mode2, but the sdamr package provides the function sample_mode to do just that: sample_mode(fifa2010teams$goals_for) ## [1] 3 1.9 Exploring data: Creating plots There are two common ways to plot data with R. Base R has various plotting functions, such as plot, hist, boxplot, which are useful for quick plots to explore your data. The resulting plots are not always the most aesthetically pleasing. The R package ggplot2 provides means to create a wide range of useful and beautiful plots. It is based on the idea of a “grammar of graphics,” which makes it extremely flexible, but also a little difficult to get your head around. In the following, I will show you how to use both base R and ggplot2. 1.9.1 Histogram R has many built-in plotting functions. These tend to be a little basic, and much prettier plots can be made with packages such as ggplot2 (Wickham et al. 2020, my current favourite!). But for quick data exploration, the built-in plotting functions are faster. A histogram is plotted through the hist function. In the following example, I first generate some random data, store it in an object called dat and then plot a histogram: hist(fifa2010teams$goals_for) There are many parameters you can change. In the following, I give the plot a new title and x-axis labels, as well as request the number of bins to be 20: hist(fifa2010teams$goals_for,main=&quot;Histogram of points scored by teams in the FIFA 2010 World Cup&quot;, xlab=&quot;Goals for&quot;, breaks=20) To create a nicer looking plot, you can use ggplot2. library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram() Well, that’s not actually so pretty. We can make it better by changing some of the defaults: library(ggplot2) ggplot(fifa2010teams,aes(x=goals_for)) + geom_histogram(bins=10, colour=&quot;black&quot;, fill=&#39;#8C8279&#39;) + xlab(&quot;Goals scored&quot;) Note that within the geom_histogram function, I’ve specified to use 10 bins, and draw a black line around the bars, and fill the bars with colour specified by the hexadecimal colour code ‘#8C8279.’ Finally,, I’m using the xlab function to generate a better label for the x-axis. ggplot2 is very powerful and flexible, so there are many such adjustments you can make. 1.9.2 Boxplot For a quick boxplot, you can use the base R function with the same name: boxplot(fifa2010teams$goals_for) ggplot2 also provides a boxplot through the geom_boxplot function. Note that in the aes specification, I’m now using goals_for as the y-axis. ggplot(fifa2010teams,aes(y=goals_for)) + geom_boxplot() Not very pretty! A somewhat better version can be obtained by: ggplot(fifa2010teams,aes(x=&quot;&quot;,y=goals_for)) + geom_boxplot(width=.2) + xlab(&quot;&quot;) 1.10 Scatterplot plot(x=fifa2010teams$matches_played, y=fifa2010teams$goals_for) ggplot(fifa2010teams, aes(x=matches_played, y=goals_for)) + geom_point() 1.11 Raincloud plot A basic (but reasonably flexible) function to create a raincloud plot is provided in the sdamr package through the plot_raincloud function. The data argument expects a data frame, and the y argument expects the name of the variable for which you want to create the plot. Note that as the jitter applied to the plot is random, to get exactly the same plot again, you need to set the random number seed through set.seed before. This is only necessary if you want to recreate a plot exactly. We’ll talk more about random number generation later. set.seed(467) plot_raincloud(data=fifa2010teams, y=goals_for) This section contains material adapted from https://ourcodingclub.github.io/tutorials/intro-to-r/↩︎ There is a function mode, but this does something rather different!↩︎ "],["statistical-modelling.html", "Chapter 2 Statistical modelling 2.1 Distributions 2.2 Estimation 2.3 Hypothesis testing directly with the binomial distribution", " Chapter 2 Statistical modelling As R was written by statisticians for statisticians, it naturally has very good support for statistical modelling. 2.1 Distributions R includes functions to calculate probabilities and generate random data from a wide variety of distributions (type ?distributions for an overview). The main distributions that we will use are: Distribution R name Parameters Binomial binom size (\\(n\\)), prob (\\(\\theta\\)) Chi-squared chisq df F f df1, df2 Normal norm mean (\\(\\mu\\)), sd (\\(\\sigma\\)) Student’s t t df The statistical distribution functions all have a similar interface: you can generate random data by calling the R name with an r (for random) before it (e.g. rbinom for random draws from the Binomial distribution, and rnorm for random draws from the Normal distribution) you can compute the probability (or density function value) of a particular value by calling the function with a d (for density) before it (e.g. dbinom and dnorm) you can compute a cumulative probability (the probability of a particular value and anything lower than it) by calling the function with a p (for probability) before it (e.g. pbinom) you can compute a quantile (the value such that there is a particular probability of sampling a value equal to or lower that it) by calling the function with a q (for probability) before it (e.g. qbinom) 2.1.1 Generating random data let’s use a coin tossing model to generate lots of replications of an experiment in which Paul is asked to provide 8 predictions. The dependent variable in each experiment is \\(Y_i = k\\), where \\(k\\) is the number of correct predictions (out of \\(n=8\\)). To generate 100 replications of such an experiment (e.g. 100 “alternate universes” where Paul might have made different predictions), we can use the rbinom function: set.seed(4638) Y &lt;- rbinom(n=100, size=8, prob=0.5) Y ## [1] 5 4 5 2 5 2 4 5 3 4 3 5 4 5 3 4 6 3 2 3 0 3 5 5 1 2 5 4 6 3 5 4 4 1 3 5 3 ## [38] 5 6 8 5 5 3 2 3 3 3 5 4 5 4 2 2 4 5 3 5 4 4 5 3 5 5 5 2 3 4 8 1 6 4 3 4 4 ## [75] 3 6 3 4 5 4 1 3 1 5 3 5 4 5 5 4 5 3 3 6 4 4 3 5 1 6 You can see quite some variability in the outcomes, perhaps easier in a histogram hist(Y, breaks=8) Computers can’t actually generate real random data. Computers are deterministic, performing computations according to instructions. But clever algorithms have been designed that produce sequences of numbers that are (almost) indistinguishable from purely random sequences. These algorithms are called random number generators and the default in R is the so-called “Mersenne-Twister” algorithm. You don’t have to worry about the details of such algorithms. We can just pretend that they produce truly random numbers. One thing I do want to point out is that before you use one of such algorithms, they need to be initialized with a special number, called the random seed. A number like this basically sets the algorithm in motion. Starting it with a different seed will produce a different sequence of random numbers. If you start it with the same random seed, you get exactly the same sequence of random numbers. If you don’t supply a value, R will use the current time as the random seed the first time you ask it to produce a random number in a session. For the purposes of replicating things, it may be useful to set the seed explicitly (so that a subsequent call to a random number generator will produce exactly the same results.) Therefore, you will now and then see calls such as set.seed(4638) in these notes, which ensures that these notes have the same content every time I run R. 2.2 Estimation 2.2.1 The likelihood function If we take a particular result, e.g. \\(Y=8\\) correct, it is straightforward to compute the probability of that result for different values of \\(\\theta\\). Earlier, we already used 1:10 to obtain a sequence of integers from 1 to 10. Not we ae going to use a more general function seq, to obtain a sequence of possible values for \\(\\theta\\) between 0 and 1. We will then supply this sequence as the prob argument to calculate the probability of the result according to each. possible_theta &lt;- seq(0,1,length=100) lik_theta &lt;- dbinom(8,size=8,possible_theta) plot(x=possible_theta,y=lik_theta,type=&quot;l&quot;) Instead of the first argument in dbinom being 8, you can try this for different values. The maximum likelihood estimate \\(\\hat{\\theta} = \\frac{k}{n}\\) is very simple to compute. Moreover, computations with R are vectorized. For instance, if Y is a vector, then Y/8 divides each element of Y by 8. And if Y and X are vectors of the same length, then Y/X divides each element of Y by the corresponding element of X. So, to compute the maximum likelihood estimate of \\(\\theta\\) for each of the simulated replications of the experiment, we can simply run: theta_est &lt;- Y/8 which stores the estimates in the object theta_est. 2.2.2 Calculating the likelihood ratio Having computed the estimate \\(\\hat{\\theta}\\) for each simulated dataset, we can also straightforwardly compute the likelihood ratio for each data set, comparing e.g. our MODEL R where we assume \\(\\theta = 0.5\\) to MODEL G where we use \\(\\theta = \\hat{\\theta}\\) instead. First, let’s caclulate the likelihood of each simulated dataset according to MODEL R. Again, we will make use of the fact that many functions in R are vectorized, such that if we ask R to calculate the probability for a vector of outcomes Y, the result is a vector with the probabilities for each element in Y likelihood_R &lt;- dbinom(Y, size=8, prob=.5) Remember, if you want to see what is in an R object, you can just type in the name: likelihood_R ## [1] 0.21875000 0.27343750 0.21875000 0.10937500 0.21875000 0.10937500 ## [7] 0.27343750 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 ## [13] 0.27343750 0.21875000 0.21875000 0.27343750 0.10937500 0.21875000 ## [19] 0.10937500 0.21875000 0.00390625 0.21875000 0.21875000 0.21875000 ## [25] 0.03125000 0.10937500 0.21875000 0.27343750 0.10937500 0.21875000 ## [31] 0.21875000 0.27343750 0.27343750 0.03125000 0.21875000 0.21875000 ## [37] 0.21875000 0.21875000 0.10937500 0.00390625 0.21875000 0.21875000 ## [43] 0.21875000 0.10937500 0.21875000 0.21875000 0.21875000 0.21875000 ## [49] 0.27343750 0.21875000 0.27343750 0.10937500 0.10937500 0.27343750 ## [55] 0.21875000 0.21875000 0.21875000 0.27343750 0.27343750 0.21875000 ## [61] 0.21875000 0.21875000 0.21875000 0.21875000 0.10937500 0.21875000 ## [67] 0.27343750 0.00390625 0.03125000 0.10937500 0.27343750 0.21875000 ## [73] 0.27343750 0.27343750 0.21875000 0.10937500 0.21875000 0.27343750 ## [79] 0.21875000 0.27343750 0.03125000 0.21875000 0.03125000 0.21875000 ## [85] 0.21875000 0.21875000 0.27343750 0.21875000 0.21875000 0.27343750 ## [91] 0.21875000 0.21875000 0.21875000 0.10937500 0.27343750 0.27343750 ## [97] 0.21875000 0.21875000 0.03125000 0.10937500 We can follow the same procedure to calculate the likelihood for each simulated dataset according to the estimated MODEL G. In this case though, we are going to use two vectorized arguments simultaneously. For each simulated dataset, we have an observed number of correct predictions (in our object Y), as well as a corresponding estimate of the probability of a correct response (in our object theta_est). If we supply each as arguments to the dbinom function, R will match each element in Y with the corresponding element of theta_est, i.e. Y[1] is matched with theta_est[1], y[2] with theta_est[2], etc. You will have to be quite certain that the indices (i.e., the number between the brackets, such as [1] and [2]) of one argument correspond to the indices of another to get the correct results. In this case, we computed theta_est directly from Y, and therefore we know each value in Y corresponds to the value of theta_est = y/8. Sowe can safely compute the likelihood of MODEL G with twp vectorised arguments as: likelihood_G &lt;- dbinom(Y, size=8, prob = theta_est) To compute the likelihood ratio for all these simulated datasets, we can now simply divide each element in likelihood_R by the corresponding element in likelihood_G: likelihood_ratio &lt;- likelihood_R/likelihood_G Great! We now have a distribution of likelihood ratio values, simulated from MODEL R. We can get closer and closer to the sampling distribution of the likelihood ratio values by increasing the number of simulated datasets. Here, I’m only simulating 100 datasets, but you can easily simulate many more. Let’s have a look at the distribution of likelihood ratio values with a histogram: hist(likelihood_ratio) Compared to an infinite number of datasets, 100 is quite a small number, but nevertheless the histogram already looks quite a bit like the true distribution of the likelihood ratio under MODEL R. 2.3 Hypothesis testing directly with the binomial distribution To perform hypothesis tests directly with the binomial distribution, you can use the binom.test function. This function has 5 arguments: x: the number of “successes” (e.g., the number of correct guesses). n: the number of trials (e.g. the total number of guesses) p: the probability of success assumed under the null hypothesis (e.g. \\(\\underline{\\theta}\\)). The default value is p = 0.5. alternative: the range of values for p (i.e. \\(\\theta\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\theta &gt; \\underline{\\theta}\\) allowed), or less (only values \\(\\theta &lt; \\underline{\\theta}\\) allowed). The default value is alternative = \"two.sided\". conf.level: the confidence level for the returned confidence interval. Should be specified as a probability (i.e., to get the 95% confidence interval, you should specify conf.level = 0.95). The default value is conf.level = 0.95. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(0 \\leq \\theta \\leq 1\\) corresponds to a two-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta \\neq 0.5\\). This test can be performed as: binom.test(x=8, n = 8, p = 0.5) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.007812 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.6305834 1.0000000 ## sample estimates: ## probability of success ## 1 The output provides the estimated \\(\\theta\\) for MODEL G under sample estimates: probability of success. Remember that the p-value of the test reflects the probability of the provided number of successes or more extreme ones under the null hypothesis. Comparing MODEL R with \\(\\theta = 0.5\\) against MODEL G with \\(\\theta &gt; 0.5\\) corresponds to a one-sided test of the null hypothesis \\(H_0\\): \\(\\theta = 0.5\\) against the alternative hypothesis that \\(\\theta &gt; 0.5\\). This test can be performed by binom.test(x=8, n = 8, p = 0.5, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: 8 and 8 ## number of successes = 8, number of trials = 8, p-value = 0.003906 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.687656 1.000000 ## sample estimates: ## probability of success ## 1 "],["the-one-sample-t-test.html", "Chapter 3 The one-sample t-test 3.1 Missing values 3.2 Selecting subsets of data 3.3 One-sample t-test", " Chapter 3 The one-sample t-test You can open the anchoring data as follows: library(sdamr) data(anchoring) head(anchoring) ## session_id sex age citizenship referrer us_or_international lab_or_online ## 1 2400853 f 18 US abington US In-lab ## 2 2400856 f 19 CN abington US In-lab ## 3 2400860 f 18 US abington US In-lab ## 4 2400868 m 18 US abington US In-lab ## 5 2400914 f 18 US abington US In-lab ## 6 2400916 f 18 US abington US In-lab ## anchor everest_feet everest_meters ## 1 high 30000 NA ## 2 high 30000 NA ## 3 low 5000 NA ## 4 low 2400 NA ## 5 low 10000 NA ## 6 high 25000 NA 3.1 Missing values Sometimes a data set has missing values. In R, a missing value is shown as NA (for Not Available). For example, you can see missing in the everest_meters variable. If a variable has missing values, functions such as mean and sd will return a missing value, rather than a numeric value. For instance: mean(anchoring$everest_meters) ## [1] NA sd(anchoring$everest_meters) ## [1] NA You could get rid of all the missing values from your data.frame by using a subset function, but generally, I think it is better to keep the dataset as is, and use other means to avoid problems with missing values. Luckily, many functions have arguments to deal with missing values. For instance, the two functions above have an na.rm argument (for not-available-remove). Setting this to true will call the function only on the non-missing-values: mean(anchoring$everest_meters,na.rm=TRUE) ## [1] 6636.786 sd(anchoring$everest_meters,na.rm=TRUE) ## [1] 3942.714 3.2 Selecting subsets of data There are two main ways to select a subset of observations in R. You can either use an “indexing variable,” or use the subset function. ### Indexing variables An indexing variable is used to specify the rows in a data.frame that you want to use. Generally, an indexing variable is a logical variable, which takes the value TRUE for cases (rows) that you want to include, and FALSE for cases that you want to exclude. Using an index variable, we will treat the data.frame as a matrix, which allows us to use square brackets, as in data[row,column] to either select rows or columns. For example, anchoring[,\"age\"] selects the column named “age” and returns it, while anchoring[1:10,] selects rows 1 to 10. The nice thing about R is that instead of providing row row numbers, we can create a logical variable based on the data itself to select rows. To do so, we can use the logical comparators and operators: == “equal to” != “not equal to” &gt; “greater than” &gt;= “greater than or equal to” &lt; “smaller than” &lt;= “smaller than or equal to” &amp; “and” | “or” Some examples of using index variables are as follows. An index variable which is TRUE for males and FALSE for females can be computed as follows: index &lt;- anchoring$sex == &quot;m&quot; Let’s see what this variable looks like: head(index) ## [1] FALSE FALSE FALSE TRUE FALSE FALSE It is indeed a logical variable which is TRUE whenever sex is equal to \"m\", and FALSE otherwise. You can use it to select all the males in the anchoring data by: dat &lt;- anchoring[index,] Note that you don’t have to create an index variable separately. You can obtain the same result by computing the index variable within the brackets, like so: dat &lt;- anchoring[anchoring$sex == &quot;m&quot;,] You can select all males over 30 years of age, and check the number of observations in this subset by the nrow function, as follows: dat &lt;- anchoring[anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;,] nrow(dat) ## [1] 361 You can select all participants who are male and over 30 years of age, or females who are female and over 30 years of age by: dat &lt;- anchoring[(anchoring$age &gt; 30 &amp; anchoring$sex == &quot;m&quot;) | (anchoring$age &gt; 30 &amp; anchoring$sex == &quot;f&quot;),] 3.2.1 The subset function The subset function is quite similar to using index variables, but it doesn’t require the treatment of the data.frame as a matrix and it looks for variable names in the data.frame so you don’t have to use e.g. anchoring$ before the variable name. This makes the subset function a bit easier to use than using indexing variables. The subset function has the following arguments: * x: the object (e.g. the data.frame) for which you want to select a subset of cases * subset: a logical expression indicating elements or rows to keep * select: an optional expression which indicates which columns to select from a data frame I generally use just the first two arguments. We can replicate the selections above using the subset function as follows: dat &lt;- subset(anchoring, sex == &quot;m&quot;) dat &lt;- subset(anchoring, age &gt; 30 &amp; sex == &quot;m&quot;) dat &lt;- subset(anchoring, (age &gt; 30 &amp; sex == &quot;m&quot;) | (age &gt; 30 &amp; sex == &quot;f&quot;)) For more information on indexing and subsetting, have a look at e.g. http://www.cookbook-r.com/Basics/Getting_a_subset_of_a_data_structure/ The data analysed in the SDAM book was selected as follows: dat &lt;- subset(anchoring,(referrer == &quot;swps&quot; | referrer == &quot;swpson&quot;) &amp; anchor == &quot;low&quot;) 3.3 One-sample t-test R has a t.test function which allows you to compute a variety of t-tests. For a one-sample t-test, you would use the following arguments * x: the variable for which to compute the t-test * mu: the assumed value of the mean, i.e. \\(\\underline{\\mu}\\) * alternative: similar as in binom.test, the range of values for mu (i.e. \\(\\mu\\)) considered in MODEL G. This must be either two.sided (all values allowed), greater (only values \\(\\mu &gt; \\underline{\\mu}\\) allowed), or less (only values \\(\\mu &lt; \\underline{\\mu}\\) allowed). The default value is alternative = \"two.sided\". For instance, we can run the two-sided t-test also reported in the SDAM book by t.test(dat$everest_meters, mu=8848) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 1.558e-13 ## alternative hypothesis: true mean is not equal to 8848 ## 95 percent confidence interval: ## 5716.848 6907.537 ## sample estimates: ## mean of x ## 6312.193 A one-sided test where MODEL R assumes \\(\\mu = 8848\\) and MODEL G assumes that \\(\\mu &lt; 8848\\), is obtained by t.test(dat$everest_meters, mu=8848, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 7.791e-14 ## alternative hypothesis: true mean is less than 8848 ## 95 percent confidence interval: ## -Inf 6810.498 ## sample estimates: ## mean of x ## 6312.193 "],["regression.html", "Chapter 4 Regression 4.1 Estimating an testing a simple regression model 4.2 Model comparisons 4.3 Estimating and testing a multiple regression model 4.4 Residuals and predicted values 4.5 Plotting pairwise scatterplots for many variables", " Chapter 4 Regression 4.1 Estimating an testing a simple regression model Regression analysis is done through the lm function, with the following syntax: lm(formula, data,...). The first argument is called formula and expects a symbolic description of your model. I will tell you more about how to specify models with the formula syntax later, when we discuss moderation. For now, a few simple examples will suffice. To specify a simple regression model where you predict a dependent variable y by a predictor x, you would use the formula y ~ x On the left-hand side of the formula you need to provide the name of the dependent variable. After the name of the dependent variable, you need to put a tilde (~) (which you can read is “is modelled as a function of”). On the right-hand side, you then provide the name of the predictor. R will automatically include an intercept in the model. In R, the intercept is actually represented as a special predictor which always (for every row in the data set) has the value 1. The formula above is actually interpreted as y ~ x + 1 Because the intercept is included in most models, the authors of R have decided to save the you trouble of typing + 1 in each formula, by making this part of the formula implicit. You can fit a model without an intercept (which is the same as fixing the value of the intercept to 0), by instead of + 1, putting - 1 in the formula, as in y ~ x - 1 The second argument of the lm function is called data and expects the name of the data.frame in which the variables are stored. To see how the lm function works in practice, let’s open the trump2016 data provided in the sdamr package. This is the data analysed in Chapter 4 and 5. We open and inspect the data as usual: library(sdamr) # load the Trump data data(&quot;trump2016&quot;) # remove the data from the District of Columbia (Washintgon D.C.) dat &lt;- subset(trump2016,state != &quot;District of Columbia&quot;) head(dat) ## state hate_groups population hate_groups_per_million ## 1 Alabama 27 4863300 5.55 ## 2 Alaska 0 741894 0.00 ## 3 Arizona 18 6931071 2.60 ## 4 Arkansas 16 2988248 5.35 ## 5 California 79 39250017 2.01 ## 6 Colorado 16 5540545 2.89 ## percent_bachelors_degree_or_higher percent_in_poverty percent_Trump_votes ## 1 15.4 18.5 62.9 ## 2 29.7 10.4 52.9 ## 3 27.7 17.4 49.5 ## 4 21.8 18.7 60.4 ## 5 32.3 15.4 32.7 ## 6 39.2 11.5 44.4 You can see that there are a number of variables in the dataset (and not all of these were analysed in the book). For more information on the variables in the dataset, you can call the help file with ?trump2016. Now let’s estimate a simple regression model to predict the percentage of votes for Trump by the number of hate groups per million citizens: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million, data=dat) modg ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Coefficients: ## (Intercept) hate_groups_per_million ## 42.9 2.3 I’ve named the resulting object modg for MODEL G. You can pick any name you like for R objects. Note that when you just print a fitted linear model (by e.g., typing the name of the object modg), R will show the parameter estimates, but nothing else. You can get the important statistics by calling the summary function on the fitted model: summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.8206 -5.8570 -0.0529 5.2632 20.7883 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.8968 2.4094 17.804 &lt; 2e-16 *** ## hate_groups_per_million 2.3004 0.6715 3.426 0.00127 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.142 on 48 degrees of freedom ## Multiple R-squared: 0.1965, Adjusted R-squared: 0.1797 ## F-statistic: 11.74 on 1 and 48 DF, p-value: 0.001265 This provides quite a lot of useful information. The output of the summary function consists of four parts: Call simply shows you the call to the lm function used to fit the model (including the model formula) Residuals shows you some summary statistics for the prediction errors of the estimated model (which are often referred to as residuals) Coefficients shows you a table with: the name of variable for which the parameter was estimated Estimate: the estimated parameters Std. Error: the standard error of the estimates (this is the standard deviation of the sampling distribution of the estimates) t value: the t statistic of the hypothesis test that the true value of the parameter is equal to 0. Pr(&gt;|t|): the p-value, which is the probability that, given that the null hypothesis is true (i.e. the true value of the parameter is equal to 0), you would find a t-statistic at least as extreme as the one computed found for this data. Some overall model statistics: Residual standard error: this is and unbiased estimate of the standard deviation of the errors. Multiple R-squared: the \\(R^2\\) or proportion of variance of the dependent variable “explained” by the model. Adjusted R-squared: an unbiased estimate of the true value of \\(R^2\\) F-statistic: the results of a model comparison comparing the estimated model (MODEL G) to a MODEL R which only includes an intercept. 4.2 Model comparisons Comparing regression models and computing the \\(F\\) statistic can be done through the anova() function. Let’s first estimate a restricted version of MODEL G above where we fix the slope of hate_groups_per_million to 0. This MODEL R is identical to a model with only an intercept. We can estimate this by not providing any predictor names, but now explicitly providing the intercept term 1. # fit a MODEL R with only an intercept modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) We can then compute the \\(F\\) test by entering this MODEL R, and the MODEL G we estimated earlier, as arguments in the anova function: anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 48 4011.4 1 980.78 11.736 0.001265 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 the output lists the formula’s of the models we compare, and then provides a table with test results. The columns in this table are * Res.Df: the denominator degrees of freedom, i.e. \\(n=-\\text{npar}(M)\\) * RSS: the “residual sum of squares” or Sum of Squared Error of the model, i.e. \\(\\text{SSE}(M)\\) * Df: the numerator degrees of freedom,,i.e. $(G) - (R) * Sum of Sq: the reduction in the Sum of Squared Error, i.e. \\(\\text{SSE}(R) - \\text{SSE}(R)\\) * F: the \\(F\\) statistic of the test * Pr(&gt;F): the p-value of the test. We can obtain a test for the intercept by fitting a different MODEL R, now without an intercept, and comparing it to MODEL G # fit a MODEL R without an intercept (through &quot; - 1&quot;) modr &lt;- lm(percent_Trump_votes ~ hate_groups_per_million - 1, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ hate_groups_per_million - 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 30501.7 ## 2 48 4011.4 1 26490 316.98 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output of the anova function isn’t particularly pretty. Also, if you want to do multiple model comparisons, first estimating models and then comparing them with the anova function becomes a little cumbersome. An easier way to obtain all the model comparisons is to use the Anova function from the car (Fox, Weisberg, and Price 2020) package to automatically construct different possible versions of MODEL R, each being one particular restriction of MODEL G which fixes the relevant parameter to 0. If you don’t have the car package installed yet, you need to install it first (e.g. by install.packages(\"car\"). You can then call: library(car) Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 26490.3 1 316.981 &lt; 2.2e-16 *** ## hate_groups_per_million 980.8 1 11.736 0.001265 ** ## Residuals 4011.4 48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that it is important to give the type = 3 argument in the Anova function. This will construct MODEL R by fixing a single parameter to 0 in turn (i.e. first fixing \\(\\beta_0=0\\) and estimating all other parameters, then another model fixing \\(\\beta_1 = 0\\) for estimating all other parameters), etc. 4.3 Estimating and testing a multiple regression model To specify a multiple regression model for a dependent variable named y and with three predictors, named x1, x2, and x3, you would use the formula y ~ x1 + x2 + x3 This is similar to the earlier formula, but you now need to provide the names of all the predictors, separated by a + sign. For instance, we can fit a model with two predictors (which we will call modg, for MODEL G), as follows: modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) summary(modg) ## ## Call: ## lm(formula = percent_Trump_votes ~ hate_groups_per_million + ## percent_bachelors_degree_or_higher, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.7268 -4.0621 0.0426 2.8937 15.8359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 81.9945 6.1555 13.321 &lt; 2e-16 *** ## hate_groups_per_million 1.3138 0.5102 2.575 0.0132 * ## percent_bachelors_degree_or_higher -1.2187 0.1839 -6.625 3.03e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.643 on 47 degrees of freedom ## Multiple R-squared: 0.5845, Adjusted R-squared: 0.5668 ## F-statistic: 33.06 on 2 and 47 DF, p-value: 1.087e-09 the output of the summary function contains the same elements as before, but the table of coefficients now includes an additional row for percent_bachelors_degree_or_higher. Also, note that all the estimates are different, because the slopes reflect unique effects, and these differ compared to models with other predictors. Finally, I’d like to point out that the last row of the output contains the “whole model test,” which compares the estimated model to a model with only an intercept. Recall that the estimate of the intercept in this latter model equals the sample mean. So we are now comparing a model with two predictors to a model which predicts all values as the sample mean. The difference in the number of estimated parameters for this comparison is \\(\\text{npar}(G) - \\text{npar}(R) = 3 - 1 = 2\\). Hence, the degrees of freedom are \\(\\text{df}_1 = 2\\) and \\(\\text{df}_2 = n - \\text{npar}(G) = 50 - 3 = 47\\). We can also get all the model comparisons for this MODEL G through: Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: percent_Trump_votes ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 7830.7 1 177.4381 &lt; 2.2e-16 *** ## hate_groups_per_million 292.7 1 6.6318 0.01322 * ## percent_bachelors_degree_or_higher 1937.2 1 43.8956 3.028e-08 *** ## Residuals 2074.2 47 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Finally, we can also obtain a “whole model test,” by comparing an intercept-only MODEL R to the full MODEL G. This is best done through the anova function as follows: modr &lt;- lm(percent_Trump_votes ~ 1, data=dat) modg &lt;- lm(percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher, data=dat) anova(modr,modg) ## Analysis of Variance Table ## ## Model 1: percent_Trump_votes ~ 1 ## Model 2: percent_Trump_votes ~ hate_groups_per_million + percent_bachelors_degree_or_higher ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 4992.2 ## 2 47 2074.2 2 2918 33.06 1.087e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.4 Residuals and predicted values You can obtain the prediction errors by the calling the residuals function on the fitted model. # store the residuals as errorg errorg &lt;- residuals(modg) head(errorg) ## 1 2 3 4 5 6 ## -7.618118 7.101376 -2.152059 -2.055564 -12.570794 6.382163 This returns a vector with, for each case in the data (each row in the data frame), the error term \\(\\hat{\\epsilon}_i\\). Note that we are only displaying the first six elements through the head function. You can obtain the predicted values by calling the predict function on the fitted model. # store the predictions as predictg predictg &lt;- predict(modg) head(predictg) ## 1 2 3 4 5 6 ## 70.51812 45.79862 51.65206 62.45556 45.27079 38.01784 This returns a vector with for, each case in the data, the predicted value \\(\\hat{Y}_{M,i}\\). You can use these variables to create e.g. a histogram of the errors: hist(errorg) and a predicted by residual plot # scatterplot of predicted vs residual plot(predictg, errorg, xlab = &quot;predicted&quot;, ylab = &quot;residual&quot;) # add a horizontal line (h=0 is for horizontal at 0, # and lty = 3 makes it a dotted line abline(h=0,lty=3) You can also call the plot function directly on the fitted model, which produces a range of plots to assess the model assumptions: plot(modg) 4.5 Plotting pairwise scatterplots for many variables A final tip relates to exploring relations between many variables (e.g. potential predictors and dependent variables). While you can inspect pairwise relations between variables by creating a scatterplot for each pair of variables, this quickly becomes tedious. You can save yourself some work by using a function that produces a matrix of pairwise scatterplots directly. One option for this is to use the pairs function, and supply this with a selection of variables in a data.frame. For instance, in the data set we considered now, we might be interested in the relations between hate_groups_per_million, percent_bachelors_degree_or_higher, percent_in_poverty, and percent_Trump_votes. We can obtain a matrix of all pairwise scatterplots between these variables as follows (note that rather than typing the variable names, I’m selecting column 4 to 7, which correspond to these variables): pairs(dat[,4:7]) If you don’t like the look of these base R graphics and prefer ggplot2, you can use the ggpairs function from the GGally (Schloerke et al. 2021) package to get a similar plot: library(GGally) ggpairs(dat[,4:7]) "],["moderation-and-mediation.html", "Chapter 5 Moderation and mediation 5.1 Moderation in linear models 5.2 Centering 5.3 Mediation analysis", " Chapter 5 Moderation and mediation 5.1 Moderation in linear models Including an interaction in a linear model in R is straightforward. If you have two predictors, x1 and x2, and want to include both the “simple slopes” as well as the slope for the “product predictor” (i.e. x1 \\(\\times\\) x2), then the model with y as dependent variable can be specified in formula form as y ~ x1 * x2 which evaluates to y ~ 1 + x1 + x2 + x1:x2 As discussed previously, 1 represents the intercept, which is automatically included in a model specification, unless you remove it explicitly by adding -1 to the formula. x1 represents the “simple effect” of x1, x2 the corresponding “simple effect” of x2, whilst x1:x2 represents the product-predictor for the interaction between x1 and x2 (perhaps a little confusing for those of you who know : as a division operator). Because you would generally want to include the simple effects for the predictors as well as the interaction, the authors of R have chosen to save you typing in the full model by expanding x1 * x2 in this way. This can be used to specify more complicated models, with three-way interactions. For instance, y ~ x1 * x2 * x3 evaluates to y ~ 1 + x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3 which is a model with all “simple effects,” all pairwise product-predictors, as well as a three-way product predictor which is x1 \\(\\times\\) x2 \\(\\times\\) x3. We won’t discuss such higher-order interactions until later, but it is good to be aware of this in advance. As a linear model with a product-predictors is just another linear model, that is really all there is to say about specifying linear models with interactions/moderation in R. Through the formula interface, R will create the necessary additional product predictor(s), and then estimate the parameters of the resulting linear model. Let’s have a look at how this works with the speeddate data which was also analysed in the SDAM book. The data is included in the sdamr package, and can be loaded and (partially) inspected as usual: library(sdamr) data(&quot;speeddate&quot;) head(speeddate) ## iid pid gender age date_like other_like date_want other_want match self_attr ## 1 132 137 female 27 8 7 0 1 0 8 ## 2 132 138 female 27 8 8 0 1 0 8 ## 3 132 139 female 27 5 8 0 1 0 8 ## 4 132 140 female 27 9 7 1 1 1 8 ## 5 132 141 female 27 7 7 0 1 0 8 ## 6 133 137 female 24 7 7 0 0 0 6 ## self_sinc self_intel self_fun self_amb other_attr other_sinc other_intel ## 1 10 9 10 10 8 8 9 ## 2 10 9 10 10 8 7 4 ## 3 10 9 10 10 8 NA 8 ## 4 10 9 10 10 7 7 7 ## 5 10 9 10 10 8 7 7 ## 6 8 8 7 7 6 10 10 ## other_fun other_amb other_shar date_attr date_sinc date_intel date_fun ## 1 8 9 7 7 9 7 9 ## 2 6 6 4 8 8 8 8 ## 3 8 7 NA 5 7 9 5 ## 4 7 7 7 8 9 9 9 ## 5 7 7 8 5 8 8 8 ## 6 6 7 5 7 7 7 8 ## date_amb date_shar self_imp_attr self_imp_sinc self_imp_intel self_imp_fun ## 1 6 9 16.67 16.67 16.67 16.67 ## 2 8 8 16.67 16.67 16.67 16.67 ## 3 9 5 16.67 16.67 16.67 16.67 ## 4 9 8 16.67 16.67 16.67 16.67 ## 5 7 7 16.67 16.67 16.67 16.67 ## 6 6 8 12.77 19.15 17.02 17.02 ## self_imp_amb self_imp_shar other_imp_attr other_imp_sinc other_imp_intel ## 1 16.67 16.67 17.39 17.39 15.22 ## 2 16.67 16.67 20.00 20.00 20.00 ## 3 16.67 16.67 18.75 16.67 18.75 ## 4 16.67 16.67 18.60 16.28 18.60 ## 5 16.67 16.67 20.83 20.83 16.67 ## 6 14.89 19.15 17.39 17.39 15.22 ## other_imp_fun other_imp_amb other_imp_shar ## 1 17.39 13.04 19.57 ## 2 20.00 6.67 13.33 ## 3 20.83 12.50 12.50 ## 4 18.60 11.63 16.28 ## 5 16.67 6.25 18.75 ## 6 17.39 13.04 19.57 There are rather a large number of variables in the dataset. You can obtain more information about each variable in the documentation of the dataset, by calling ?speeddate. In my humble opinion, the (generally quite) good documentation of R packages is a real benefit of R over some other systems, and I strongly recommend you to check out and read the documentation of functions and datasets before you use them. Functions in R are generally quite flexible and it is infeasible to discuss all the nuances and possibilities in introductory notes like these. In the book, we mainly focused on the variables starting with other_, which are the perceptions of the participant by their dating partner. For example, the model \\[\\begin{align} \\texttt{like}_i =&amp; \\beta_0 + \\beta_{\\texttt{attr}} \\times \\texttt{attr}_i + \\beta_{\\texttt{intel}} \\times \\texttt{intel}_i + \\beta_{\\texttt{fun}} \\times \\texttt{fun}_i \\\\ &amp;+ \\beta_{\\texttt{attr} \\times \\texttt{intel}} \\times (\\texttt{attr} \\times \\texttt{intel})_i + \\beta_{\\texttt{fun} \\times \\texttt{intel}} \\times (\\texttt{fun} \\times \\texttt{intel})_i + \\epsilon_i \\end{align}\\] referred to in the SDAM book can be estimated by calling: modg &lt;- lm(other_like ~ other_attr*other_intel + other_fun*other_intel, data=speeddate) Hypothesis tests with the \\(t\\) statistic are obtained as usual though summary(modg) ## ## Call: ## lm(formula = other_like ~ other_attr * other_intel + other_fun * ## other_intel, data = speeddate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2380 -0.6632 0.0239 0.6583 4.6484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.91118 0.44269 -2.058 0.039740 * ## other_attr 0.67123 0.09027 7.436 1.76e-13 *** ## other_intel 0.32393 0.05994 5.405 7.57e-08 *** ## other_fun 0.14331 0.08734 1.641 0.101045 ## other_attr:other_intel -0.04333 0.01171 -3.700 0.000224 *** ## other_intel:other_fun 0.03186 0.01139 2.798 0.005209 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 1472 degrees of freedom ## (84 observations deleted due to missingness) ## Multiple R-squared: 0.6227, Adjusted R-squared: 0.6214 ## F-statistic: 485.8 on 5 and 1472 DF, p-value: &lt; 2.2e-16 The equivalent tests with the \\(F\\) statistic are easily obtained through car::Anova(modg,type=3) ## Anova Table (Type III tests) ## ## Response: other_like ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 5.43 1 4.2365 0.0397398 * ## other_attr 70.93 1 55.2931 1.755e-13 *** ## other_intel 37.47 1 29.2094 7.568e-08 *** ## other_fun 3.45 1 2.6923 0.1010448 ## other_attr:other_intel 17.56 1 13.6898 0.0002236 *** ## other_intel:other_fun 10.04 1 7.8287 0.0052093 ** ## Residuals 1888.24 1472 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.1.1 A bit about namespaces In the code above, I’m using car:: to refer to a function in the car package. Technically, a statement like package_name:: denotes the namespace of the package with the name package_name (i.e., car in this case). This allows you to use functions from a package without loading the package completely (without loading all the functions of the package in memory). This can be (and often is!) better than first loading the package through e.g. library(car) and then calling Anova. The issue is that different packages can use the same name for a function, and when you call a function, it will be the one of the package that was last loaded. When packages use the same name for functions, the function with the same name from a package that was loaded earlier will be “masked” and R will print this as a warning in the R console. For example, if you load the dplyr package (a rather useful package for data manipulation, that is a little too much to discuss here in detail), you will see the following warnings: library(dplyr) The line The following objects are masked from 'package:stats' indicates that the functions filter and lag are \"masked from the stats package. Whenever you call these functions, they will be the corresponding functions from the dplyr package, and not those from the stats package. This does not break the functionality of the packages themselves, as a properly written package that needs the filter function from the stats package will still use the function from that namespace (package), and not the one from the dplyr one. But when you call the a function, R will try find the definition of that function in the global namespace, and the global namespace contains the version of the last provided definition of any R object. Just like you can overwrite an R object by giving it the same name as an already existing r object (as I often do deliberately through e.g. mod &lt;- lm()), a function in R is just another object. So if I specify a function like head &lt;- function(x, na.rm = FALSE) { return(&quot;Where&#39;s your head? It&#39;s almost Halloween!&quot;) } Then next time I call that function, I will get as a result head(speeddate) ## [1] &quot;Where&#39;s your head? It&#39;s almost Halloween!&quot; and not the result from utils::head(speeddate) 5.2 Centering As discussed in the SDAM book, sometimes you might want to center variables by subtracting their (sample) mean from each value. This can be done in a number of ways. You can either create new variables in a dataframe by subtracting the single value obtained through mean() from a vector, as in speeddate$other_like_c &lt;- speeddate$other_like - mean(speeddate$other_like, na.rm = TRUE) and then using the new variable other_like_c in your lm model. Alternatively, you can do by calling the scale function. By default, the scale function creates \\(Z\\) transformed variables by subtracting the mean and then dividing this mean-deviation by the standard deviation: \\[\\text{scale}(Y_i) = \\frac{Y_i - \\overline{Y}}{S_Y}\\] The scale function has three arguments: x: the variable (or matrix of variables) which you want to scale center: a logical value indicating whether you want to subtract the mean scale: a logical value indicating whether you want to divide by the standard deviation Centering (subtracting the mean, but not dividing by the standard deviation) is thus obtained by calling scale(x, scale=FALSE). Personally, I find calling a function scale with argument scale = FALSE a little confusing. The sdamr package therefore provides the function center which is basically just a version of scale which by default sets the argument scale = FALSE: center &lt;- function(x) { scale(x, center = TRUE, scale = FALSE) } Because R is a functional programming language, you can call the scale or center function directly within the call to the lm function. This saves you having to create centered variables in a dataframe first. For instance, if you have loaded the sdamr package (or if you ran the code above defining the center function), you can obtain the results of the model with centered predictors by calling modg_c &lt;- lm(other_like ~ center(other_attr)*center(other_intel) + center(other_fun)*center(other_intel), data=speeddate) summary(modg_c) ## ## Call: ## lm(formula = other_like ~ center(other_attr) * center(other_intel) + ## center(other_fun) * center(other_intel), data = speeddate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2380 -0.6632 0.0239 0.6583 4.6484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.19613 0.03151 196.609 &lt; 2e-16 *** ## center(other_attr) 0.34539 0.01922 17.967 &lt; 2e-16 *** ## center(other_intel) 0.25791 0.02351 10.970 &lt; 2e-16 *** ## center(other_fun) 0.38292 0.02094 18.287 &lt; 2e-16 *** ## center(other_attr):center(other_intel) -0.04333 0.01171 -3.700 0.000224 *** ## center(other_intel):center(other_fun) 0.03186 0.01139 2.798 0.005209 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 1472 degrees of freedom ## (84 observations deleted due to missingness) ## Multiple R-squared: 0.6227, Adjusted R-squared: 0.6214 ## F-statistic: 485.8 on 5 and 1472 DF, p-value: &lt; 2.2e-16 5.3 Mediation analysis In the SDAM book, we looked at mediation with a different dataset called legacy2015. This is also part of the sdamr package, and we can make it available and look at the initial cases as usual: data(&quot;legacy2015&quot;) head(legacy2015) ## [1] &quot;Where&#39;s your head? It&#39;s almost Halloween!&quot; Oops! We had overwritten (“masked”) the head function before. We can get rid of our new definition of the head function by removining it from the global namespace with the rm() function (the name refers to remove): rm(&quot;head&quot;) After this, we can try again: head(legacy2015) ## id sex age legacy belief intention education income donation ## 2 2 male 23 4.38 5.4 3.57 5 5 0 ## 3 3 female 59 4.75 5.2 2.43 5 6 0 ## 4 4 male 30 5.00 5.2 2.43 5 3 1 ## 5 5 female 25 2.88 5.0 2.00 4 3 0 ## 6 6 female 32 4.88 4.4 2.57 2 1 0 ## 7 7 male 38 5.12 4.6 3.71 3 2 7 5.3.1 Causal steps The causal steps approach to assessing mediation is done through testing significance in three regression models. This can be done straightforwardly with the lm() function which we have used quite a bit already. For instance, to assess whether the relation between legacy and donation is mediated by intention, we would estimate and test the parameters of the following models: mod1 &lt;- lm(donation ~ legacy, data = legacy2015) mod2 &lt;- lm(intention ~ legacy, data = legacy2015) mod3 &lt;- lm(donation ~ intention + legacy, data = legacy2015) 5.3.2 Investigating the moderated (indirect) effect with a bootstrap test There are quite a few packages in R which will allow you to test the moderated effect of a predictor on the dependent variable “via” the mediator. I have chosen here for the mediate function from the mediation package (Tingley et al. 2019), as it is a versatile option, and doesn’t require too much additional explanation. Another very good option to conduct mediation analysis is to specify the mediation model through a generalization of linear models generally called Structural Equation Models. These are multivariate models and not something we will cover in this course. For present purposes, this can be seen as a way to link different regression models (i.e. the dependent variable of one regression model becomes a predictor in another regression model) into what are conventionally called path models. The current “go-to” and most comprehensive SEM package in R is called lavaan and if you ever need to use this kind of analysis, that is my recommendation at the moment. So let’s focus on the mediation package for now. If you haven’t done so already, you will need to install it with install.packages(&quot;mediation&quot;) If you check the documentation of mediate in this package (i.e type ?mediation::mediate), you will see there are lots of arguments to specify. We will only focus one the ones important for present purposes: model.m: the name of the R object which contains the linear model predicting the mediator from the predictor, e.g. mod2 above model.y: the name of the R object which contains the linear model predicting the dependent variable from both the mediator and predictor, e.g. mod3 above sims: the number of simulations to use for the bootstrap test boot: (logical) whether to use a bootstrap test. You should set this to TRUE boot.ci.type: the type of bootstrap confidence interval to be computed. This can either be perc, which stands for percentile, and is a simple way where the empirical 2.5th and 97.5th percentiles are calculated from the ordered outcomes. This is the option chosen by default. The other option is bca which stands for bias-corrected and accelerated. This includes a correction of the percentile method to try and reduce bias. It is generally recommended to use this option in mediation analysis. Hence, you should set boot.ci.type = \"bca\" treat: the name of the predictor in the linear models specified under model.m and model.y. This is would be e.g. legacy in the models above mediator: the name of the mediator in the linear models specified under model.m and model.y. This is would be e.g. intention in the models above To run a bootstrap mediation test with 2000 simulations, we can run the following command: set.seed(20201027) med &lt;- mediation::mediate(model.m = mod2, model.y = mod3, sims = 2000, boot = TRUE, boot.ci.type = &quot;bca&quot;, treat = &quot;legacy&quot;, mediator = &quot;intention&quot;) summary(med) ## ## Causal Mediation Analysis ## ## Nonparametric Bootstrap Confidence Intervals with the BCa Method ## ## Estimate 95% CI Lower 95% CI Upper p-value ## ACME 0.245 0.104 0.45 &lt;2e-16 *** ## ADE 0.488 0.186 0.82 &lt;2e-16 *** ## Total Effect 0.733 0.425 1.07 &lt;2e-16 *** ## Prop. Mediated 0.334 0.155 0.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Sample Size Used: 237 ## ## ## Simulations: 2000 Note that rather than loading the mediation package with e.g. library(mediation) I’m calling the mediate function through mediation::mediate. Loading the mediation package results in loading quite a few other R packages, which are not all necessary to perform a mediation analysis with linear models. Calling the mediate function directly through the appropriate namespace avoids loading these other add-on packages (which then will mask some functions that I don’t want masked). The output from calling the summary function on the results of the bootstrap procedure (the R object I named med) has four rows: ACME: this is the average causal mediation effect, i.e. the average of \\(\\hat{a} \\times \\hat{b}\\) in the simulations. The value under estimate is the average, and you will also see the lower and upper bound of the 95% confidence interval under 95% CI Lower and 95% CI Upper respectively. Finally, the p-value is the probability of the found ACME or more extreme given that in reality, the ACME equals 0. I.e., this is the p-value of the hypothesis test that the true mediated effect equals 0. ADE: this is the average direct effect, and reflects the effect of the predictor which is not mediated. It is the average of \\(\\hat{c}&#39;\\) in the simulations. Total Effect is the total effect of the predictor on the dependent variable, which is the sum of the ACME and ADE. Prop. Mediated is the proportion of the effect of the predictor on the dependent variable which is mediated. This is ACME divided by Total Effect. The results under ACME show that the bootstrap confidence interval of the mediated effect does not include 0. The p-value for this effect is also smaller than \\(\\alpha = .05\\). As such, the null hypothesis that \\(a \\times b = 0\\) is rejected, and we have found evidence that the effect of legacy on donation is mediated by intention. Because the confidence interval of the ADE also does not include 0, this analysis indicates that the mediation is partial. There is also a significant direct effect of legacy on donation. About 33.4% of the effect of legacy on donation is mediated by intention, so the residual direct effect is quite substantial. "],["contrast-coding-and-oneway-anova.html", "Chapter 6 Contrast coding and oneway ANOVA 6.1 Computing contrast-coding predictors 6.2 Assigning contrasts to factors", " Chapter 6 Contrast coding and oneway ANOVA There are several ways in which you can include nominal independent variables in the General Linear Model within R. The first option is to compute the contrast-coding predictors “by hand” and then enter these as metric predictors in the lm function. The second way is to specify the nominal variable as a factor and assign an appropriate contrast to this using the contrasts function. R will then compute the contrast-coding predictors to the factor “automatically” when you enter the factor as a predictor in the lm formula. Finally, you can also use the aov (Analysis Of Variance) function, or the Anova function. These functions are more focussed on omnibus tests rather than tests of the individual contrasts. We will discuss these options using the tetris2015 data, which comes with the sdamr package. There are many variables in this dataset (for a description, see ?tetris2015). The main variables used in the book are Days_One_to_Seven_Number_of_Intrusions (the number of memory intrusions after memory reactivation) and Condition. Let’s open the data and visualize the number of intrusions for the different conditions. library(sdamr) data(&quot;tetris2015&quot;) ## as the main DV has a cumbersome name, I&#39;m creating a copy of the dataset ## with a new variable &#39;intrusions&#39; which is a copy of Days_One_to_Seven_Number_of_Intrusions dat &lt;- tetris2015 dat$intrusions &lt;- dat$Days_One_to_Seven_Number_of_Intrusions set.seed(20201104) # to replicate figure with random jitter plot_raincloud(dat, intrusions, groups = Condition) 6.1 Computing contrast-coding predictors Let’s first focus on data from a subset of the conditions, namely the Tetris+Reactivation and Reactivation-Only condition dat &lt;- subset(dat, Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)) Note the use of the %in% operator. The statement Condition %in% c(\"Tetris_Ractivation\",\"Reactivation\") returns TRUE whenever the value of Condition is equal to one of the values in the vector c(\"Tetris_Ractivation\",\"Reactivation\"). This is shorthand to the equivalent statement dat &lt;- subset(dat, Condition == &quot;Tetris_Reactivation&quot; | Condition == &quot;Reactivation&quot;) If there are lots of “or” values, using %in% can be a lot more efficient (in terms of the code you have to type, at least). Say that we would like to use a dummy coded predictor, with the value of 0 for Tetris_Reactivation, and the value of 1 for Reactivation condition. One way to compute such a variable is as follows: dat$dummy &lt;- 0 dat$dummy[dat$Condition == &quot;Reactivation&quot;] &lt;- 1 The variable dummy is first being created as a new column in dat, by appending the name of the new variable to the data.frame with the usual “$” notation, and then assigning a value to it. On the second line, I then select a subset of the values of dat$dummy (all cases where dat$Condition == \"Reactivation\") and assign the different value 1 to this subset. A quick check using the table function shows that we now indeed have a new variable with values 1 and 0: table(dat$dummy) ## ## 0 1 ## 18 18 We can now use this dummy variable like any other (metric) predictor in a linear model. mod &lt;- lm(intrusions ~ dummy, data=dat) summary(mod) ## ## Call: ## lm(formula = intrusions ~ dummy, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8333 -1.8333 -0.8333 1.1111 10.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.8889 0.6266 3.015 0.00484 ** ## dummy 2.9444 0.8861 3.323 0.00214 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.658 on 34 degrees of freedom ## Multiple R-squared: 0.2451, Adjusted R-squared: 0.2229 ## F-statistic: 11.04 on 1 and 34 DF, p-value: 0.00214 To compute other contrast-coding predictors, you can follow the same procedure. If a contrast-coding predictor only needs two values, then you can also use the ifelse function, which is a little less typing. The ifelse function has three arguments: a logical condition, the value to return when that condition is TRUE, and the value to return when that condition is FALSE. For instance, you can create a variable effect, with the value -.5 for Tetris_Reactivation and a value .5 for Reactivation, as dat$effect &lt;- ifelse(dat$Condition == &quot;Reactivation&quot;,.5,-.5) Here, when dat$Condition == \"Reactivation\", the condition is TRUE, and hence the value .5 is returned, otherwise (when the condition is not true, so Condition != \"Reactivation\"), the value -.5 is returned. As before, we can enter this as a predictor in an linear model as usual: mod &lt;- lm(intrusions ~ effect, data=dat) summary(mod) ## ## Call: ## lm(formula = intrusions ~ effect, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8333 -1.8333 -0.8333 1.1111 10.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3611 0.4431 7.586 8.2e-09 *** ## effect 2.9444 0.8861 3.323 0.00214 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.658 on 34 degrees of freedom ## Multiple R-squared: 0.2451, Adjusted R-squared: 0.2229 ## F-statistic: 11.04 on 1 and 34 DF, p-value: 0.00214 The procedure is easily extended to multiple contrast-coding predictors. For an example, let’s consider the full dataset with all four conditions. dat &lt;- tetris2015 dat$intrusions &lt;- dat$Days_One_to_Seven_Number_of_Intrusions Say that we want a set of orthogonal contrast codes Table 6.1: A set of orthogonal contrast codes. \\(c_1\\) \\(c_2\\) \\(c_3\\) Control \\(\\tfrac{3}{4}\\) 0 0 Tetris_Reactivation \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{2}\\) Tetris \\(-\\tfrac{1}{4}\\) \\(\\tfrac{2}{3}\\) 0 Reactivation \\(-\\tfrac{1}{4}\\) \\(-\\tfrac{1}{3}\\) \\(\\tfrac{1}{2}\\) The first contrast code has only two values, so we can use ifelse. The second and third have three possible values, and then we can’t use that function. # use ifelse dat$c1 &lt;- ifelse(dat$Condition == &quot;Control&quot;, 3/4, -1/4) # create c2 and c3 with default values and then use subsets for other values dat$c2 &lt;- 0 dat$c2[dat$Condition == &quot;Tetris&quot;] &lt;- 2/3 dat$c2[dat$Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)] &lt;- -1/3 # use sapply and switch dat$c3 &lt;- sapply(as.character(dat$Condition), switch, &quot;Control&quot; = 0, &quot;Tetris_Reactivation&quot; = -1/2, &quot;Tetris&quot; = 0, &quot;Reactivation&quot; = 1/2) When creating c3, I used a little R wizardry. The function sapply(X, FUN, ...) can be used to apply a function FUN to each element of a vector or list X. The argument X is that vector or list. As dat$Condition is a factor, but I want to use it as a character vector here, I’m using as.character to convert the factor in a character vector. The second argument FUN is the function you want to apply to the elements of X. I’m using the switch(EXPR, ...) function here. The sapply function will take each element in X and assign that to switch as the EXPR argument. Then any arguments specified as ... in the sapply function will be passed as additional arguments to the FUN function. In this case, what is specified under ... in the sapply function will be passed on to the ... argument of switch. For switch, the ... argument should be a list of alternative values of EXPR, with a corresponding return value. For instance, if EXPR == Control, the switch function will return 0. Using a combination of sapply and e.g. switch makes R a very powerful data manipulation tool. But the ins-and-outs of such applications will require practice. Alternatively, the dplyr package (which is part of the so-called tidyverse) has powerful functionality for data manipulation and data wrangling, which, with practice, are more straightforward to use than functions such as sapply in base R. A main reason for showing you sapply here is to show you the flexibility of R. There are many ways to obtain the same result. Which way you find most intuitive is a personal judgement. Getting back to the reason why we created the new variables in the first place, we can now use them as new predictors in a linear model modg &lt;- lm(intrusions ~ c1 + c2 + c3, data=dat) summary(modg) ## ## Call: ## lm(formula = intrusions ~ c1 + c2 + c3, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1111 -1.8889 -0.8333 1.1111 10.8889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9306 0.3743 10.502 7.11e-16 *** ## c1 1.5741 0.8643 1.821 0.073 . ## c2 0.5278 0.9168 0.576 0.567 ## c3 2.9444 1.0586 2.781 0.007 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.176 on 68 degrees of freedom ## Multiple R-squared: 0.1434, Adjusted R-squared: 0.1056 ## F-statistic: 3.795 on 3 and 68 DF, p-value: 0.01409 We can also obtain the equivalent \\(F\\) tests through the Anova function in the car package car::Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: intrusions ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 1112.35 1 110.2886 7.108e-16 *** ## c1 33.45 1 3.3165 0.072989 . ## c2 3.34 1 0.3314 0.566727 ## c3 78.03 1 7.7364 0.006996 ** ## Residuals 685.83 68 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 To obtain an omnibus test for Condition (i.e. a test that all these slopes in modg are equal to 0), we can create a suitable intercept-only MODEL R and perform a model comparison as follows: modr &lt;- lm(intrusions ~ 1, data=dat) anova(modr, modg) ## Analysis of Variance Table ## ## Model 1: intrusions ~ 1 ## Model 2: intrusions ~ c1 + c2 + c3 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 71 800.65 ## 2 68 685.83 3 114.82 3.7948 0.01409 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the results of this model comparison were already provided in the output of summary(modg). 6.2 Assigning contrasts to factors Creating new variables in a dataset yourself gives you full control, but it can also be a bit cumbersome. Luckily, R has functionality build-in to assign contrasts to nominal variables. These nominal variables with associated contrast codes are factors. In the tetris2015 dataset, the Condition column is already a factor. In other datasets, a nominal variable might be a character vector. You would then first need to turn this into a factor by calling e.g. # This would be useful if dat$Condition is a character vector # it is not needed here! dat$Condition &lt;- as.factor(dat$Condition) Factors have contrast codes associated to them. In R, the default contrast code is dummy coding. You can view (and set) the contrasts via the contrasts() function. First, let’s have a look at what the contrast for dat$Condition looks like: contrasts(dat$Condition) ## Tetris_Reactivation Tetris Reactivation ## Control 0 0 0 ## Tetris_Reactivation 1 0 0 ## Tetris 0 1 0 ## Reactivation 0 0 1 The contrast is a matrix with each column representing a contrast code, and each row a level of the nominal variable. Remember, when there are four levels, we need three contrast codes. The default dummy coding uses the first level as the reference group, and then each contrast code represents a comparison of a later level to the reference level. You can choose your own contrast codes by assigning a matrix with contrast-code values to e.g. contrasts(dat$Condition). For instance, we can use the orthogonal contrast code defined earlier. In the code below, I first create the contrast matrix by combining columns with the cbind() function. You can give the columns names you find intuitive with the colnames function. codes &lt;- cbind(c(3/4,-1/4,-1/4,-1/4), c(0, -1/3, 2/3,-1/3), c(0, -1/2, 0, 1/2)) colnames(codes) &lt;- c(&quot;ctrl-vs-other&quot;,&quot;tetr-vs-memory&quot;, &quot;react-vs-t+r&quot;) contrasts(dat$Condition) &lt;- codes When we now call contrasts again, we can see are new contrast codes: contrasts(dat$Condition) ## ctrl-vs-other tetr-vs-memory react-vs-t+r ## Control 0.75 0.0000000 0.0 ## Tetris_Reactivation -0.25 -0.3333333 -0.5 ## Tetris -0.25 0.6666667 0.0 ## Reactivation -0.25 -0.3333333 0.5 A nice thing about the lm function is that you can also supply factors as predictors directly. Internally, the lm function will then create the necessary contrast-coding predictors from the contrasts supplied to the factor. Let’s try this: modg &lt;- lm(intrusions ~ Condition, data=dat) summary(modg) ## ## Call: ## lm(formula = intrusions ~ Condition, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1111 -1.8889 -0.8333 1.1111 10.8889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9306 0.3743 10.502 7.11e-16 *** ## Conditionctrl-vs-other 1.5741 0.8643 1.821 0.073 . ## Conditiontetr-vs-memory 0.5278 0.9168 0.576 0.567 ## Conditionreact-vs-t+r 2.9444 1.0586 2.781 0.007 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.176 on 68 degrees of freedom ## Multiple R-squared: 0.1434, Adjusted R-squared: 0.1056 ## F-statistic: 3.795 on 3 and 68 DF, p-value: 0.01409 You can see that the output (apart from the names of the effects) is exactly the same as when we created c1, c2, and c3. So that’s pretty neat! As I said, the lm function will create the contrast-coding predictors for factors. You can view the resulting “design matrix” (the matrix with values for all predictors actually used when estimating the parameters) with the model.matrix function (as the output is rather long, I’m calling this within the head function to only show the first few rows) head(model.matrix(modg)) ## (Intercept) Conditionctrl-vs-other Conditiontetr-vs-memory ## 1 1 0.75 0 ## 2 1 0.75 0 ## 3 1 0.75 0 ## 4 1 0.75 0 ## 5 1 0.75 0 ## 6 1 0.75 0 ## Conditionreact-vs-t+r ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 As you can see, the design matrix also includes a column for the intercept. The value of this column is 1 for every case in the data. If you think about it, you can view the intercept as the slope of a predictor variable which always has the value 1: \\[\\beta_0 = \\beta_0 \\times 1\\] 6.2.1 Default coding schemes In addition to assigning your own contrast codes, there are functions to create several “default” coding matrices. These are contr.treatment: dummy coding. contr.sum: effect-coding (sum-to-zero) contr.helmert: orthogonal contrast codes comparing each level of a factor to all levels before it. contr.poly: orthogonal contrast codes, usually used for ordinal levels. You can call each of these functions by specifying how many levels the factor has. E.g. for our Condition factor with four levels, the output of these functions is contr.treatment(4) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 contr.sum(4) ## [,1] [,2] [,3] ## 1 1 0 0 ## 2 0 1 0 ## 3 0 0 1 ## 4 -1 -1 -1 contr.helmert(4) ## [,1] [,2] [,3] ## 1 -1 -1 -1 ## 2 1 -1 -1 ## 3 0 2 -1 ## 4 0 0 3 contr.poly(4) ## .L .Q .C ## [1,] -0.6708204 0.5 -0.2236068 ## [2,] -0.2236068 -0.5 0.6708204 ## [3,] 0.2236068 -0.5 -0.6708204 ## [4,] 0.6708204 0.5 0.2236068 When you look at the output of these functions, you might notice that the scale of each can be different. For instance, in the contr.helmert function, the difference between the highest and lowest value ranges from 2 to 4. In the book, I used values such that a one-unit increase on a contrast-coding predictor reflects a difference between conditions. This convention is not followed in the various contr. functions. If you want default contrast coding schemes which follow this convention, making the parameters of the model a little easier to interpret, you can use the various contrast coding schemes implemented in the codingMatrices package (Venables 2021). this package also implements several other default coding schemes not implemented in the stats package. Instead of contr., this package provides contrast codes through function names starting with code_. For example, you can obtain a Helmert contrast with a different scaling as follows: library(codingMatrices) code_helmert(4) ## H2 H3 H4 ## 1 -0.5 -0.3333333 -0.25 ## 2 0.5 -0.3333333 -0.25 ## 3 0.0 0.6666667 -0.25 ## 4 0.0 0.0000000 0.75 Another nice feature of the codingMatrices package is the mean_contrasts() function, which will show you how the intercept of the resulting model is related to the group means, and how the slope of each contrast-coding predictor is a function of the group means. For instance mean_contrasts(code_helmert(4)) ## m1 m2 m3 m4 ## Ave 1/4 1/4 1/4 1/4 ## H2 -1 1 . . ## H3 -1/2 -1/2 1 . ## H4 -1/3 -1/3 -1/3 1 shows you in the row labelled Ave that the intercept is the sum of each group mean (in the columns) multiplied by \\(\\tfrac{1}{4}\\); i.e. it is the average of averages. The next row (labelled H2) shows you how the slope of the first contrast-coding predictor can be computed from the group means (as the difference between the second mean and the first mean). By contrast, the contr.helmert() function will provide the same intercept, but slopes that are fractions of these differences mean_contrasts(contr.helmert(4)) ## m1 m2 m3 m4 ## Ave 1/4 1/4 1/4 1/4 ## -1/2 1/2 . . ## -1/6 -1/6 1/3 . ## -1/12 -1/12 -1/12 1/4 "],["factorial-anova.html", "Chapter 7 Factorial ANOVA 7.1 Rainclouds for factorial designs 7.2 Formulating, estimating, and testing a factorial ANOVA 7.3 Type 1, 2, and 3 Sums of Squares 7.4 Planned comparisons and post-hoc tests with emmeans 7.5 Testing general contrasts with emmeans", " Chapter 7 Factorial ANOVA In this chapter, we will look at factorial ANOVA, different ways of model comparisons (different SS types), and some ways to perform multiple comparisons and post-hoc tests. We will introduce these with the same data – from a study investigating the role of experimenter belief in social priming – used in Chapter 7 of SDAM. The dataset is available in the sdamr package as expBelief. We can load it from there, and inspect the first six cases, as usual: library(sdamr) data(&quot;expBelief&quot;) head(expBelief) ## pid exptrNum age gender yearInUni ethnicity englishFluency ## 1 1001 1 18 F 1 caucasian 7 ## 2 1003 1 18 F 1 CAUCASIAN 7 ## 3 1004 1 18 F 1 MiddleEastern 7 ## 4 1005 1 19 M 1 asian 5 ## 5 1006 1 21 M 1 china 5 ## 6 1007 1 19 F 1 caucasion 7 ## experimenterBelief primeCond powerPRE powerPOST ApproachAdvantage attractive ## 1 H LPP 56.1 68.8 -149.14290 6 ## 2 H LPP 98.2 89.0 -171.48810 1 ## 3 H HPP 46.5 70.2 -27.33083 5 ## 4 H LPP 58.8 44.8 374.15660 5 ## 5 L HPP 64.7 53.6 68.55114 5 ## 6 H HPP 61.4 50.3 -92.96860 5 ## competent friendly trustworthy ## 1 7 7 7 ## 2 3 5 6 ## 3 7 7 7 ## 4 5 6 5 ## 5 6 6 6 ## 6 7 7 7 7.1 Rainclouds for factorial designs The experiment had a 2 (social prime: low-power bs high power prime) by 2 (experimenter belief: low-power vs high-power manipulation). The two experimental factors are called primeCond and experimenterBelief in the data frame. The dependent variable we looked at is called ApproachAdvantage. We can use the plot_raincloud function from sdamr for plotting the data. The function has an argument groups which allows you to plot separate rainclouds for different levels of a grouping variable. In this case, we need four rainclouds. Because there is no variable to reflect the combinations of the levels of primeCond and experimenterBelief, we should create one first. The interaction() function is a useful function to create a new factor from the combinations of a set of given factors. As I’m going to make changes to the original dataset, I like to first create a new copy of the data fro this, so that I can later still use the original data set. # create a copy of expBelief and call it &quot;dat&quot; dat &lt;- expBelief dat$condition &lt;- interaction(dat$primeCond, dat$experimenterBelief) # show the levels of the newly created factor: levels(dat$condition) ## [1] &quot;HPP.H&quot; &quot;LPP.H&quot; &quot;HPP.L&quot; &quot;LPP.L&quot; You can see that the condition factor has four levels, which concatenate the levels of primeCond (which are LPP for low-power prime, and HPP for high-power prime) and experimenterBelief (which as L for when the experimenter is made to believe the participant received the low-power prime, and H for when the experimenter believed this was the high-power prime). We can now create a raincloud plot for the four conditions as follows: plot_raincloud(dat, ApproachAdvantage, groups=condition) In the book, I first changed the labels of the two variables before calling the interaction function. If you want the same plot as in the book, you could run the following code (which is not evaluated here): # turn primeCond and experimenterBelief in factors and change the labels dat$primeCond &lt;- factor(dat$primeCond, labels=c(&quot;PH&quot;,&quot;PL&quot;)) dat$experimenterBelief &lt;- factor(dat$experimenterBelief, labels=c(&quot;EH&quot;,&quot;EL&quot;)) # now create an interaction factor, and change the separation sign to &quot;-&quot; instead of &quot;.&quot; dat$condition &lt;- interaction(dat$primeCond, dat$experimenterBelief, sep=&quot;-&quot;) plot_raincloud(dat, ApproachAdvantage, groups=condition) The raincloud plot above effectively treats the design as a oneway design. If we want the plot to more directly reflect the factorial design, we can add some functionality from the ggplot2 package. In particular, we can use so-called facets, which basically allow you to repeatedly draw a similar plot for different levels of an independent variable. Because the plot_raincloud produces a raincloud plot by calling underlying ggplot2 functions, and the result is a ggplot, you can use any function from ggplot2 to make changes to the resulting plot. For instance, we can, within a plot, separate the levels of the experimenterBelief manipulation, and then create two panels (facets) for the levels of the primeCond condition. This is done as follows: plot_raincloud(dat, ApproachAdvantage, groups = experimenterBelief) + facet_grid(~primeCond) As usual, it pays to read the documentation for the facet_grid function (try calling ?facet_grid). There is an alternative for facet_grid, called facet_wrap, which provides slightly different labelling to the panels. facet_grid is particularly useful when you have two independent variables in a factorial design for which you would like to create different panels (we will show an example of this later). As we are considering factorial designs here, I chose to use facet_grid, but you can try facet_wrap as well. 7.2 Formulating, estimating, and testing a factorial ANOVA Formulating a factorial ANOVA model, where we distinguish between main effects and interactions, is not any different from formulating a moderated regression model. We can use the formula interface to indicate that we want to include predictors, as well as the product predictors required to assess interactions. In this case, however, we will enter nominal independent variables into the formula. When these are defined as factors with associated contrast codes, R will automatically expand the model to include the contrast-coded predictors, as well as all relevant product-predictors. The first thing to do is to make sure that the variables are defined as factors # check what type the two IVs are class(dat$primeCond) ## [1] &quot;character&quot; class(dat$experimenterBelief) ## [1] &quot;character&quot; # turn each into a factor dat$primeCond &lt;- as.factor(dat$primeCond) dat$experimenterBelief &lt;- as.factor(dat$experimenterBelief) # let&#39;s check the class for one of them to make sure class(dat$primeCond) ## [1] &quot;factor&quot; # that worked :-) Now let’s define appropriate contrast codes. As usual, it is a good idea to first check the existing contrast, as this shows the order of the factor levels: contrasts(dat$primeCond) ## LPP ## HPP 0 ## LPP 1 We can see that we need to define a single contrast, with the value for HPP (high-power prime) first and then the value for LPP (low-power prime) second. As the social priming hypothesis would predict the ApproachAdvantage score to be higher for LPP than for HPP, the following contrast makes sense: contrasts(dat$primeCond) &lt;- c(1/2, -1/2) contrasts(dat$primeCond) ## [,1] ## HPP 0.5 ## LPP -0.5 We define the contrast for experimenterBelief in the same way: contrasts(dat$experimenterBelief) ## L ## H 0 ## L 1 # H comes before l contrasts(dat$experimenterBelief) &lt;- c(1/2, -1/2) contrasts(dat$experimenterBelief) ## [,1] ## H 0.5 ## L -0.5 Now we are ready to estimate the linear model. To estimate a model with the main effects and interaction, we would use: modg &lt;- lm(ApproachAdvantage ~ primeCond*experimenterBelief, data=dat) Remember that this notation will expand the formula to ApproachAdvantage ~ 1 + primeCond + experimenterBelief + primeCond:experimenterBelief i.e. to a model with an intercept, a main effect of primeCond, a main effect of experimenterBelief, and an interaction primeCond:experimenterBelief. The easiest way to obtain the parameter estimates (and t-tests for those) is to use the summary function on this fitted linear model: summary(modg) ## ## Call: ## lm(formula = ApproachAdvantage ~ primeCond * experimenterBelief, ## data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -556.7 -176.3 10.6 169.8 605.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.46 10.72 2.002 0.046 * ## primeCond1 11.98 21.44 0.559 0.577 ## experimenterBelief1 90.51 21.44 4.221 3.02e-05 *** ## primeCond1:experimenterBelief1 37.64 42.88 0.878 0.381 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 214.4 on 396 degrees of freedom ## Multiple R-squared: 0.04565, Adjusted R-squared: 0.03842 ## F-statistic: 6.314 on 3 and 396 DF, p-value: 0.0003425 Alternatively, we can use the Anova function from the car package to obtain Type 3 (omnibus) tests: car::Anova(modg, type=3) ## Anova Table (Type III tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 184285 1 4.0084 0.04596 * ## primeCond 14343 1 0.3120 0.57678 ## experimenterBelief 819158 1 17.8175 3.018e-05 *** ## primeCond:experimenterBelief 35410 1 0.7702 0.38069 ## Residuals 18206049 396 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can see that while the Anova function reports \\(F\\) statistics, the tests and the corresponding \\(p\\)-values are identical. In this case, each factor only has two levels, and hence one contrast code. As there is only one parameter associated to each main effect and interaction (the slope of the single contrast-coding predictor for that effect), the omnibus test is a single parameter test. We will see an example where this is not the case shortly. 7.2.1 A threeway ANOVA We can also try to assess the experimenter effects by including this as an additional factor. Experimenter has four levels, so we’ll need three contrast codes for this variable. In the data.frame, Experimenter is included as exptrNum, which is a numerical variable. So we will first convert it into a factor class(dat$exptrNum) ## [1] &quot;integer&quot; dat$exptrNum &lt;- factor(dat$exptrNum, labels=paste0(&quot;E&quot;,1:4)) I’m using factor here rather than as.factor, because the former allows me to add labels to the levels, through the labels argument. Note that I’m using the paste0 function to create a vector with labels. This function can create combinations of (character) vectors, and is quite handy. The paste0 function is very similar to the paste function, but doesn’t include a space between the combinations: paste0(&quot;E&quot;,1:4) ## [1] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E4&quot; paste(&quot;E&quot;,1:4) ## [1] &quot;E 1&quot; &quot;E 2&quot; &quot;E 3&quot; &quot;E 4&quot; Right, so let’s define a contrast for exptrNum. contrasts(dat$exptrNum) ## E2 E3 E4 ## E1 0 0 0 ## E2 1 0 0 ## E3 0 1 0 ## E4 0 0 1 contrasts(dat$exptrNum) &lt;- cbind(c(-1/2,1/2,0,0), c(-1/3,-1/3,2/3,0), c(-1/4,-1/4,-1/4,3/4)) contrasts(dat$exptrNum) ## [,1] [,2] [,3] ## E1 -0.5 -0.3333333 -0.25 ## E2 0.5 -0.3333333 -0.25 ## E3 0.0 0.6666667 -0.25 ## E4 0.0 0.0000000 0.75 Before conducting the analysis, it is always a good idea to look at the data. Let’s create a slightly different raincloud plot than the one in Chapter 8 of SDAMR, now more explicitly reflecting the factorial nature of the design: plot_raincloud(dat, ApproachAdvantage, groups = experimenterBelief) + facet_grid(primeCond ~ exptrNum) This plot is not necessarily better than the one in SDAM. It does quite clearly highlight that experimenter belief does not seem to have an effect for Experimenter 4. However, personally, I find it more difficult to assess the effect of prime condition. For that, we could create a second plot to show the effect of primeCond within each panel: plot_raincloud(dat, ApproachAdvantage, groups = primeCond) + facet_grid(experimenterBelief ~ exptrNum) This indicates quite clearly that priming condition does not seem to have much of an effect for any experimenter of experimenter belief condition. Back to the analysis, then. We can estimate a threeway factorial ANOVA by simply adding another independent variable to the formula: modg_exp &lt;- lm(ApproachAdvantage ~ primeCond*experimenterBelief*exptrNum, data=dat) This formula is expanded to ApproachAdvantage ~ 1 + primeCond + experimenterBelief + exptrNum + primeCond:experimenterBelief + primeCond:exptrNum + experimenterBelief:exptrNum + primeCond:experimenterBelief:exptrNum In other words, the model includes all the main effects, all pairwise interactions between the factors, as well as the threeway interaction. We can see the parameter estimates and associated \\(t\\)-tests as usual through the summary function: summary(modg_exp) ## ## Call: ## lm(formula = ApproachAdvantage ~ primeCond * experimenterBelief * ## exptrNum, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -507.20 -179.77 11.34 167.78 589.70 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.401 10.742 1.992 0.0471 ## primeCond1 11.908 21.485 0.554 0.5797 ## experimenterBelief1 90.441 21.485 4.210 3.19e-05 ## exptrNum1 -7.411 30.387 -0.244 0.8075 ## exptrNum2 -5.076 26.312 -0.193 0.8471 ## exptrNum3 42.309 24.807 1.706 0.0889 ## primeCond1:experimenterBelief1 37.381 42.969 0.870 0.3849 ## primeCond1:exptrNum1 -60.323 60.774 -0.993 0.3215 ## primeCond1:exptrNum2 1.765 52.625 0.034 0.9733 ## primeCond1:exptrNum3 -21.445 49.613 -0.432 0.6658 ## experimenterBelief1:exptrNum1 -17.453 60.774 -0.287 0.7741 ## experimenterBelief1:exptrNum2 11.056 52.625 0.210 0.8337 ## experimenterBelief1:exptrNum3 -116.864 49.613 -2.356 0.0190 ## primeCond1:experimenterBelief1:exptrNum1 48.352 121.548 0.398 0.6910 ## primeCond1:experimenterBelief1:exptrNum2 -8.115 105.249 -0.077 0.9386 ## primeCond1:experimenterBelief1:exptrNum3 68.237 99.227 0.688 0.4921 ## ## (Intercept) * ## primeCond1 ## experimenterBelief1 *** ## exptrNum1 ## exptrNum2 ## exptrNum3 . ## primeCond1:experimenterBelief1 ## primeCond1:exptrNum1 ## primeCond1:exptrNum2 ## primeCond1:exptrNum3 ## experimenterBelief1:exptrNum1 ## experimenterBelief1:exptrNum2 ## experimenterBelief1:exptrNum3 * ## primeCond1:experimenterBelief1:exptrNum1 ## primeCond1:experimenterBelief1:exptrNum2 ## primeCond1:experimenterBelief1:exptrNum3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 214.8 on 384 degrees of freedom ## Multiple R-squared: 0.07105, Adjusted R-squared: 0.03476 ## F-statistic: 1.958 on 15 and 384 DF, p-value: 0.01719 Wow, there are a lot of estimates and tests here (16 in total)! While these tests are informative, it is common to (at least also) consider omnibus tests. Experimenter has four levels, so three associated contrasts, and we can’t find a test of the “overall” main effect of Experimenter in the output above. For these omnibus tests, we can (as before) use the Anova function from the car package: car::Anova(modg_exp,type=3) ## Anova Table (Type III tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 183165 1 3.9689 0.04705 * ## primeCond 14178 1 0.3072 0.57972 ## experimenterBelief 817797 1 17.7205 3.19e-05 *** ## exptrNum 138582 3 1.0010 0.39238 ## primeCond:experimenterBelief 34927 1 0.7568 0.38487 ## primeCond:exptrNum 54288 3 0.3921 0.75875 ## experimenterBelief:exptrNum 262131 3 1.8933 0.13018 ## primeCond:experimenterBelief:exptrNum 29490 3 0.2130 0.88738 ## Residuals 17721491 384 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you’d just consider the output from this function, which does not provide a significant Experimenter by Belief interaction, you probably would have missed the potentially interesting experimenterBelief1:exptrNum3 interaction, which was discussed in SDAM. 7.3 Type 1, 2, and 3 Sums of Squares Whilst intended (I think!) as a factorial experiment with an equal sample size for each priming condition, experimenter belief, and experimenter combination, the sample sizes are actually slightly unbalanced. One way to count the number of cases for each combination of factor levels is through the ftable function (which stands for frequency table). The function has various interfaces, and I find the formula interface easiest to work with. On the left-hand side of the formula, you can provide the name of a factor which you want to place in the columns of the table, and on the right-hand side you can include multiple factors which make up the rows, separated by a “+” sign: ftable(exptrNum ~ primeCond + experimenterBelief, data=dat) ## exptrNum E1 E2 E3 E4 ## primeCond experimenterBelief ## HPP H 26 25 25 25 ## L 25 25 25 25 ## LPP H 25 25 25 25 ## L 25 24 25 25 As you can see, Experimenter 1 tested 26 participants in the high-power prime and high experimenter belief condition, whilst experimenter 2 tested 24 participants in the low-power prime and low experimenter belief condition. The result of unbalanced data is that the contrast-coding predictors are no longer orthogonal. As a result, different ways of performing model comparisons will give different results. The differences are likely to be rather subtle here, because the sample sizes are mostly equal. Nevertheless, let’s consider how we can obtain results for the Type 1 and Type 2 SS procedures. A Type 2 procedure is easily obtained by using the car::Anova function, now with argument type=2: car::Anova(modg_exp,type=2) ## Anova Table (Type II tests) ## ## Response: ApproachAdvantage ## Sum Sq Df F value Pr(&gt;F) ## primeCond 14682 1 0.3181 0.5731 ## experimenterBelief 819687 1 17.7615 3.125e-05 *** ## exptrNum 138507 3 1.0004 0.3926 ## primeCond:experimenterBelief 34707 1 0.7520 0.3864 ## primeCond:exptrNum 53901 3 0.3893 0.7608 ## experimenterBelief:exptrNum 261854 3 1.8913 0.1305 ## primeCond:experimenterBelief:exptrNum 29490 3 0.2130 0.8874 ## Residuals 17721491 384 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you compare the results to those of the Type 3 procedure used earlier, you can see some subtle differences. You can see that (apart from the threeway interaction), the SSR terms are slightly different, leading to small differences in the \\(F\\) statistic and associated \\(p\\)-value. Unfortunately, the car::Anova function will not work with type=1. To get the results of a Type 1 procedure, you can use the anova function from the default stats package: anova(modg_exp) ## Analysis of Variance Table ## ## Response: ApproachAdvantage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## primeCond 1 14565 14565 0.3156 0.5746 ## experimenterBelief 1 820883 820883 17.7874 3.084e-05 ## exptrNum 3 138657 46219 1.0015 0.3921 ## primeCond:experimenterBelief 1 35260 35260 0.7640 0.3826 ## primeCond:exptrNum 3 54707 18236 0.3951 0.7566 ## experimenterBelief:exptrNum 3 261854 87285 1.8913 0.1305 ## primeCond:experimenterBelief:exptrNum 3 29490 9830 0.2130 0.8874 ## Residuals 384 17721491 46150 ## ## primeCond ## experimenterBelief *** ## exptrNum ## primeCond:experimenterBelief ## primeCond:exptrNum ## experimenterBelief:exptrNum ## primeCond:experimenterBelief:exptrNum ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 again, comparing this to the results of those obtained previously, you can see differences in the SSR terms. It is important to realise that the Type 1 procedure depends on the order of the factors in the formula. For instance, if we change this order as follows: anova(lm(ApproachAdvantage ~ experimenterBelief*exptrNum*primeCond, data=dat)) ## Analysis of Variance Table ## ## Response: ApproachAdvantage ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## experimenterBelief 1 820878 820878 17.7873 3.084e-05 ## exptrNum 3 138443 46148 1.0000 0.3928 ## primeCond 1 14784 14784 0.3204 0.5717 ## experimenterBelief:exptrNum 3 263137 87712 1.9006 0.1290 ## experimenterBelief:primeCond 1 34782 34782 0.7537 0.3859 ## exptrNum:primeCond 3 53901 17967 0.3893 0.7608 ## experimenterBelief:exptrNum:primeCond 3 29490 9830 0.2130 0.8874 ## Residuals 384 17721491 46150 ## ## experimenterBelief *** ## exptrNum ## primeCond ## experimenterBelief:exptrNum ## experimenterBelief:primeCond ## exptrNum:primeCond ## experimenterBelief:exptrNum:primeCond ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 you get slightly different results. I should also mention the aov function from the stats package, which will also provide Type 1 ANOVA tests. I have wrestled with this function often when I started using R a long time ago. I’ve happily not used it for some time now, so will only mention its existence here. While the differences are very subtle here, this should not lead you to believe that the methods generally provide the same results. When the design is more unbalanced, the results can change quite dramatically. Finally, I want to point out again that all three procedures test the SSR terms against the same SSE term (the SS given under Residuals). This is the SSE of the full model (the model with all effects) and this is exactly the same for all three procedures. The procedures differ in how they compute the SSR terms for the different effects, as you can see. 7.4 Planned comparisons and post-hoc tests with emmeans The emmeans package (Lenth 2021) is very useful when you want to do more comparisons than can be implemented in the contrast codes within a single model, whether these are planned comparisons or post-hoc tests. The name of the package stands for estimated marginal means. One part of the functionality of the package is to compute the (unweighted) marginal means according to different types of models, including linear models. In the SDAM book, we discussed how these marginal means can be computed using contrast codes. For example, when using orthogonal contrasts, the intercept represents the grand mean, which is a simple average of averages, where the sample means of all groups are added and then divided by the number of groups. If the groups have unequal sample sizes, this is not taken into account in computing the grand mean. That is what is meant by unweighted marginal means. You can think of the marginal means as the estimated population means assuming all groups have an equal sample size. The emmeans function (from the emmeans package with the same name) provides a simple way to compute the estimated marginal means for each condition, but also for the levels of one factor (averaging over the levels of other factors). The emmeans requires at least two arguments: an object, which is an estimated model, and specs, which is either a character vector with the names for the predictors for which the estimated marginal predictors should be computed, or a formula. Here, we will use the formula interface, as it is flexible and intuitive. Going back to our simpler 2 by 2 design (ignoring Experimenter), the estimated marginal means of the four conditions can be computed with emmeans as follows: # load the package. If you don&#39;t have it installed, you will need to run # install.packages(&quot;emmeans&quot;) first! library(emmeans) # call emmeans with modg as the object emmeans(modg, specs = ~ primeCond:experimenterBelief) ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 You can see that the emmeans function computes an estimated marginal mean for each combination of primeCond and experimenterBelief. For each mean, we get an estimate, a standard error of that estimate, the degrees of freedom (\\(n - \\text{npar}(G)\\)), and a confidence interval. The marginal means for each group in the design are just the sample means in the groups in this case, but things become more complicated when we add additional metric predictors to the design, as we will see when we discuss ANCOVA in another chapter. You can also obtain estimated marginal means for the levels of one factor, averaging over the levels of the others. These are the marginal means that are compared in the main effects of that factor. For instance, for the primeCond factor, the marginal means are emmeans(modg, specs = ~ primeCond) ## primeCond emmean SE df lower.CL upper.CL ## HPP 27.5 15.1 396 -2.28 57.2 ## LPP 15.5 15.2 396 -14.41 45.4 ## ## Results are averaged over the levels of: experimenterBelief ## Confidence level used: 0.95 and for experimenterBelief they are: emmeans(modg, specs = ~ experimenterBelief) ## experimenterBelief emmean SE df lower.CL upper.CL ## H 66.7 15.1 396 37.0 96.45 ## L -23.8 15.2 396 -53.7 6.09 ## ## Results are averaged over the levels of: primeCond ## Confidence level used: 0.95 In addition to computing marginal means and providing confidence intervals for each, the package has a reasonably straightforward interface for testing differences between estimated marginal means. Such differences are effectively the contrasts that we have specified with contrast codes, and tested with Type 3 tests. A benefit of using emmeans is that you can test more of these contrasts than the required number of contrast codes (i.e. 3 in this example). If you want all pairwise comparisons between the means, you can get these by entering pairwise as the left-hand side of the formula: emmeans(modg, specs = pairwise ~ primeCond:experimenterBelief) ## $emmeans ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## HPP H - LPP H 30.79 30.2 396 1.018 0.7390 ## HPP H - HPP L 109.33 30.2 396 3.614 0.0019 ## HPP H - LPP L 102.49 30.3 396 3.380 0.0044 ## LPP H - HPP L 78.53 30.3 396 2.590 0.0487 ## LPP H - LPP L 71.69 30.4 396 2.358 0.0870 ## HPP L - LPP L -6.84 30.4 396 -0.225 0.9960 ## ## P value adjustment: tukey method for comparing a family of 4 estimates which automatically uses the Tukey HSD procedure to adjust the significance level of each test to obtain a family-wise significance level of \\(\\alpha_\\text{FW} = .05\\). You can obtain other corrections through the adjust argument. Some options to enter there, which were discussed in SDAM, are: tukey scheffe bonferroni holm There are other possibilities (see ?summary.emmGrid for details). For instance, we can apply the Scheffé adjustment with: emmeans(modg, specs = pairwise ~ primeCond:experimenterBelief, adjust=&quot;scheffe&quot;) ## $emmeans ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## HPP H - LPP H 30.79 30.2 396 1.018 0.7924 ## HPP H - HPP L 109.33 30.2 396 3.614 0.0049 ## HPP H - LPP L 102.49 30.3 396 3.380 0.0103 ## LPP H - HPP L 78.53 30.3 396 2.590 0.0836 ## LPP H - LPP L 71.69 30.4 396 2.358 0.1369 ## HPP L - LPP L -6.84 30.4 396 -0.225 0.9970 ## ## P value adjustment: scheffe method with rank 3 7.4.1 Adjusted p-values One thing I should mention is that rather than showing you the corrected significance level \\(\\alpha\\) for each test, emmeans provides you with an adjusted p-value. A benefit of this is that you can just compare each \\(p-value\\) to the usual criterion level and call each test significant when \\(p&lt;.05\\). However, I personally don’t find the resulting \\(p\\) value easy to interpret as a probability. Remember that the conventional \\(p\\)-value is the probability of obtaining a test result as large or more extreme, assuming that the null hypothesis is true. This is itself already a tricky concept, but with some experience with statistical distributions and the conceptual foundations of null-hypothesis significance testing, it is a valid probability that is interpretable as such. Wright (1992) defines the adjusted p-value as the smallest family-wise significance level at which the tested null-hypothesis would be rejected. This isn’t really a probability any more, as far as I can see it. It is true that the conventional \\(p\\)-value is, by definition, also equal to the smallest significance level at which the null-hypothesis would be rejected. For instance, if a particular test provides a \\(p\\)-value of \\(p = .07\\), then you would reject the null-hypothesis by setting \\(\\alpha \\geq .07\\). Hence, \\(\\alpha = .07\\) is the smallest value of \\(\\alpha\\) which would provide a significant test result. Similarly, if the test provided a \\(p\\)-value of \\(p=.004\\), then \\(\\alpha = .004\\) is the smallest significance level for which the test would provide a significant result. Although the \\(p\\)-value is equivalent to this “minimum \\(\\alpha\\)” value, it is also a valid probability, and when you move to the domain of corrections for multiple comparisons, defining the \\(p\\)-value as the minimum family-wise significance level \\(\\alpha_\\text{FW}\\) for which the individual test would provide a significant test result, the correspondence with a proper probability is lost. For the Bonferroni correction, the adjusted \\(p\\)-value is easy to compute. Remember that the Bonferroni correction for a total of \\(q\\) tests is to set the significance level of each individual test to \\(\\alpha = \\frac{\\alpha}{q}\\). We can adjust the \\(p\\)-values correspondingly as \\(p_\\text{adj} = q \\times p\\). But if you’d perform \\(q=100\\) tests, and \\(p=.2\\), then \\(p_\\text{adj} = 100 \\times .2 = 20\\), which is obviously not a valid probability! Adjusted \\(p\\)-values are in this sense just convenience values which can be compared to e.g. \\(\\alpha_\\text{FW} = .05\\), but nothing more. 7.5 Testing general contrasts with emmeans When you have computed the required estimated marginal means, you can then use these to define a set of general contrasts that you want to test. This set can include more contrasts than \\(g-1\\), but each contrast is defined in a way that we are used to. Let’s consider an example. ems &lt;- emmeans(modg, specs = ~ primeCond:experimenterBelief) ems ## primeCond experimenterBelief emmean SE df lower.CL upper.CL ## HPP H 82.1 21.3 396 40.17 124.1 ## LPP H 51.3 21.4 396 9.17 93.5 ## HPP L -27.2 21.4 396 -69.36 14.9 ## LPP L -20.4 21.5 396 -62.74 22.0 ## ## Confidence level used: 0.95 We have four estimated marginal means, and the order that these are presented in is seen above: HPP,H, LPP,H, HPP,L, and LPP,L. We can now use the contrast function from emmeans (note that there is no “s” at the end, so this is a different function than contrasts!) to supply the ems object and a list of (named) contrasts. Suppose we want to test the following set of (somewhat arbitrary) contrasts: \\(c_1\\) \\(c_2\\) \\(c_3\\) \\(c_4\\) \\(c_5\\) \\(c_6\\) \\(c_7\\) \\(c_8\\) HPP,H \\(\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{2}{3}\\) \\(0\\) LPP,H \\(-\\tfrac{1}{2}\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{3}\\) HPP,L \\(\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(0\\) \\(\\tfrac{1}{2}\\) \\(0\\) \\(\\tfrac{2}{3}\\) LPP,L \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{2}\\) \\(0\\) \\(-\\tfrac{1}{2}\\) \\(-\\tfrac{1}{3}\\) \\(-\\tfrac{1}{3}\\) These can be interpreted as \\(c_1\\): main effect of Prime \\(c_2\\): main effect of Belief \\(c_3\\): comparing high and low belief for low-power prime \\(c_4\\): comparing high and low belief for high-power prime \\(c_5\\): comparing high-power and low-power prime for high belief \\(c_6\\): comparing high-power and low-power prime for low belief \\(c_7\\): comparing high-power prime with high belief to low-power prime conditions \\(c_8\\): comparing high-power prime with low belief to low-power prime conditions Using the contrast function, all these comparisons can be tested simultaneously, using the Scheffé adjustment, as follows: contrast(ems, method = list(c1 = c(1/2, -1/2, 1/2, -1/2), c2 = c(1/2, 1/2, -1/2, -1/2), c3 = c(1/2, 0, -1/2, 0), c4 = c(0, 1/2, 0, -1/2), c5 = c(1/2, -1/2, 0, 0), c6 = c(0, 0, 1/2, -1/2), c7 = c(2/3, -1/3, 0, -1/3), c8 = c(0, -1/3, 2/3, -1/3)), adjust=&quot;scheffe&quot;) ## contrast estimate SE df t.ratio p.value ## c1 11.98 21.4 396 0.559 0.9577 ## c2 90.51 21.4 396 4.221 0.0006 ## c3 54.66 15.1 396 3.614 0.0049 ## c4 35.85 15.2 396 2.358 0.1369 ## c5 15.40 15.1 396 1.018 0.7924 ## c6 -3.42 15.2 396 -0.225 0.9970 ## c7 44.43 17.5 396 2.544 0.0926 ## c8 -28.46 17.5 396 -1.624 0.4518 ## ## P value adjustment: scheffe method with rank 3 If this were a set of planned comparison, and you were confident enough to not apply a correction for multiple comparison, you could leave out the adjust argument, or provide the value adjust=\"none\". "],["repeated-measures-anova.html", "Chapter 8 Repeated-measures ANOVA", " Chapter 8 Repeated-measures ANOVA "],["linear-mixed-effects-models.html", "Chapter 9 Linear mixed-effects models 9.1 Formulating and estimating linear mixed-effects models with lme4 9.2 Obtaining p-values with afex::mixed", " Chapter 9 Linear mixed-effects models In this Chapter, we will look at how to estimate and perform hypothesis tests for linear mixed-effects models. The main workhorse for estimating linear mixed-effects models is the lme4 package. This package allows you to formulate a wide variety of mixed-effects and multilevel models through an extension of the R formula syntax. It is a really good package. But the main author of the package, Douglas Bates, has chosen not to provide \\(p\\)-values for the model parameters. We will therefore also consider the afex package, which provides an interface to the two main approximations (Kenward-Roger and Satterthwaite) to provide the degrees of freedom to compute \\(p\\)-values for \\(F\\) tests. While the mixed function it provides is in principle all you need to estimate the models and get the estimates, I think it is useful to also understand the underlying lme4 package (Bates et al. 2020), so we will start with a discussion of this, and then move to the afex package (Singmann et al. 2021). If you install the afex package, it will install quite a few other packages on which it relies. So to get all the required packages for this chapter, you can just type install.packages(&quot;afex&quot;) In the R package, and you should have everything you need. 9.1 Formulating and estimating linear mixed-effects models with lme4 The gold standard for fitting linear mixed-effects models in R is the lmer() (for linear mixed-effects regression) in the lme4 package. This function takes the following arguments (amongst others, for the full list of arguments, see ?lmer): formula: a two-sided linear formula describing both the fixed-effects and random-effects part of the model, with the response on the left of the ~ operator and predictors and random effects on the right-hand side of the ~ operator. data: A data.frame, which needs to be in the so-called “long” format, with a single row per observation. This may be different from what you might be used to when dealing with repeated-measures. A repeated-measures ANOVA in SPSS requires data in the “wide” format, where you use columns for the different repeated measures. Data in the “wide” format has a single row for each participants. In the “long” format, you will have multiple rows for the data from a single grouping level (e.g., participant, lab, etc.). REML: A logical variable whether to estimate the model with restricted maximum likelihood (REML = TRUE, the default) or with maximum likelihood (REML = FALSE). As correct use of the formula interface is vital, let’s first consider again how the formula interface works in general. Formulas allow you to specify a linear model succinctly (and by default, any model created with a formula will include an intercept, unless explicitly removed). Here are some examples (adapted from Singmann and Kellen (2019)): Formula Description a + b main effects of a and b (and no interaction) a:b only interaction of a and b (and no main effects) a * b main effects and interaction of a and b (expands to: a + b + a:b) (a+b+c)^2 main effects and two-way interactions, but no three-way interaction (expands to: a + b + c + a:b + b:c + a:c) (a+b)*c all main effects and pairwise interactions between c and a or b (expands to: a + b + c + a:c + b:c) 0 + a 0 suppresses the intercept resulting in a model that has one parameter per level of a (identical to: a - 1) The lme4 package extends the formula interface to specify random effects structures. Random effects are added to the formula by writing elements between parentheses (). Within these parentheses, you provide the specification of the random effects to be included on the left-hand side of a conditional sign |. On the right-hand side of the sign, you specify the grouping factor, or grouping factors, on which these random effects depend. The grouping factors _need to be of class factor__ (i.e., they can __not_ be numeric variables). Here are some examples of such specifications (again adapted from Singmann and Kellen (2019)): Formula Description (1|s) random intercepts for unique level of the factor s (1|s) + (1|i) random intercepts for each unique level of s and for each unique level of i (a|s) random intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated. (identical to (a*b|s)) (a*b|s) random intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated. (0+a|s) | random slopes for a for each level of s, but no random intercepts (a||s) | random intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e. they are set to 0). This expands to: (0+a|s) + (1|s)) 0 + a | 0 suppresses the intercept resulting in a model that has one parameter per level of a 9.1.1 Random intercepts model Now let’s try to define a relatively simple linear mixed-effects model for the anchoring data set in the sdamr package. We will use the data from all the referrers try a few variations to get acquinted with the lmer() function. First, let’s load the packages and the data: library(sdamr) library(lme4) data(&quot;anchoring&quot;) Now let’s estimate a first linear mixed-effects model, with a fixed effect for anchor, and random intercepts, using everest_feet as the dependent variable. We will first ensure that anchor is a factor and associate a sum-to-zero contrast to it. We will also make referrer a factor; the contrast for this shouldn’t really matter, so we’ll leave it as a dummy code. We then set up the model, using (1|referrer) to specify that random intercept-effects should be included for each level of the referrer factor. Finally, we use the summary() function on the estimated model to obtain the estimates of the parameters. anchoring$anchor &lt;- as.factor(anchoring$anchor) contrasts(anchoring$anchor) &lt;- c(1/2, -1/2) # alphabetical order, so high before low # define a lmer mod &lt;- lmer(everest_feet ~ anchor + (1|referrer), data=anchoring) summary(mod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + (1 | referrer) ## Data: anchoring ## ## REML criterion at convergence: 98195.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.4433 -0.7178 -0.1361 0.7419 3.5546 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2717134 1648 ## Residual 93929988 9692 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22788.3 346.8 65.70 ## anchor1 23047.3 285.8 80.64 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.022 The output first shows some information about the structure of the model, and the value of the optimized “-2 log REML” (the logarithm of the minus two restricted maximum likelihood). Then some summary statistics for the standardized residuals are shown (these are the “raw” residuals divided by the estimated standard deviation of the residuals). Under the Random effects: header, you will find a table with estimates of the variance and standard deviation of the random effects terms for each grouping factor (just referrer in this case). So the estimated standard deviation of the random intercept: \\(\\hat{\\sigma}_{\\gamma_0} = 1648\\). You will also find an estimate of the variance and standard deviation of the residuals in this table: \\(\\hat{\\sigma}_\\epsilon = 9692\\)}. Under the Fixed effects: header, you will find a table with an estimate for each fixed effect, as well as the standard error of this estimate, and an associated \\(t\\) statistic. This output is much like the output of calling the summary() function on a standard model estimated with the lm function. But you won’t find the \\(p\\)-value for these estimates. This is because the author of the lme4 package, perhaps rightly, finds none of the approximations to the error degrees of freedom good enough for general usage. Opinions on this will vary. It is agreed that the true Type 1 error rate when using one of the approximations will not be exactly equal to the \\(\\alpha\\) level. In some cases, the difference may be substantial, but often the approximation will be reasonable enough to be useful in practice. For further information on this, there is a section in a very useful GLMM FAQ. You can also see ?lme4::pvalues for some information about various approaches to obtaining \\(p\\)-values. The final table, under the Correlation of Fixed Effects, shows the approximate correlation between the estimators of the fixed effects. You can think of it as the expected correlation between the estimates of the fixed effects over all different datasets that you might obtain (assuming that the predictors have the same values in each). It is not something you generally need to be concerned about. 9.1.2 Visually assessing model assumptions You can use the predict and residuals function to obtain the predicted values and residuals for a linear mixed effects model. You can then plot these, using e.g. ggplot2, as follows: library(ggplot2) tdat &lt;- data.frame(predicted=predict(mod), residual = residuals(mod)) ggplot(tdat,aes(x=predicted,y=residual)) + geom_point() + geom_hline(yintercept=0, lty=3) It might be cool to colour the residuals by referrer as follows: tdat &lt;- data.frame(predicted=predict(mod), residual = residuals(mod), referrer=anchoring$referrer) ggplot(tdat,aes(x=predicted,y=residual, colour=referrer)) + geom_point() + geom_hline(yintercept=0, lty=3) If the legend gets in the way, you can turn it off as follows: ggplot(tdat,aes(x=predicted,y=residual, colour=referrer)) + geom_point() + geom_hline(yintercept=0, lty=3) + theme(legend.position = &quot;none&quot;) The theme function allows for lots of functionality (check ?theme). You can also get a quick predicted vs residual plot by simply calling plot(mod). We can get a histogram of the residuals, and a QQ plot, as follows: ggplot(tdat,aes(x=residual)) + geom_histogram(bins=20, color=&quot;black&quot;) ggplot(tdat,aes(sample=residual)) + stat_qq() + stat_qq_line() 9.1.3 Random intercepts and slopes Now let’s estimate a model with random intercepts and random slopes for anchor. To do so, we can simply add anchor in the mixed effects structure specification, as follows: modg &lt;- lmer(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring) summary(modg) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: anchoring ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 2470481 1572 ## anchor1 36036021 6003 -0.80 ## Residual 87887265 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22840.8 329.4 69.34 ## anchor1 23578.8 1139.0 20.70 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 As you can see, the model now estimates a variance of the random slopes effects, as well as a correlation between the random intercept and slope effects. We could try to get a model without the correlations as follows: modr &lt;- lmer(everest_feet ~ anchor + (1 + anchor||referrer), data=anchoring) As you can see in the warning messages, this leads to various estimation issues. Moreover, the correlation is actually still there! summary(modr) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: everest_feet ~ anchor + ((1 | referrer) + (0 + anchor | referrer)) ## Data: anchoring ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 72369 269 ## referrer.1 anchorhigh 3835401 1958 ## anchorlow 18979098 4357 -0.77 ## Residual 87887261 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22840.8 329.4 69.34 ## anchor1 23578.8 1139.0 20.70 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 ## optimizer (nloptwrap) convergence code: 0 (OK) ## unable to evaluate scaled gradient ## Model failed to converge: degenerate Hessian with 1 negative eigenvalues As it turns out, the || notation does not work with factors!. It only works with metric predictors. We can get the desired model by defining a contrast-coding predictor for anchor explicitly, as follows: anchoring$anchor_contrast &lt;- 1/2 anchoring$anchor_contrast[anchoring$anchor == &quot;low&quot;] &lt;- -1/2 modr &lt;- lmer(everest_feet ~ anchor_contrast + (1 + anchor_contrast||referrer), data=anchoring) summary(modr) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: ## everest_feet ~ anchor_contrast + ((1 | referrer) + (0 + anchor_contrast | ## referrer)) ## Data: anchoring ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 anchor_contrast 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 22773.6 326.7 69.71 ## anchor_contrast 23526.1 1141.8 20.60 ## ## Correlation of Fixed Effects: ## (Intr) ## anchr_cntrs 0.011 That is a little annoying, especially if you have a factor with lots of levels, in which case you would have to specify a lot of contrast-coding predictors. The lmer_alt() function in the afex package will automatically generate the contrast-coding predictors needed, which will be convenient. You can try this by running: modr &lt;- afex::lmer_alt(everest_feet ~ anchor + (1 + anchor||referrer), set_data_arg = TRUE, data=anchoring) summary(modr) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## Data: anchoring ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 re1.anchor1 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22773.65 326.69 29.08 69.71 &lt;2e-16 *** ## anchor1 23526.08 1141.84 28.22 20.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.011 Note the use of the additional set_data_arg = TRUE argument, which is necessary to later use the object for model comparisons with the likelihood ratio test in the next section. Also note that the parameters now do come with \\(p\\)-values (using the Satterthwaite approximation). 9.1.4 Likelihood ratio test with the anova function We now have two versions of our random intercepts + slopes model, one which estimates the correlation between the random intercept and slope, and one which sets this to 0. A likelihood-ratio test comparing these two models is easily obtained as: anova(modr,modg) ## Data: anchoring ## Models: ## modr: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## modg: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## modr 5 98000 98032 -48995 97990 ## modg 6 97985 98024 -48987 97973 16.339 1 5.296e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note the message refitting model(s) with ML (instead of REML). The likelihood-ratio test requires that the models are estimated by maximum likelihood, rather than restricted maximum likelihood (REML). The lme4 package is clever enough to realize this, and first re-estimates the model before computing the likelihood ratio test. Also note that the test statistic is now called “Chisq,” for Chi-squared. This is the one we want. The test result is significant, and hence we can reject the null hypothesis that the correlation between the random intercept and slope is \\(\\rho_{\\gamma_0,\\gamma_1} = 0\\). 9.1.5 Confidence intervals While the lme4 package does not provide \\(p\\)-values, it does have functionality to compute confidence intervals via the confint() function. The default option is to compute so-called profile likelihood confidence intervals for all (fixed and random) parameters: confint(modg) ## 2.5 % 97.5 % ## .sig01 1083.558297 2187.2825617 ## .sig02 -0.956541 -0.5042915 ## .sig03 4471.113419 7999.4737204 ## .sigma 9185.991570 9570.4663804 ## (Intercept) 22184.246089 23499.3202067 ## anchor1 21298.714977 25838.8350190 Note that .sig01 refers to the standard deviation of the random intercept (i.e. \\(\\sigma{\\gamma_0}\\)), .sig02 refers to the correlation between the random intercept and random slope (i.e. \\(\\rho_{\\gamma_0,\\gamma_1}\\)), and .sig03 to the standard deviation of the random slope (i.e. \\(\\sigma_{\\gamma_1}\\)). The value of .sig refers to the standard deviation of the error term (i.e. \\(\\sigma_\\epsilon\\)). Unfortunately, these are not the most informative labels, so it pays to check the values reported in summary(modg) to match them to the output here. Parametric bootstrap confidence (via simulation) can be obtained by setting the argument method = \"boot\". This is a very computationally intensive procedure, so you will have to wait some time for the results! Moreover, due to the random simulation involved, the results will vary (hopefully a little) every time you run the procedure: set.seed(20201201) confint(modg, method=&quot;boot&quot;) ## 2.5 % 97.5 % ## .sig01 1041.469 2111.5327662 ## .sig02 -1.000 -0.5192476 ## .sig03 4369.214 7716.5436429 ## .sigma 9171.071 9573.5222532 ## (Intercept) 22138.746 23535.5279557 ## anchor1 21366.564 26111.6466861 confint(modg, method=&quot;boot&quot;) ## 2.5 % 97.5 % ## .sig01 1045.6131551 2151.4335327 ## .sig02 -0.9976517 -0.5486075 ## .sig03 4296.8794715 7695.7776431 ## .sigma 9164.6786544 9584.8451898 ## (Intercept) 22141.6525509 23504.3587997 ## anchor1 21047.7038556 25819.4349753 Note the warning messages. By default, the bootstrap simulates nsim=500 datasets and re-estimates the model for each. In some of the simulated datasets, the estimation may fail, which provides the resulting warning messages. While confidence intervals and hypothesis tests, in the case of “standard” linear models give the same results, this is not necessarily the case for mixed models, as the \\(F\\) tests for the fixed effects involve approximation of the error degrees of freedom (\\(\\text{df}_2\\)), whilst the computation of the confidence intervals rely on other forms of approximation (e.g. simulation for the parametric bootstrap). As confidence intervals are included by default in lme4, it seems like the author of the package believes these are perhaps more principled than the \\(F\\) tests for the fixed effects. 9.1.6 Plotting model predictions It can be useful to plot the model predictions for each level of the random grouping factor. We can obtain such a plot by storing the model predictions with the data. By adding the group = and colour = arguments in the aes() function, you can then get separate results for all levels of the random effect factor (referrer here). For instance, we can plot the predictions for the different levels of the anchor factor with connecting lines as follows: anchoring$pred &lt;- predict(modg) ggplot(anchoring,aes(x=anchor,y=pred,colour=referrer, group=referrer)) + geom_point() + geom_line() + theme(legend.position=&quot;bottom&quot;, legend.direction = &quot;horizontal&quot;) This would also work if the variable of the x-axis is continuous, rather than categorical. 9.2 Obtaining p-values with afex::mixed Despite some of the concerns about the validity of the \\(p\\)-values for \\(F\\)-tests of the fixed effects, they are often useful (if only to satisfy reviewers of your paper). Packages such as pbkrtest (Højsgaard 2020) and lmerTest (Kuznetsova, Bruun Brockhoff, and Haubo Bojesen Christensen 2020) have been developed to provide these for mixed-effects models estimated with the lmer() function, using the Kenward-Roger or parametric bootstrap, and Satterthwaite approximations, respectively. The afex package (Singmann et al. 2021) provides a common interface to the functionality of these packages, via its mixed function. The mixed function offers some other convenient features, such as automatically using sum-to-zero contrasts (via contr.sum()), although I prefer setting my own contrasts and turn this off. Some of the main arguments to the mixed function (see ?mixed for the full overview) are: formula: a two-sided linear formula describing both the fixed-effects and random-effects part of the model, with the response on the left of the ~ operator and predictors and random effects on the right-hand side of the ~ operator. data: A data.frame, which needs to be in the so-called “long” format, with a single row per observation. This may be different from what you might be used to when dealing with repeated-measures. A repeated-measures ANOVA in SPSS requires data in the “wide” format, where you use columns for the different repeated measures. Data in the “wide” format has a single row for each participants. In the “long” format, you will have multiple rows for the data from a single grouping level (e.g., participant, lab, etc.). type: Sums of Squares type to use (1, 2, or 3). Default is 3. method: Character vector indicating which approximation method to use for obtaining the p-values. \"KR\" for the Kenward-Roger approximation (default), \"S\" for the Satterthwaite approximation, \"PB\" for a parametric bootstrap, and \"LRT\" for the likelihood ratio test. test_intercept: Logical variable indicating whether to obtain a test of the fixed intercept (only for Type 3 SS). Default is FALSE check_contrasts: Logical variable indicating whether contrasts for factors should be checked and changed to contr.sum if they are not identical to contr.sum. Default is TRUE. You should set this to FALSE if you supply your own orthogonal contrasts. expand_re: Logical variable indicating whether random effect terms should be expanded (i.e. factors transformed into contrast-coding numerical predictors) before fitting with lmer. This allows proper use of the || notation with factors. Let’s try it out! First, let’s load the package: library(afex) Note that after loading the afex package, the lmer function from lme4 will be “masked” and the corresponding function from the afex namespace will be used (it is actually the same as the one from the lmertest namespace), which is mostly the same, but expands the class of the returned object somewhat. Afterwards, you either have to use lme4::lmer whenever you explicitly want the function from the lme4 package, or avoid loading the afex package, and always type e.g. afex::mixed. Either is fine, and mostly you wouldn’t need to worry, but sometimes the overriding of function names in the global workspace can give confusion and unexpected results, so it is good to be aware if this behaviour. In the code below, I use the mixed function to estimate the model and compute \\(p\\)-values for the fixed effect of anchor and the intercept with the default \"KR\" option (note that this takes some time!): afmodg &lt;- mixed(everest_feet ~ anchor + (1 + anchor|referrer), data=anchoring, check_contrasts = FALSE, test_intercept = TRUE) ## Fitting one lmer() model. [DONE] ## Calculating p-values. [DONE] The class of the returned object saved as afmodg is different from the usual one returned by the lmer function. To get the \\(F\\) tests, you can just type in the name of the object: afmodg ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: anchoring ## Effect df F p.value ## 1 (Intercept) 1, 28.88 4759.47 *** &lt;.001 ## 2 anchor 1, 29.66 427.80 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 You can also use the summary() function to obtain the parameter estimates (now with associated \\(p\\)-values with the Satterthwaite approximation): summary(afmodg) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + anchor | referrer) ## Data: data ## ## REML criterion at convergence: 97944.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5558 -0.6374 -0.1029 0.6551 3.8431 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## referrer (Intercept) 2470481 1572 ## anchor1 36036021 6003 -0.80 ## Residual 87887265 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22840.77 329.42 27.49 69.34 &lt;2e-16 *** ## anchor1 23578.76 1138.95 27.32 20.70 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 -0.658 We can also estimate the model without correlation between the random effects as follows: afmodr &lt;- mixed(everest_feet ~ anchor + (1 + anchor||referrer), data=anchoring, check_contrasts = FALSE, test_intercept = TRUE, expand_re = TRUE) ## Fitting one lmer() model. [DONE] ## Calculating p-values. [DONE] and get the \\(F\\) tests for this model: afmodr ## Mixed Model Anova Table (Type 3 tests, KR-method) ## ## Model: everest_feet ~ anchor + (1 + anchor || referrer) ## Data: anchoring ## Effect df F p.value ## 1 (Intercept) 1, 28.86 4843.68 *** &lt;.001 ## 2 anchor 1, 29.69 424.13 *** &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 and parameter estimates summary(afmodr) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## Data: data ## ## REML criterion at convergence: 97960.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.6330 -0.6464 -0.1011 0.6732 3.8333 ## ## Random effects: ## Groups Name Variance Std.Dev. ## referrer (Intercept) 2350312 1533 ## referrer.1 re1.anchor1 36126381 6011 ## Residual 87899463 9375 ## Number of obs: 4632, groups: referrer, 31 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 22773.65 326.69 29.08 69.71 &lt;2e-16 *** ## anchor1 23526.08 1141.84 28.22 20.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## anchor1 0.011 Note that entering the two models as is into the anova function will not provide the desired re-estimation of the models by maximum likelihood: anova(afmodr,afmodg) ## Data: data ## Models: ## afmodr: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## afmodg: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## afmodr 5 97970 98002 -48980 97960 ## afmodg 6 97956 97995 -48972 97944 15.781 1 7.111e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 and this is not clear from the output (apart from the missing refitting model(s) with ML (instead of REML) message). For the correct results, you will need to provide the lmer model, stored in the afmodr and afmodg objects under $full_model: anova(afmodr$full_model, afmodg$full_model) ## Data: data ## Models: ## afmodr$full_model: everest_feet ~ anchor + (1 + re1.anchor1 || referrer) ## afmodg$full_model: everest_feet ~ anchor + (1 + anchor | referrer) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## afmodr$full_model 5 98000 98032 -48995 97990 ## afmodg$full_model 6 97985 98024 -48987 97973 16.339 1 5.296e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Although the differences are small, the first test compares the “-2 log REML,” instead of the desired “-2 log ML” values. The assumptions underlying the likelihood-ratio test require the latter. "],["bayesian-hypothesis-testing-through-bayes-factors.html", "Chapter 10 Bayesian hypothesis testing through Bayes Factors 10.1 The BayesFactor package", " Chapter 10 Bayesian hypothesis testing through Bayes Factors In this chapter, we will discuss how to compute Bayes Factors for a variety of General Linear Models using the BayesFactor package (Morey and Rouder 2018). The package implements the “default” priors discussed in the SDAM book. 10.1 The BayesFactor package The BayesFactor package implements Bayesian model comparisons for General Linear Models (as well as some other models for e.g. contingency tables and proportions) using JZS-priors for the parameters, or fixing those parameters to 0. Because Bayes Factors are transitive, in the sense that a ratio of Bayes Factor is itself another Bayes factor: \\[\\begin{align} \\text{BF}_{1,2} &amp;= \\frac{p(Y_1,\\ldots,Y_n|\\text{MODEL 1})}{p(Y_1,\\ldots,Y_n|\\text{MODEL 2})} \\\\ &amp;= \\frac{p(Y_1,\\ldots,Y_n|\\text{MODEL 1})/p(Y_1,\\ldots,Y_n|\\text{MODEL 0})} {p(Y_1,\\ldots,Y_n|\\text{MODEL 2})/p(Y_1,\\ldots,Y_n|\\text{MODEL 0})} \\\\ &amp;= \\frac{\\text{BF}_{1,0}}{\\text{BF}_{2,0}} \\end{align}\\] You can compute many other Bayes Factors which might not be provided by the package by simply dividing the Bayes factors that the package does provide. This makes the procedure of model comparison very flexible. If you haven’t installed the BayesFactor package yet, you need to do so first. Then you can load it as usual by: library(BayesFactor) 10.1.1 A Bayesian one-sample t-test A Bayesian alternative to a \\(t\\)-test is provided via the ttestBF function. Similar to the t.test function in the base stats package, this function allows computation of a Bayes factor for a one-sample t-test and independent-samples t-tests (as well as a paired t-test, which we haven’t covered in the course). Let’s re-analyse the data we considered before, concerning participants’ judegments of the height of Mount Everest. The one-sample t-test we computed before, comparing the judgements to an assumed mean of \\(\\mu = 8848\\), was: # load the data library(sdamr) data(&quot;anchoring&quot;) # select the subset we analysed in Chapter 3 dat &lt;- subset(anchoring,(referrer == &quot;swps&quot; | referrer == &quot;swpson&quot;) &amp; anchor == &quot;low&quot;) # compute the Frequentist one-sample t-test t.test(dat$everest_meters, mu=8848) ## ## One Sample t-test ## ## data: dat$everest_meters ## t = -8.4429, df = 108, p-value = 1.558e-13 ## alternative hypothesis: true mean is not equal to 8848 ## 95 percent confidence interval: ## 5716.848 6907.537 ## sample estimates: ## mean of x ## 6312.193 The syntax for the Bayesian alternative is very similar, namely: bf_anchor &lt;- ttestBF(dat$everest_meters, mu=8848) This code provides a test of the following models: \\[\\begin{align} H_0\\!&amp;: \\mu = 8848 \\\\ H_1\\!&amp;: \\frac{\\mu - 8848}{\\sigma_\\epsilon} \\sim \\textbf{Cauchy}(r) \\end{align}\\] After computing the Bayes factor and storing it in an object bf_anchor, we just see the print-out of the result by typing in the name of the object: bf_anchor ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 46902934208 ±0% ## ## Against denominator: ## Null, mu = 8848 ## --- ## Bayes factor type: BFoneSample, JZS This output is quite sparse, which is by no means a bad thing. It shows a few important things. Under Alt. (which stands for the alternative hypothesis), we first see the scaling factor \\(r\\) used for the JZS prior distribution on the effect size. We then see the value of the Bayes Factor, which is “extreme,” showing that the alternative hypothesis at 46,902,934,208. Quite clearly, the average judgements differed from the true height of Mount Everest! After the computed value of the Bayes factor, you will find a proportional error estimate on the Bayes factor. In general, the marginal likelihoods that constitute the numerator (“top model”) and denominator (“bottom model”) of the Bayes factor can not be computed exactly, and have to be approximated by numerical integration routines, or simulation. This results in some (hopefully small) error in computation, and the error estimate indicates the extend to which the true Bayes factor might differ from the computed one. In this case, the error is (proportionally) very small, and hence we can be assured that our conclusion is unlikely to be affected by error in the approximation. As we didn’t set the scaling factor explicitly, the default value is used, which is \\(r = \\frac{\\sqrt{2}}{2} = 0.707\\). Note that this is actually different from the default value of \\(r=1\\) used in Rouder et al. (2009), which first introduced this version of the Bayesian t-test to a psychological audience, and the one used to illustrate the method in the SDAM book. While reducing the default value to \\(r=0.707\\) is probably reasonable given the effect sizes generally encountered in psychological studies, a change in the default prior highlights the subjective nature of the prior distribution in the Bayesian model comparison procedure. You should also realise that different analyses, such as t-tests, ANOVA, and regression models, use different default values for the scaling factor. As shown in the SDAM book, the value of the Bayes factor depends on the choice for the scaling factor. Although the default value may be deemed reasonable, the choice should really be based on a consideration of the magnitude of the effect sizes you (yes, you!) expect in a particular study. This is not always easy, but you should pick one (the default value, for instance, if you can’t think of a better one) before conducting the analysis. If you feel that makes the test too subjective, you may may want to check the robustness of the result for different choices of the scaling factor. You can do this by computing the Bayes factor for a range of choices of the scaling factor, and then inspecting whether the strength of the evidence is in line with your choice for a reasonable range of values around your choice. The code below provides an example of this: # create a vector with different values for the scaling factor r rscale &lt;- seq(.001,3,length=100) # create an ampty vector to store the resulting Bayes factors BFs &lt;- vector(&quot;double&quot;, length=100) # compute the Bayes factor for each value of r for(i in 1:100) { temp_bf &lt;- ttestBF(dat$everest_meters, mu=8848, r = rscale[i]) # the object returned by the ttestBF function is a so-called S4 object # this has &quot;slots&quot; which can be accessed through the &quot;@&quot; operatpr # the actual value for the BF is extracted by @bayesFactor$bf # which is stored in a logarithmic scale, so we need to use &quot;exp&quot; # to obtain the value of the actual Bayes factor BFs[i] &lt;- exp(temp_bf@bayesFactor$bf) } # use ggplot to plot the values of the Bayes factor against the choice of r library(ggplot2) ggplot(data.frame(r=rscale,BF=BFs),aes(x=r,y=BF)) + geom_line() Given the scale of the \\(y\\)-axis, there is overwhelming evidence against the null-hypothesis for most choices of the scaling factor. Hence, the results seem rather robust to the exact choice of prior. 10.1.2 A Bayesian two-sample t-test To compare the means of two groups, we can revisit the Tetris study, where we considered whether the number of memory intrusions is reduced after playing Tetris in combination with memory reactivation, compared to just memory reactivation by itself. The ttestBF function allows us to provide the data for one group as the x argument, and the data for the other group as the y argument, so we can perform our model comparison, by subsetting the dependent variable appropriately, as follows: data(&quot;tetris2015&quot;) bf_tetr &lt;- ttestBF(x=tetris2015$Days_One_to_Seven_Number_of_Intrusions[tetris2015$Condition == &quot;Reactivation&quot;], y = tetris2015$Days_One_to_Seven_Number_of_Intrusions[tetris2015$Condition == &quot;Tetris_Reactivation&quot;]) bf_tetr ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 16.78482 ±0% ## ## Against denominator: ## Null, mu1-mu2 = 0 ## --- ## Bayes factor type: BFindepSample, JZS Which shows strong evidence for the alternative hypothesis over the null hypothesis that the means are identical (i.e. that the difference between the means is zero, \\(\\mu_1 - \\mu_2 = 0\\)), as the alternative model is 2.82 times more likely than the null model, which sets the difference between the means to exactly \\(\\mu_1 - \\mu_2 = 0\\), rather than allowing different values of this difference through the prior distribution. A two-sample t-test should really be identical to a two-group ANOVA model, as both concern the same General Linear Model (a model with a single contrast-coding predictor, with e.q. values of \\(-\\tfrac{1}{2}\\) and \\(\\tfrac{1}{2}\\)). Before fully discussing the way to perform an ANOVA-type analysis with the BayesFactor package, let’s just double-check this is indeed the case: dat &lt;- subset(tetris2015, Condition %in% c(&quot;Tetris_Reactivation&quot;,&quot;Reactivation&quot;)) # remove levels of Condition that are no longer needed due to subsetting dat$Condition &lt;- droplevels(dat$Condition) #contrasts(dat$Condition) &lt;- c(-1/2,1/2) bf2_tetr &lt;- anovaBF(Days_One_to_Seven_Number_of_Intrusions ~ Condition, data = dat) bf2_tetr ## Bayes factor analysis ## -------------- ## [1] Condition : 16.78482 ±0% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS The results are indeed identical. Note that this is because both the ttestBF and anovaBF function use the same prior distribution for the effect. 10.1.3 A Bayesian ANOVA More general ANOVA-type models can be tested though the anovaBF function. This function takes the following important arguments: formula: a formula like in the lm function, specifying which factors to include as well as their interactions (by e.g. using an * operator to specify you want to include the main effects as well as their interactions. Note that unlike in the lm function, all terms must be factors. data: a data.frame containing data for all factors in the formula. whichRandom: a character vector specifying which factors are random. Random factors can be used to obtain a model similar to a repeated-measures ANOVA, or a (restricted) set of linear-mixed effects models (with only factors for the fixed effects). whichModels: a character vector specifying which models to compare. The allowed values are \"all\", \"withmain\" (the default), \"top\", and \"bottom\". Setting whichModels to \"all\" will test all models that can be created by including or not including a main effect or interaction. \"top\" will test all models that can be created by removing or leaving in a main effect or interaction term from the full model. \"bottom\" creates models by adding single factors or interactions to the null model. \"withmain\" will test all models, with the constraint that if an interaction is included, the corresponding main effects are also included. Setting the argument to top produces model comparisons similar to the Type 3 procedure, comparing the full model to a restricted model with each effect removed. Setting the argument to withMain produces model comparisons similar to the Type 2 procedure, with model comparisons that respect the “principle of marginality,” such that tests of the main effects do not consider higher-order interactions, whilst a test of any interaction includes the main effects that constitute the elements in the interactions. rscaleFixed: prior scale for fixed effects. The value can be given numerically, or as one of three strings: \"medium\" (\\(r = \\tfrac{1}{2}\\)), \"wide\": \\(r = \\tfrac{\\sqrt{2}}{2}\\), or \"ultrawide\" (\\(r=1\\)). The default is \"medium\". rscaleRandom: prior scale for random effects. Accepts the same strings as rscaleFixed, and in addition \"nuisance\" (\\(r = 1\\)). The default is \"nuisance\". rscaleEffects: a named vector with prior scales for individual factors. The anovaBF function will (as far as I can gather) always use contr.sum() contrasts for the factors. So setting your own contrasts will have no effect on the results. The exact contrast should not really matter for omnibus tests, and sum-to-zero are a reasonable choice in general (contr.sum implements what we called effect-coding before).3 While the anovaBF function always uses the JZS prior for any effects, it allows you to specify exactly which scaling factor to use for every effect, if so desired. Let’s see what happens when we use a Bayesian ANOVA-type analysis for the data on experimenter beliefs in social priming. First, let’s load the data, and turn the variables reflecting the experimental manipulations into factors: data(&quot;expBelief&quot;) dat &lt;- expBelief dat$primeCond &lt;- factor(dat$primeCond) dat$experimenterBelief &lt;- factor(dat$experimenterBelief) We can now use the anofaBF function to compute the Bayes factors: bf_expB &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat) bf_expB ## Bayes factor analysis ## -------------- ## [1] experimenterBelief : 537.3879 ±0% ## [2] primeCond : 0.1282136 ±0% ## [3] experimenterBelief + primeCond : 69.75452 ±0.99% ## [4] experimenterBelief + primeCond + experimenterBelief:primeCond : 15.18165 ±1.41% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS A main thing to note here is that the comparisons of different versions of MODEL G are against the same MODEL R, which is an intercept-only model. We can see that all models which include experimenterBelief receive strong evidence against the intercept-only model, apart from the model which only includes primeCond, which has less evidence than the intercept-only model. Although this indicates that the primeCond effect might be ignorable, the comparisons are different from comparing reduced models to the general MODEL G with all effects included. We can obtain these Type 3 comparisons by setting the whichModels argument to `“top”``: bf_expB2 &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat, whichModels = &quot;top&quot;) bf_expB2 ## Bayes factor top-down analysis ## -------------- ## When effect is omitted from experimenterBelief + primeCond + experimenterBelief:primeCond , BF is... ## [1] Omit experimenterBelief:primeCond : 4.726269 ±3.59% ## [2] Omit primeCond : 8.262924 ±4.69% ## [3] Omit experimenterBelief : 0.001940934 ±3.64% ## ## Against denominator: ## ApproachAdvantage ~ experimenterBelief + primeCond + experimenterBelief:primeCond ## --- ## Bayes factor type: BFlinearModel, JZS It is very important to realise that the output now concerns the comparison of the reduced model (in the numerator, ie. the “top model”) against the full model (in the denominator, i.e. the “bottom model”), as is stated in the Against denimonator part of the output. That means that low values of the Bayes factor now indicate evidence for the alternative hypothesis that an effect is different from 0. We can change the output in a measure of the alternative model against the null model by inverting the Bayes factor as follows: 1/bf_expB2 ## denominator ## numerator experimenterBelief + primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 0.2115834 ## denominator ## numerator experimenterBelief + experimenterBelief:primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 0.1210225 ## denominator ## numerator primeCond + experimenterBelief:primeCond ## experimenterBelief + primeCond + experimenterBelief:primeCond 515.2159 which now shows strong evidence for the effect of experimenterBelief when we remove it from the full model. The transitivity of the Bayes factor means that we can also obtain some of these results through a ratio of the Bayes factors obtained earlier. For instance, a Type 3 test of the effect of experimenterBelief:primeCond interaction can be obtained by comparing a model with all effects included to a model without this interaction. In the analysis stored in bf_expB, we compared a number of the possible models to an intercept-only model. By comparing the Bayes factors of the model which excludes the interaction to a model which includes it, we can obtain the same Bayes factor of that interaction as follows. In the output of bf_expB, the fourth element compared the full model to the intercept-only model, whilst in the third element, a model with only the main effects of experimenterBelief and primeCond are compared to an intercept-only model. The Type 3 test of the interaction can then be obtained through the ratio of these two Bayes factors: bf_expB[4]/bf_expB[3] ## Bayes factor analysis ## -------------- ## [1] experimenterBelief + primeCond + experimenterBelief:primeCond : 0.217644 ±1.72% ## ## Against denominator: ## ApproachAdvantage ~ experimenterBelief + primeCond ## --- ## Bayes factor type: BFlinearModel, JZS which indicates evidence for the null hypothesis that there is no moderation of the effect of experimenterbelief by primeCond, as the Bayes factor is well below 1. We cannot replicate all Type 3 analyses with the results obtained earlier, unless we ask the function to compare every possible model against the intercept-only model, by specifying whichModels = \"all\": bf_expB3 &lt;- anovaBF(ApproachAdvantage ~ primeCond*experimenterBelief, data = dat, whichModels = &quot;all&quot;) bf_expB3 ## Bayes factor analysis ## -------------- ## [1] experimenterBelief : 537.3879 ±0% ## [2] primeCond : 0.1282136 ±0% ## [3] experimenterBelief:primeCond : 0.1613884 ±0% ## [4] experimenterBelief + primeCond : 67.2707 ±2.01% ## [5] experimenterBelief + experimenterBelief:primeCond : 121.471 ±2.62% ## [6] primeCond + experimenterBelief:primeCond : 0.02731733 ±1.78% ## [7] experimenterBelief + primeCond + experimenterBelief:primeCond : 15.09079 ±1.38% ## ## Against denominator: ## Intercept only ## --- ## Bayes factor type: BFlinearModel, JZS For instance, we can now obtain a Type 3 test for experimenterBelief by comparing the full model (the 7th element in the output) to a model which just excludes this effect (i.e. the 6th element): bf_expB3[7]/bf_expB3[6] ## Bayes factor analysis ## -------------- ## [1] experimenterBelief + primeCond + experimenterBelief:primeCond : 552.4256 ±2.25% ## ## Against denominator: ## ApproachAdvantage ~ primeCond + experimenterBelief:primeCond ## --- ## Bayes factor type: BFlinearModel, JZS which reproduces mostly the result we obtained by setting whichModel = \"top\" before. 10.1.4 Bayesian regression and ANCOVA Apart from different default values of the scaling factor \\(r\\) in the scaled-Cauchy distribution, the BayesFactor package works in the same way for models which include metric predictors. In a multiple regression model with only metric predictors, we can use the convenience function regressionBF. If you want to mix metric and categorical predictors, as in an ANCOVA model, you will have to use the generalTestBF function. All functions discussed so far are really just convenience interfaces to the generalTestBF, which implements Bayes factors for the General Linear Model. These convenience functions are used to determine an appropriate scaling factor for the different terms in the model, but not much else of concern, so you can replicate all the previous analyses through the generalTestBFfunction, if you’d like. Curiously, a scaling factor of \\(r = \\tfrac{1}{2}\\) in this case corresponds to a scaling factor of \\(r = \\tfrac{\\sqrt{2}}{2}\\), which is something I don’t immediately understand, and will require further investigation.↩︎ "],["reproducible-reports-with-rmarkdown.html", "Chapter 11 Reproducible reports with RMarkdown 11.1 YAML headers 11.2 Markdown syntax 11.3 R code chunks and inline R code 11.4 APA documents with papaja", " Chapter 11 Reproducible reports with RMarkdown One of the many things that makes R extremely useful as a data analysis platform is its ability to generate high-quality reproducible reports and documents via R Markdown. Markdown itself is a lightweight markup language, with a plain-text formatting syntax. Markdown documents can be “parsed” to produce documents in a variety of formats, including HTML, PDF, Open Document Type, Microsoft Word, you name it… R Markdown integrates Markdown with R, allowing you to use R to include figures, tables, and R output (as well as the corresponding code, if you wish) directly in the document. A big benefit of this is that can keep your analysis and write-up in one place, so you don’t have to copy-paste results from one place to another (which often results in errors and issues). Including all analysis in the same document allows for completely reproducible research. Anyone with the R Markdown file would be able to reproduce everything in a scientific paper. R Markdown is extremely flexible and useful. For instance, this book was completely written in R Markdown, with help from the additional bookfown package (https://bookdown.org/). This chapter provides a brief introduction to R Markdown, as well as the papaja package to produce output in the APA style. As an introduction, there are many things left unsaid, so you will quite likely have to consult other sources to get properly acquainted with all the possibilities of R Markdown. A good reference to R Markdown is the R Markdown cookbook. Another source showcasing the many possibilities of R Markdown is R Markdown: The Definitive Guide. This chapter includes parts of the latter book. To get started with R Markdown, you can create a template from within RStudio, by clicking on the menu File &gt; New File &gt; R Markdown. You can fill in the title of your document, your name, and choose the output format (HTML, PDF, or Word). A new R Markdown document will be generated. If you click on the knit button, just above the file, you will be prompted to save the file, and then the file will be parsed and the output document generated. 11.1 YAML headers Headers in R Markdown files contain some metadata about your document, which you can customize to your liking. They use a syntax called YAML. Below is a simple example that purely states the title, author name(s), date, and output format. --- title: &quot;My title&quot; author: &quot;Mr My full Name&quot; date: &quot;December 21, 2020&quot; output: html_document --- You can change the output option to pdf_document, or word_document. Some other arguments you may want to provide in the YAML header are for the bibliography and the style of the output. For example, the previous header can be expanded to --- title: &quot;My title&quot; author: &quot;Mr My full Name&quot; date: &quot;December 21, 2020&quot; output: html_document: toc: true toc_float: true number_sections: true theme: cerulian bibliography: references.bib biblio-style: apalike --- This would produce an HTML output with a floating table of contents (toc: true, toc_float: true), numbered sections, and the “cerulian” theme (see e.g. https://www.datadreaming.org/post/r-markdown-theme-gallery/ for some of the available themes). In addition, it tells the R Markdown compiler to look for BibTex references in the references.bib file, and use an APA style for references. There are many things that can be specified in the header file. To make this process easier, you can install the ymlthis package, which also provides an RStudio plugin with a graphical user interface for some common options (see https://ymlthis.r-lib.org/). Note that if you want to create PDF documents you additionally need a TeX distribution. If you have no use for TeX beyond rendering R Markdown documents, I recommend you use TinyTex. TinyTex can be installed from within R as follows. if(!&quot;tinytex&quot; %in% rownames(installed.packages())) install.packages(&quot;tinytex&quot;) tinytex::install_tinytex() Other, more full-fledged LaTeX MikTeX for Windows, MacTeX for Mac, or TeX Live for Linux. 11.2 Markdown syntax The text in an R Markdown document is written with the Markdown syntax. Precisely speaking, it is Pandoc’s Markdown. There are many flavours of Markdown invented by different people, and Pandoc’s flavour is the most comprehensive one to our knowledge. You can find the full documentation of Pandoc’s Markdown at https://pandoc.org/MANUAL.html. We strongly recommend that you read this page at least once to know all the possibilities with Pandoc’s Markdown, even if you will not use all of them. 11.2.1 Inline formatting Inline text will be italic if surrounded by underscores or asterisks, e.g., _text_ or *text*. Bold text is produced using a pair of double asterisks (**text**). A pair of tildes (~) turn text to a subscript (e.g., H~3~PO~4~ renders H3PO4). A pair of carets (^) produce a superscript (e.g., Cu^2+^ renders Cu2+). To mark text as inline code, use a pair of backticks, e.g., `code`. To include \\(n\\) literal backticks, use at least \\(n+1\\) backticks outside, e.g., you can use four backticks to preserve three backtick inside: ```` ```code``` ````, which is rendered as ```code```. Hyperlinks are created using the syntax [text](link), e.g., [RStudio](https://www.rstudio.com). The syntax for images is similar: just add an exclamation mark, e.g., ![alt text or image title](path/to/image). Footnotes are put inside the square brackets after a caret ^[], e.g., ^[This is a footnote.]. There are multiple ways to insert citations, and we recommend that you use BibTeX databases, because they work better when the output format is LaTeX/PDF. Section 2.8 of Xie (2016) explains the details. The key idea is that when you have a BibTeX database (a plain-text file with the conventional filename extension .bib) that contains entries like: @Manual{R-base, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2017}, url = {https://www.R-project.org/}, } You may add a field named bibliography to the YAML metadata, and set its value to the path of the BibTeX file. Then in Markdown, you may use @R-base (which generates “R Core Team (2021)”) or [@R-base] (which generates “(R Core Team 2021)”) to reference the BibTeX entry. Pandoc will automatically generated a list of references in the end of the document. 11.2.2 Block-level elements Section headers can be written after a number of pound signs, e.g., # First-level header ## Second-level header ### Third-level header If you do not want a certain heading to be numbered, you can add {-} or {.unnumbered} after the heading, e.g., # Preface {-} Unordered list items start with *, -, or +, and you can nest one list within another list by indenting the sub-list, e.g., - one item - one item - one item - one more item - one more item - one more item The output is: one item one item one item one more item one more item one more item Ordered list items start with numbers (you can also nest lists within lists), e.g., 1. the first item 2. the second item 3. the third item - one unordered item - one unordered item The output does not look too much different with the Markdown source: the first item the second item the third item one unordered item one unordered item Blockquotes are written after &gt;, e.g., &gt; &quot;I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.&quot; &gt; &gt; --- Mark Twain The actual output (we customized the style for blockquotes in this book): “I thoroughly disapprove of duels. If a man should challenge me, I would take him kindly and forgivingly by the hand and lead him to a quiet place and kill him.” — Mark Twain Plain code blocks can be written after three or more backticks, and you can also indent the blocks by four spaces, e.g., ``` This text is displayed verbatim / preformatted ``` Or indent by four spaces: This text is displayed verbatim / preformatted In general, you’d better leave at least one empty line between adjacent but different elements, e.g., a header and a paragraph. This is to avoid ambiguity to the Markdown renderer. For example, does “#” indicate a header below? In R, the character # indicates a comment. And does “-” mean a bullet point below? The result of 5 - 3 is 2. Different flavours of Markdown may produce different results if there are no blank lines. 11.2.3 Math expressions You can write mathematical expressions using LaTeX syntax. LaTeX is a system for scientific typesetting widely used in academia. It has excellent support for mathematical notation. Below, you will find some examples. For more extensive information on mathematical notation, you can consult e.g. https://en.wikibooks.org/wiki/LaTeX/Mathematics Inline LaTeX equations can be written in a pair of dollar signs using the LaTeX syntax, e.g., $f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$ (actual output: \\(f(k)={n \\choose k}p^{k}(1-p)^{n-k}\\)); math expressions of the display style can be written in a pair of double dollar signs, e.g., $$f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$$, and the output looks like this: \\[f\\left(k\\right)=\\binom{n}{k}p^k\\left(1-p\\right)^{n-k}\\] You can also use math environments inside $ $ or $$ $$, e.g., $$\\begin{array}{ccc} x_{11} &amp; x_{12} &amp; x_{13}\\\\ x_{21} &amp; x_{22} &amp; x_{23} \\end{array}$$ \\[\\begin{array}{ccc} x_{11} &amp; x_{12} &amp; x_{13}\\\\ x_{21} &amp; x_{22} &amp; x_{23} \\end{array}\\] $$X = \\begin{bmatrix}1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3} \\end{bmatrix}$$ \\[X = \\begin{bmatrix}1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3} \\end{bmatrix}\\] $$\\Theta = \\begin{pmatrix}\\alpha &amp; \\beta\\\\ \\gamma &amp; \\delta \\end{pmatrix}$$ \\[\\Theta = \\begin{pmatrix}\\alpha &amp; \\beta\\\\ \\gamma &amp; \\delta \\end{pmatrix}\\] $$\\begin{vmatrix}a &amp; b\\\\ c &amp; d \\end{vmatrix}=ad-bc$$ \\[\\begin{vmatrix}a &amp; b\\\\ c &amp; d \\end{vmatrix}=ad-bc\\] 11.3 R code chunks and inline R code In R Studio, you can insert an R code chunk either using the RStudio toolbar (the Insert button) or the keyboard shortcut Ctrl + Alt + I (Cmd + Option + I on macOS). There are a lot of things you can do in a code chunk: you can produce text output, tables, or graphics. You have fine control over all these output via chunk options, which can be provided inside the curly braces (between ```{r and }). For example, you can choose hide text output via the chunk option results = 'hide', or set the figure height to 4 inches via fig.height = 4. Chunk options are separated by commas, e.g., ```{r, chunk-label, results=&#39;hide&#39;, fig.height=4} The value of a chunk option can be an arbitrary R expression, which makes chunk options extremely flexible. For example, the chunk option eval controls whether to evaluate (execute) a code chunk, and you may conditionally evaluate a chunk via a variable defined previously, e.g., ```{r} # execute code if the date is later than a specified day do_it = Sys.Date() &gt; &#39;2018-02-14&#39; ``` ```{r, eval=do_it} x = rnorm(100) ``` There are a large number of chunk options in knitr documented at https://yihui.name/knitr/options. We list a subset of them below: eval: Whether to evaluate a code chunk. echo: Whether to echo the source code in the output document (someone may not prefer reading your smart source code but only results). results: When set to 'hide', text output will be hidden; when set to 'asis', text output is written “as-is,” e.g., you can write out raw Markdown text from R code (like cat('**Markdown** is cool.\\n')). By default, text output will be wrapped in verbatim elements (typically plain code blocks). collapse: Whether to merge text output and source code into a single code block in the output. This is mostly cosmetic: collapse = TRUE makes the output more compact, since the R source code and its text output are displayed in a single output block. The default collapse = FALSE means R expressions and their text output are separated into different blocks. warning, message, and error: Whether to show warnings, messages, and errors in the output document. Note that if you set error = FALSE, rmarkdown::render() will halt on error in a code chunk, and the error will be displayed in the R console. Similarly, when warning = FALSE or message = FALSE, these messages will be shown in the R console. include: Whether to include anything from a code chunk in the output document. When include = FALSE, this whole code chunk is excluded in the output, but note that it will still be evaluated if eval = TRUE. When you are trying to set echo = FALSE, results = 'hide', warning = FALSE, and message = FALSE, chances are you simply mean a single option include = FALSE instead of suppressing different types of text output individually. cache: Whether to enable caching. If caching is enabled, the same code chunk will not be evaluated the next time the document is compiled (if the code chunk was not modified), which can save you time. However, I want to honestly remind you of the two hard problems in computer science (via Phil Karlton): naming things, and cache invalidation. Caching can be handy but also tricky sometimes. fig.width and fig.height: The (graphical device) size of R plots in inches. R plots in code chunks are first recorded via a graphical device in knitr, and then written out to files. You can also specify the two options together in a single chunk option fig.dim, e.g., fig.dim = c(6, 4) means fig.width = 6 and fig.height = 4. out.width and out.height: The output size of R plots in the output document. These options may scale images. You can use percentages, e.g., out.width = '80%' means 80% of the page width. fig.align: The alignment of plots. It can be 'left', 'center', or 'right'. dev: The graphical device to record R plots. Typically it is 'pdf' for LaTeX output, and 'png' for HTML output, but you can certainly use other devices, such as 'svg' or 'jpeg'. fig.cap: The figure caption. child: You can include a child document in the main document. This option takes a path to an external file. Chunk options in knitr can be surprisingly powerful. You are encouraged to read the knitr documentation to discover the possibilities. You may also read Xie, Allaire, and Grolemund (2018), which is freely available online at https://bookdown.org/yihui/rmarkdown/. There is an optional chunk option that does not take any value, which is the chunk label. It should be the first option in the chunk header. Chunk labels are mainly used in filenames of plots and cache. If the label of a chunk is missing, a default one of the form unnamed-chunk-i will be generated, where i is incremental. I strongly recommend that you only use alphanumeric characters (a-z, A-Z and 0-9) and dashes (-) in labels, because they are not special characters and will surely work for all output formats. Other characters, spaces and underscores in particular, may cause trouble in certain packages, such as bookdown. If a certain option needs to be frequently set to a value in multiple code chunks, you can consider setting it globally in the first code chunk of your document, e.g., ```{r, setup, include=FALSE} knitr::opts_chunk$set(fig.width = 8, collapse = TRUE) ``` Besides code chunks, you can also insert values of R objects inline in text. For example: ```{r} x = 5 # radius of a circle ``` For a circle with the radius `r x`, its area is `r pi * x^2`. 11.3.1 Figures By default, figures produced by R code will be placed immediately after the code chunk they were generated from. For example: ```{r} plot(cars, pch = 18) ``` You can provide a figure caption using fig.cap in the chunk options. If the document output format supports the option fig_caption: true (e.g., the output format rmarkdown::html_document), the R plots will be placed into figure environments. In the case of PDF output, such figures will be automatically numbered. If you also want to number figures in other formats (such as HTML), please see the bookdown package in https://bookdown.org/yihui/rmarkdown/books.html. PDF documents are generated through the LaTeX files generated from R Markdown. A highly surprising fact to LaTeX beginners is that figures float by default: even if you generate a plot in a code chunk on the first page, the whole figure environment may float to the next page. This is just how LaTeX works by default. It has a tendency to float figures to the top or bottom of pages. Although it can be annoying and distracting, we recommend that you refrain from playing the “Whac-A-Mole” game in the beginning of your writing, i.e., desparately trying to position figures “correctly” while they seem to be always dodging you. You may wish to fine-tune the positions once the content is complete using the fig.pos chunk option (e.g., fig.pos = 'h'). See https://www.overleaf.com/learn/latex/Positioning_images_and_tables for possible values of fig.pos and more general tips about this behavior in LaTeX. In short, this can be a difficult problem for PDF output. To place multiple figures side-by-side from the same code chunk, you can use the fig.show='hold' option along with the out.width option. Figure 11.1 shows an example with two plots, each with a width of 50%. par(mar = c(4, 4, .2, .1)) plot(cars, pch = 19) plot(pressure, pch = 17) Figure 11.1: Two plots side-by-side. If you want to include a graphic that is not generated from R code, you may use the knitr::include_graphics() function, which gives you more control over the attributes of the image than the Markdown syntax of ![alt text or image title](path/to/image) (e.g., you can specify the image width via out.width). ```{r, out.width=&#39;25%&#39;, fig.align=&#39;center&#39;, fig.cap=&#39;...&#39;} knitr::include_graphics(&#39;images/cute-cat-picture.png&#39;) ``` 11.3.2 Tables The easiest way to include tables is by using knitr::kable(), which can create tables for HTML, PDF and Word outputs.4 Table captions can be included by passing caption to the function, e.g., ```{r tables-mtcars} knitr::kable(iris[1:5, ], caption = &#39;A caption&#39;) ``` Tables in non-LaTeX output formats will always be placed after the code block. For LaTeX/PDF output formats, tables have the same issue as figures: they may float. If you want to avoid this behavior, you will need to use the LaTeX package longtable, which can break tables across multiple pages. This can be achieved by adding \\usepackage{longtable} to your LaTeX preamble, and passing longtable = TRUE to kable(). If you are looking for more advanced control of the styling of tables, you are recommended to use the kableExtra package, which provides functions to customize the appearance of PDF and HTML tables. Formatting tables can be a very complicated task, especially when certain cells span more than one column or row. It is even more complicated when you have to consider different output formats. For example, it is difficult to make a complex table work for both PDF and HTML output. We know it is disappointing, but sometimes you may have to consider alternative ways of presenting data, such as using graphics. 11.4 APA documents with papaja To help you write documents according to the APA guidelines, you can use the papaja package. This section contains materials adapted from the papaja readme. 11.4.1 Installation To use papaja you need either an up-to-date version of RStudio or pandoc. papaja is not yet available on CRAN but you can install it from this repository: # Install devtools package if necessary if(!&quot;devtools&quot; %in% rownames(installed.packages())) install.packages(&quot;devtools&quot;) # Install the stable development verions from GitHub devtools::install_github(&quot;crsh/papaja&quot;) # Install the latest development snapshot from GitHub devtools::install_github(&quot;crsh/papaja@devel&quot;) 11.4.2 How to use papaja Once papaja is installed, you can select the APA template when creating a new Markdown file through the RStudio menus. APA template selection If you want to add citations specify your BibTeX-file in the YAML front matter of the document (bibliography: my.bib) and you can start citing. If necessary, have a look at R Markdown’s overview of the citation syntax. You may also be interested in citr, an R Studio addin to swiftly insert Markdown citations. 11.4.2.1 Helper functions to report analyses The functions apa_print() and apa_table() facilitate reporting results of your analyses. Take a look at the R Markdown-file of the example manuscript in the folder example and the resulting PDF. Drop a supported analysis result, such as an htest- or lm-object, into apa_print() and receive a list of possible character strings that you can use to report the results of your analysis. my_lm &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Width + Petal.Length, data = iris) apa_lm &lt;- apa_print(my_lm) One element of this list is apa_lm$table that, in the case of an lm-object, will contain a complete regression table. Pass apa_lm$table to apa_table() to turn it into a proper table in your PDF or Word document. apa_table(apa_lm$table, caption = &quot;Iris regression table.&quot;) Table 11.1: Iris regression table. Predictor \\(b\\) 95% CI \\(t(146)\\) \\(p\\) Intercept 1.04 \\([0.51\\), \\(1.58]\\) 3.85 &lt; .001 Sepal Length 0.61 \\([0.48\\), \\(0.73]\\) 9.77 &lt; .001 Petal Width 0.56 \\([0.32\\), \\(0.80]\\) 4.55 &lt; .001 Petal Length -0.59 \\([-0.71\\), \\(-0.46]\\) -9.43 &lt; .001 papaja currently provides methods for the following object classes: A-B B-L L-S S-Z afex_aov BFBayesFactorTop* lm summary.aovlist anova default lsmobj* summary.glht* Anova.mlm emmGrid* manova summary.glm aov glht* papaja_wsci summary.lm aovlist glm summary_emm* summary.ref.grid* BFBayesFactor* htest summary.Anova.mlm BFBayesFactorList* list summary.aov * Not fully tested, don’t trust blindly! 11.4.2.2 Plot functions Be sure to also check out apa_barplot(), apa_lineplot(), and apa_beeplot() (or the general function apa_factorial_plot()) if you work with factorial designs: apa_factorial_plot( data = npk , id = &quot;block&quot; , dv = &quot;yield&quot; , factors = c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;) , ylim = c(0, 80) , level = .34 , las = 1 , ylab = &quot;Yield&quot; , plot = c(&quot;swarms&quot;, &quot;lines&quot;, &quot;error_bars&quot;, &quot;points&quot;) ) If you prefer creating your plots with ggplot2 try theme_apa(). 11.4.3 Getting help For an in-depth introduction to papaja, check out the current draft of the manual. 11.4.4 Other related R packages By now, there are a couple of R packages that provide convenience functions to facilitate the reporting of statistics in accordance with APA guidelines. apa: Format output of statistical tests in R according to APA guidelines APAstats: R functions for formatting results in APA style and other stuff apaTables: Create American Psychological Association (APA) Style Tables pubprint: This package takes the output of several statistical tests, collects the characteristic values and transforms it in a publish-friendly pattern schoRsch: Tools for Analyzing Factorial Experiments sigr: Concise formatting of significances in R Obviously, not all journals require manuscripts and articles to be prepared according to APA guidelines. If you are looking for other journal article templates, the following list of rmarkdown/pandoc packages and templates may be helpful. rticles: LaTeX Journal Article Templates for R Markdown chi-proc-rmd-template: ACM CHI Proceedings R Markdown Template Michael Sachs’ pandoc journal templates: Pandoc templates for the major statistics and biostatistics journals Bates, Douglas, Martin Maechler, Ben Bolker, and Steven Walker. 2020. Lme4: Linear Mixed-Effects Models Using Eigen and S4. https://github.com/lme4/lme4/. Fox, John, Sanford Weisberg, and Brad Price. 2020. Car: Companion to Applied Regression. https://CRAN.R-project.org/package=car. Højsgaard, Ulrich Halekoh Søren. 2020. Pbkrtest: Parametric Bootstrap and Kenward Roger Based Methods for Mixed Model Comparison. http://people.math.aau.dk/~sorenh/software/pbkrtest/. Kuznetsova, Alexandra, Per Bruun Brockhoff, and Rune Haubo Bojesen Christensen. 2020. lmerTest: Tests in Linear Mixed Effects Models. https://github.com/runehaubo/lmerTestR. Lenth, Russell V. 2021. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://github.com/rvlenth/emmeans. Morey, Richard D., and Jeffrey N. Rouder. 2018. BayesFactor: Computation of Bayes Factors for Common Designs. https://richarddmorey.github.io/BayesFactor/. R Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rouder, Jeffrey N, Paul L Speckman, Dongchu Sun, Richard D Morey, and Geoffrey Iverson. 2009. “Bayesian t Tests for Accepting and Rejecting the Null Hypothesis.” Psychonomic Bulletin &amp; Review 16 (2): 225–37. Schloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to Ggplot2. https://CRAN.R-project.org/package=GGally. Singmann, Henrik, Ben Bolker, Jake Westfall, Frederik Aust, and Mattan S. Ben-Shachar. 2021. Afex: Analysis of Factorial Experiments. https://CRAN.R-project.org/package=afex. Singmann, Henrik, and David Kellen. 2019. “An Introduction to Linear Mixed Modeling in Experimental Psychology.” In New Methods in Cognitive Psychology, 4–31. Psychology Press. http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf, preprint. Tingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, Kosuke Imai, Minh Trinh, and Weihuang Wong. 2019. Mediation: Causal Mediation Analysis. https://imai.princeton.edu/projects/mechanisms.html. Venables, Bill. 2021. codingMatrices: Alternative Factor Coding Matrices for Linear Model Formulae. https://CRAN.R-project.org/package=codingMatrices. Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2020. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2. Wright, S Paul. 1992. “Adjusted p-Values for Simultaneous Inference.” Biometrics, 1005–13. Xie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with r Markdown. CRC Press. Xie, Yihui, Joseph J Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. CRC Press. You may also consider the pander package. There are several other packages for producing tables, including xtable, Hmisc, and stargazer, but these are generally less compatible with multiple output formats.↩︎ "]]
